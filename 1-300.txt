L a company has an application that sells tickets online and experiences bursts of demand every 7 days. the application has a stateless presentation layer running on amazon ec2, an oracle database to store unstructured data catalog information, and a backend api layer. the front-end layer uses an elastic loadbalancer to distribute the load across nine on-demand instances over three availability zones (azs). the oracle database is running on a single ec2 instance. the company is experiencing performance issues when running more than two concurrent campaigns. a solutions architect must design a solution that meets the following requirements:?address scalability issues?increase the level of concurrency?eliminate licensing costs?improve reliabilitywhich set of steps should the solutions architect take?
A. create an auto scaling group for the front end with a combination of on-demand and spot instances to reduce costs convert the oracle database into a single amazon rds reserved db instance.
B. create an auto scaling group for the front end with a combination of on-demand and spot instances to reduce costs create two additional copies of the database instance, then distribute the databases in separate azs.
C. create an auto scaling group for the front end with a combination of on-demand and spot instances to reduce costs convert the tables in the oracle database into amazon dynamodb tables
D. convert the on-demand instances into spot instances to reduce costs for the front end convert the tables in the oracle database into amazon dynamodb tables.

Mi-fr: spot instances (option d) is not valid response, because dont meet de requirements of "address scalability issues".it is required to address "scalability issue", which can be addressed only by autoscaling. that means the options area, b or c no "licensing cost - that is addressed by only option cthe other two requirements are: "increase the level of concurrency "and "improve reliability" - options c meets those.


2, a company is migrating its three-tier web application from on-premises to the aws cloud.the company has the following requirements for the migration process:?ingest machine images from the on-prermises environment. ynchronize changes from the on­ promises environment to the aws environment until the production cutover. ?minimize downtime when executing the production cutover. ?migrate the virtual machines' root volumes and data volumes.which solution will satisfy these requirements with minimal operational overhead?
A. use aws server migration service (sms) to create and launch a replication job tor each tier of the application launch instances from the amis created by aws sms. after initial testing, perform a final replication and create new instances from the updated amis.
A. 
B. create an aws di vm import/export script to migrate each virtual machine. schedule the script to run incrementally to maintain changes in the application. launch instances from the amis created by vm import/export. once testing is done, rerun the script to do a final import and launch the instances from the amis.
C. use aws server migration service (sms) to upload the operating system volumes. use the aws di import- snapshot command for the data volumes. launch instances from the amis created by aws sms and attach the date volumes to the instances. after initial testing, perform a final replication, launch new instances from the replicated amis, and attach the data volumes to the instances.
D. use aws application discovery service and aws migration hub to group the virtual machines as an application. use the aws di vm import/export script to import the virtural machines as amis.schedule the script to run incrementally to maintain changes in the application. launch instances from the amis.after initial testing.perform a final virtual machine import and launch new instances from the amis.




3, a mobile app has become very popular, and usage has gone from a few hundred to millions of users. users capture and upload images of activities within a city, and provide ratings and recommendations data access patterns are unpredictable. the current application is hosted on amazon ec2 instances behind an application load balancer (alb). the application is experiencing slowdowns and costs are growing rapidly. which changes should a solutions architect make to the application architecture to control costs and improve performance?
A. create an amazon cloudfront distribution and place the alb behind the distribution. store static content in amazon s3 in an infrequent access storage class.
B. store static content in an amazon s3 bucket using the intelligent tiering storage class. use an amazon cloudfront distribution in front of the s3 bucket and the alb.
C. place aws global accelerator in front of the alb. migrate the static content to amazon efs, and then run an aws lambda function to resize the images during the migration process
D. move the application code to aws fargate containers and swap out the ec2 instances with the fargate containers.




4, a company wants to host a global web application on aws. it has the following design requirements:--the access pattern must allow for fetching data from multiple data sources.--

minimize the cost of api calls,--keep page load times to within 50 ms.--provide user authentication and authorization and manage data access for different user --personas (for example, administrator,manager, or engineer).--use a serverless designwhich set of strategies should a solutions architect use?
A. use amazon cloudfront with amazon s3 to host the web application. use amazon api gateway to build the application apis with aws lambda for the custom authorizer. authorize data access by performing user lookup in simple ad.
B. use amazon cloudfront with aws waf to host the web application. use aws appsync to build the applicationapis. use iam groups for each user persona. authorize data access by leveraging iam groups in aws appsync resolvers.
C. use amazon cloudfront with amazon s3 to host the web application. use aws appsync to build the application apis. use amazon cognito groups for each user persona. authorize data access by leveraging amazon cognito groups in aws appsync resolvers.
D. use aws direct connect with amazon s3 to host the web application. use amazon api gateway to build the application apis. use aws lambda for custom authentication and authorization.authorize data access by leveraging iam roles.




5, a company hosts an application on amazon ec2 instances and needs to store files in amazon s3. the files should never traverse the public internet,and only the application ec2 instances are granted access to a specific amazon s3 bucket,a solutions architect has created a vpc endpoint for amazon s3 and connected the endpoint to the application vpc.which additional steps should the solutions architect take to meet these requirements?
A. assign an endpoint policy to the endpoint that restricts access to a specific s3 bucket. attach a bucket policy to the s3 bucket that grants access to the vpc endpoint add the gateway prefix list to a nacl of the instances to limit access to the application ec2 instances only.
B. attach a bucket policy to the s3 bucket that grants access to application ec2 instances only using the aws:sourcelp condition. update the vpc route table so only the application ec2 instances can access the vpc endpoint
C. assign an endpoint policy to the vpc endpoint that restricts access to a specific s3 bucket attach a bucket policy to the s3bucket that grants access to the vpc endpoint assign an iam role to the application ec2 instances and only allow access to this role in the s3 bucket's policy.vpc endpoint that restricts access to s3 in the current region attach abucket's policy.
D. assign an endpoint policy to the vpc endpoint that restricts access to s3 in the current region.attach a bucket policy to the s3 bucket that grants access to the vpc the application
A. 
ec2 instances only. add the gateway prefix list to a nacl to limit access to the application ec2 instances only.




6, a company is manually deploying its application to production and wants to move to a more mature deployment pattern. the company has asked a solutions architect to design a solution that leverages its current chef tools and knowledge. the application must be deployed to a staging environment for testing and verification before being deployed to production. any new deployment must be rolled back in 5 minutes if errors are discovered after a deployment.which aws service and deployment pattern should the solutions architect use to meet these requirements?
A. use aws elastic beanstalk and deploy the application using a rolling update deployment strategy.
B. use aws codepipeline and deploy the application using a rolling update deployment strategy.
C. use aws codebuild and deploy the application using a canary deployment strategy.
D. use aws opsworks and deploy the application using a blue/green deployment strategy.




7, a startup company recently migrated a large ecommerce website to aws the website has experienced a 70% increase in sales.software engineers are using a private github repository to manage code. the devops team is using jenkins for builds and unit testing.the engineers need to receive notifications for bad builds and zero downtime during deployments the engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.the software engineers have decided to use aws codepipeline to manage their build and deployment process. which solution will meet these requirements?
A. use github websockets to trigger the codepipeline pipeline. use the jenkins plugin for aws codebuild to conduct unit testing. send alerts to an amazon sns topic for any bad builds. deploy in an in-place, all-at once deployment configuration using aws codedeploy
B. use github webhooks to trigger the codepipeline pipeline. use the jenkins plugin for aws codebuild to conduct unit testing. send alerts to an amazon sns topic for any bad builds. deploy in a blue/green deployment using aws codedeploy.
C. use github websockets to trigger the codepipeline pipeline. use aws x-ray for unit testing. and static code analysis. send alerts to an amazon sns topic for any bad builds. deploy in a blue/green deployment using aws codedeploy.
D. use github webhooks to trigger the codepieline pipeline use aws x ray tor unit testing and static code analysis sendalerts to an amazon sns topic for any bad builds. deploy in an in place, ll-at-once deployment configuration using aws codedeploy.

B. Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy.
Here's the breakdown:
GitHub Integration: Using GitHub webhooks to trigger CodePipeline ensures that the pipeline is automatically triggered when there are changes in the GitHub repository.
Jenkins Integration: Leveraging the Jenkins plugin for AWS CodeBuild allows you to seamlessly integrate Jenkins into the AWS CodePipeline for build and unit testing processes.
Alerts for Bad Builds: Sending alerts to an Amazon SNS topic for any bad builds provides a notification mechanism for the engineers to be aware of issues promptly.
Deployment Strategy: Blue/green deployment using AWS CodeDeploy is recommended. This allows for zero downtime during deployments, and if any major issues are discovered after deployment, it provides an easy rollback option.

8, a company has several amazon ec2 instances in both public and private subnets within a vpc that is not connected to the corporate network. a security group associated with the ec2 instances allows the company to use the windows remote desktop protocol (rdp) over the internet to access the instances. the security team has noticed connection attempts from unknown sources. the company wants to implement a more secure solution to access the ec2 instances.which strategy should a solutions architect implement?
A. deploy a linux bastion host on the corporate network that has access to all instances in the vpc. session manager, restricting
B. deploy aws systems manager agent on the ec2 instances. access the ec2 instances using access to users with permission.
C. deploy a linux bastion host with an elastic ip address in the public subnet. allow access 10 the bastion host tom 0.0.0.0/0.
D. establish site-to-site vpn connecting the corporate network to the vpc update the security groups to allow access from the corporate network only.

B. Deploy AWS Systems Manager Agent on the EC2 instances. Access the EC2 instances using AWS Systems Manager Session Manager, restricting access to users with permission.
Here's the breakdown:
AWS Systems Manager Agent (SSM Agent): Deploying the AWS Systems Manager Agent on the EC2 instances allows you to use AWS Systems Manager services, including Session Manager. This eliminates the need to expose RDP directly to the internet.
Session Manager: AWS Systems Manager Session Manager provides a secure and auditable way to connect to your instances without the need for an open RDP port. It works over the Systems Manager service and doesn't require inbound internet connectivity.
Restricted Access: Access to the EC2 instances using Session Manager can be restricted to specific IAM (Identity and Access Management) users or roles, ensuring that only authorized users can connect.


9, a company wants to run a serverless application on aws the company plans to provision its application in docker containers running in en amazon ecs cluster the application requires a mysql database and the company plans to use amazon rds the company has documents that need to be accessed frequently for the first 3 months, and rarely after that the documents must be retained for years. what is the most cost effective solution to meet these requirements?
A. create an ecs cluster using on-demand instances provision the database and its read replicas in amazon rds using spot instances. store the documents in an encrypted ebs volume, and create a cron job to delete the documents after 7 years.
B. create an ecs cluster using a fleet of spot instances, with spot instance draining enabled. provision the database and its read replicas in amazon rds using reserved instances. store the documents in a secured amazon s3 bucket with a lifecycle policy to move the documents that are older than 3 months to amazon s3 glacier, then delete the documents from amazon s3 glacier that are more than 7 years old.
C. create an ecs cluster using on-demand instances. provision the database and its read replicas in amazon rds using on-demand instances. store the documents in amazon efs create a cron job to move the documents that are older than months to amazon s3 glacier. create an aws lambda function to delete the documents in s3 glacier that are older than 7 years.
D. create an ecs cluster using a fleet of spot instances with spot instance draining enabled. provision the database and its read replicas in amazon rds using on-demand instances. store the documents in a secured amazon s3 bucket with a lifecycle policy to move the documents that are older than 3 months to amazon s3 glacier, then delete the documents in amazon s3 glacier after 7 years .

B. 
Here's the breakdown:
ECS Cluster with Spot Instances: Using spot instances for the ECS cluster can be cost-effective. Spot instances are available at a lower cost than on-demand instances, and ECS can take advantage of them. Spot instance draining is enabled to gracefully handle interruptions.
Database on RDS with Reserved Instances: Provisioning the database and its read replicas using reserved instances in Amazon RDS is a cost-effective approach, providing a discount compared to on-demand pricing.
Document Storage in Amazon S3 with Lifecycle Policy: Storing documents in Amazon S3 is suitable for scalability and durability. The lifecycle policy ensures that documents older than 3 months are moved to Amazon S3 Glacier, a more cost-effective storage class for infrequently accessed data. After 7 years, documents in Amazon S3 Glacier are deleted.


10, a developer reports receiving an error 403: access denied message when they try to download an object from an amazon s3 bucket.the s3 bucket is accessed using an s3 endpoint inside a vpc, and is encrypted with an aws kms key. a solutions architect has verified that the developer is assuming the correct iam role in the account that allows the object to be downloaded. the s3 bucket policy and the nacl are also valid.which additional step should the solutions architect take to troubleshoot this issue?
A. ensure that blocking all public access has not been enabled in the s3 bucket.
B. verity that the iam role has permission to decrypt the referenced kms key.
C. verify that the iam role has the correct trust relationship configured.
D. check that local firewall rules are not preventing access to the s3 endpoint.

B.
Here's the explanation:
IAM Role Permissions: The IAM role assumed by the developer must have the necessary permissions not only to access the S3 bucket but also to decrypt the KMS key if server-side encryption with AWS Key Management Service (KMS) is used. The 403 error might indicate a lack of decryption permissions.


11,  an enterprise company's data science team wants to provide a safe, cost-effective way to provide easy access to amazon sagemaker. the data scientists have limited aws knowledge and need to be able to launch a jupyter notebook instance. the notebook instance needs to have a preconfigured aws kms key to encrypt data at rest on the machine learning storage volume without exposing the complex setup requirements.which approach will allow the company to set up a self-service mechanism for the data scientists to launch jupyter notebooks in itsaws accounts with the 1 east amount of operational overhead?
A. create a serverless front end using a static amazon s3 website to allow the data scientists to request a jupyter notebook instance by filling out a form. use amazon api gateway to receive requests from the s3 website and trigger a central aws lambda function to make an api call to amazon sagemaker that will launch a notebook instance with a preconfigured kms key for the data scientists. then call back to the front-end website to display the url to the notebook instance.
B. create an aws cloudformation template to launch a jupyter notebook instance using the aws:sagemakerc:notebooklnstance resource type with a preconfigured kms key. add a user­ friendly name to the cloudformation template. display the url to the notebook using the outputs section.distribute the cloudformation template to the data scientists using a shared amazon s3 bucket.
C. create an aws cloudformation template to launch a jupyter notebook instance using the aws:sagemaker:notebookinstance resource type with a preconfigured kms key. simplify the parameter names, such as the instance size, by mapping them to small, large, and x-large using the mappings section in cloudformation. display the url to the notebook using the outputs section, then upload the template into an aws service catalog product in the data scientist's portfolio, and share it with the data scientist's iam role.
D. create an aws di script that the data scientists can run locally. provide step by-step instructions about the parameters to be provided while executing the aws di script to launch a jupyter notebook with a preconfigured kms key. distribute the di script to the data scientists using a shared amazon s3 bucket.

C
Here's the breakdown:
AWS CloudFormation Template: By creating a CloudFormation template, you can define the infrastructure as code, making it easy to launch a Jupyter notebook instance with preconfigured settings, including the KMS key.
Simplify Parameter Names: Using the mappings section to simplify parameter names, such as instance size, helps in providing a user-friendly experience for the data scientists.
Outputs Section: Displaying the URL to the notebook in the outputs section makes it convenient for data scientists to access the newly launched instance.
AWS Service Catalog: Uploading the CloudFormation template into an AWS Service Catalog product allows you to create a self-service mechanism. Data scientists can launch the notebook instance from the AWS Service Catalog, and you can control access using IAM roles.


12, a company has developed a mobile game. the backend for the game runs on several virtual machines located in an on-premises data center. the business logic is exposed using a rest api with multiple functions. player session data is stored in central file storage.backend services use different api keys for throttling and to distinguish between live and test traffic.the load on the game backend varies throughout the day. during peak hours, the server capacity is not sufficient. there are also latency issues when fetching player session data.management has asked a solutions architect to present a cloud architecture that can

handle the game's varying load and provide low-latency data access. the api model should not be changed.which solution meets these requirements?
A. implement the rest api using a network load balancer (nib). run the business logic on an amazon ec2 instance behind the nib. store player session data in amazon aurora serverless.
B. implement the rest api using an application load balancer(alb). run the business logic in aws lambda. store player session data in amazon dynamodb with on demand capacity.
C. implement the rest api using amazon ap gateway run the business logic in aws lambda.store player session data in amazon dynamodb with on- demand capacity.
D. implement the rest api using aws appsync. run the business logic in aws lambda. store player session data in amazon aurora serverless.

C.
Here's the breakdown:
Amazon API Gateway: It allows you to create and publish RESTful APIs with different endpoints to expose the existing business logic without changing the API model.
AWS Lambda: Running the business logic in AWS Lambda provides a serverless and scalable solution. Lambda automatically scales with the incoming requests, helping to handle varying loads efficiently.
Amazon DynamoDB: Storing player session data in DynamoDB with on-demand capacity ensures that the database scales automatically based on the workload. DynamoDB is designed for low-latency and high-throughput access, making it suitable for gaming applications.


13, a solutions architect is implementing infrastructure as code for a two-tier web application in an aws cloudformation template. the web frontend application will be deployed on amazon ec2 instances in an auto scaling group. the backend database will be an amazon rds for mysql db instance. the database password will be rotated every 60 days.how can the solutions architect most securely manage the configuration of the applications database credentials?
A. provide the database password as a parameter in the cloudformation template. create an initialization script in the auto scaling group's launch configuration userdata property to reference to password parameter using the ref intrinsic function store the password on the ec2 instances reference the parameter for the value of the masterusorpassword property in the aws rds:: dbInstance resource using the ref intrinsic function.
B. create a new aws secrets manager secret resource in the cloudformation template to be used as the database password. configure the application to retrieve the password from secrets manager when needed reference the secret resource for the value of the masteruserpassword property in the aws: :rds dbinstance resource using a dynamic reference.
C. create a new aws secrets manager secret resource in the cloudformation template to be used as the database password create an initialization script in the auto scaling group's launch configuration userdata property to reference the secret resource using the ref intrinsic function. reference the secret resource for the value of the masteruserpassword property in the aws rds::dblnstance resource using the ref intrinsic function.
D. create a new aws systems manager parameter store parameter in the cloudformation template to be used as the database password.create an initialization script in the auto scaling group's launch configuration userdata property to reference the parameterfor the value of the masteruerpassword property an the aws ros::dblnstance resource using the fn::getatt intrinsic function.




14, a media company has a static web application that is generated programmatically the company has a build pipeline that generates html content that is uploaded to an amazon s3 bucket served by amazon cloudfront. the build pipeline runs inside a build account. the s3 bucket and clouderont distribution are in a distribution account. the build pipeline uploads the files to amazon s3 using an iam role in the build account. the s3 bucket has a bucket policy that only allows clouderont to read objects using an origin access identity (cal). during testing, all attempts to access the application using the cloudfront url result in an http 403 access denied response.what should a solutions architect suggest to the company to allow access the objects in amazon s3 through cloudfront?
A. modify the s3 upload process in the build account to add the bucket-owner-full-control aol to the objects at upload.
B. create a new cross-account iam role in the distribution account with write access to the 83 bucket.modify the build pipeline to assume this role to upload the files to the distribution account.
C. modify the s3 upload process in the build account to set the object owner to the distribution account.
D. create a new iam role in the distribution account with read access to the 83 bucket. configure cloudftont to use this new role as its oai. modify the build pipeline to assume this role when uploading files from the build account.


fAIHfr: distribution account owns the s3 bucket and build account is uploading the objects. cross account role is needed for this operation. questions does not says any problem with operation means this role already exists.problem is: objects uploaded by build account are not accessible by distribution account even though distribution account is the bucket owner. we need canned acl concept here. build account need to grant either bucket-owner-read or bucket-owner-full-control acl.


15, a company has a complex web application that leverages amazon cloudfront for global scalability and performance. over time, users report that the web application is slowing down. the company's operations team reports that the cloudfront cache hit ratio has been dropping steadily. the cache metrics report indicates that query strings on some urls are

inconsistently ordered and are specified sometimes in mixed case letters and sometimes in lowercase letters.which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?
A. deploy a lambda@edge function to sort parameters by name and force them to be lowercase. select the cloudfront viewer request trigger to invoke the function.
B. update the cloudf ront distribution to disable caching based on query string parameters.
C. deploy a reverse proxy after the load balancer to post process the emitted urls in the application to force the url strings to be lowercase.
D. update the cloudfront distribution to specify casing-insensitive query string processing.

Mi-fr: before cloudfront serves content from the cache it will trigger any lambda function associated with the viewer request, in which we can normalize parameters.


16, a finance company hosts a data lake in amazon s3. the company receives financial data records over sftp each night from several third parties. the company runs its own sftp server on an amazon ec2 instance in a public subnet of a vpc. after the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance.the sftp server is reachable on dns sftp.example.com through the use of amazon route 53. what should a solutions architect do to improve the reliability and scalability of the sftp solution?
A. move the ec2 instance into an auto scaling group.place the ec2 instance behind an application load balancer (alb). update the dns record sftp.example.com in route 53 to point to the alb.
B. migrate the sftp server to aws transfer for sftp.update the dns record sftp.example.com in
route 53 to point to the server endpoint hostname.
C. migrate the sftp server to a file gateway in aws storage gateway.update the dns record sftp.example.com in route 53 to point to the file gateway endpoint.
D. place the ec2 instance behind a network load balancer (nlb).update the dns record sftp.example.com in route 53 to point to the nib.




17,	a company runs a proprietary stateless etl application on an amazon ec2 linux instance. the application is a linux binary, and the source code cannot be modified. the application is single-threaded, uses 2 gb of ram. and is highly cpu intensive. the application is scheduled to

run every 4 hours and runs for up to 20 minutes. a solutions architect wants to revise the architecture for the solution.which strategy should the solutions architect use?
A. use aws lambda to run the application.use amazon cloudwatch logs to invoke the lambda function every 4 hours.
B. use aws batch to run the application.use an aws step functions state machine to invoke the aws batch job every 4 hours.
C. use aws fargate to run the application.use amazon eventbridge (amazon cloudwatch events) to invoke the fargate task every 4 hours.
D. use amazon ec2 spot instances to run the application.use aws codedeploy to deploy and run the application every 4 hours.

Mi-fr: state machine doesn't have scheduler function, it needs to be invoked by other services like cloudeventl) step state machine doesn't have schedule 2) fargate task is good for the job and can be invoked by eventbridge
https:/ / docs.aws.amazon.com/eventbridge /latest/userguide/eb-ecs­ tutorial.htmlcloudeventl) 2)fargateeven tbridge
https:/ / docs.aws.amazon.com/eventbridge /latest/userguide/eb-ecs-tutorial.html


18, a north american company with headquarters on the east coast is deploying a new web application running on amazon ec2 in the us-east-1 region. the application should dynamically scale to meet user demand and maintain resiliency. additionally, the application must have disaster recovery capabilities in an active- passive configuration with the us-west-1 region.which steps should a solutions architect take after creating a vpc in the us-east-1 region?
A. create a vpc in the us-west-1 region use inter. region vpc peering to connect both vpcs. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us-east-1 region. deploy ec2 instances across multiple azs in each region as part of an auto scaling group spanning both vpcs and served by the alb
B. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us- east- 1 region.deploy ec2 instances across multiple azs as part of an auto scaling group served by the alb deploy the same solution to the us-west-1 region. create an amazon route 53 record set with a failover routing policy and health checks enabled to provide high availability across both regions.
C. create a vpc in the us-west-1 region. use inter-region vpc peering to connect both vpcs. deploy an application load balancer (alb) that spans both vpcs. deploy ec2 instances across
A. 
multiple availability zones as part of an auto scaling group in each vpc served by the alb. create an amazon route 53 record that points to the alb.
D. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us east-1 region.deploy ec2 instances across multiple azs as part of an auto scaling group served by the alb. deploy the same solution to the us-west-1 region. create separate amazon route 53 records in each region that point to the alb in the region. use route 53 health checks to provide high availability across both regions.

Mtfr: keywords active-passive failover for dr. we dont need albs to span across vpcs using peering.


19,  an aws partners company it building a service in aws organizations using its organization named orgl. this service requires the partner company to have access to aws resources in a customer account,which is in a separate organization named org2.the company must establish least privilege security access using an api or command line foot to the customer account.what is the most secure way to allow orgl to access resources in org2?
A. the customer should provide the partner company with their aws account access keys to log in and perform the required tasks.
B. the customer should create an iam user and assign the required permissions to the iam user. the customer should then provide the credentials to the partner company to log in and perform the required tasks.
C. the customer should create an iam role and assign the required permissions to the iam role. the partner company should then use the iam role's amazon resource name (arn) when requesting access to perform the required tasks.
D. the customer should create an iam role and assign the required permissions to the iam role. the partner company should then use the iam role's amazon resource name (arn), including the external id in the iam role's trust policy, when requesting access to perform the required tasks.

Mtfr: https:/ / docs.aws.amazon.com/iam/latest/userguide/id_roles_create_for­ user_externalid.html


20, the company user aws organizations with a single ou named production to manage multiple accounts.all accounts are members of the production ou administrators use deny

list scps in the root of the organization to manage access to restricted services.the company recently acquired a new business unit and invited the new unit' s existing aws account to the organization once on boarded,the administrators of the new business unit discovered that they are not able to update existing aws config rules to meet the company' s policies.which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?
A. remove the organization's root scps that limit access to aws config. create aws service catalog products tor the company's standard aws config rules and deploy them throughout the organization, including the new account.
B. create a temporary ou named onboarding for the new account apply an scp to the onboarding ou to allow aws config actions. move the new account to the production ou when adjustments to aws config are complete.
C. convert the organization's root scps from deny list scps to allow list scps to allow the required services only. temporarily apply an scp to the organization's root that allows aws config actions for principals only in the new account.
D. create a temporary ou named onboarding for the new account apply an scp to the onboarding ou to allow aws config actions. move the organization's root scp to the production ou move the new account to the production ou when adjustments to aws config are complete.




21, a company is managing all of its aws accounts by using an organization in aws organizations. a recent audit revealed that some of the company's documents were stored in amazon s3 buckets outside the company's compliant aws regions. after the audit, the company moved all the noncompliant s3 buckets to a compliant region in the united states (us). the company needs an scp that prevents the creation of s3 buckets outside the us. which scp should the company use to meet these requirements with the least amount of operational overhead?
A. {"version"':"2012-10-
17 ","statement":[{"notaction": ["s3.*'],"resource":"*""effect'":"deny""condition":{"stringnote
quals":{"aws:requestedregion":["af-south-1""ap-east-1""ap-south-1""ap-northeast-1""ap­ northeast-2""ap-northeast-3""ap-southeast-1""ap-southeast-2""ap-southeast-3""ca-central-  1"}}}}
B. {"version""2012-10-17""statement":["notaction":["s3"resource"."*"'fete'"d eny'"condition":{"stringequals*":{"aws:requestedregion":["us-east-1""us-east-2" "us-west- 1""us-west-2"
A. 
C. "version"':"2012-10-17","statement"':["notaction":["s3-
*kesource"effet'"deny'""condition"{"stringnotequals":{"aws requestedregion":["us-east­ l""us-east-2""us-west-l""us-west-2"
D. "version""2012-10-17*"statement" ["action"["s3:*"resource""effect':"deny"condition":{"stringequals"("aws requestedregion':["af-south-1" "ap-east-1""ap-south-1""ap-northeast-1""ap-northeast- 2""ap-northeast-3""ap-southeast-1""ap-southeast-2""ap-southeast-3""ca-central-1"."eu­  central-1""eu-west-1""eu-west-2 ""eu-west-3","eu-south-1""eu-north-1","me-south-1""sa­ east-1



22, a company needs to implement a patching process for its servers. the on-premises servers and amazon ec2 instances use a variety of tools to perform patching. management requires a single report showing the patch status of all the servers and instances. which set of actions should a solutions architect take to meet these requirements?
A. use aws systems manager to manage patches on the on-premises servers and ec2 instances.use systems manager to generate patch compliance reports.
B. use aws opsworks to manage patches on the on-premises servers and ec2 instances.use amazon quicksight integration with opsworks to generate patch compliance reports.
C. use an amazon eventbridge (amazon cloudwatch events) rule to apply patches by scheduling an aws systems manager patch remediation job.use amazon inspector to generate patch compliance reports
D. use aws opsworks to manage patches on the on-premises servers and ec2 instances.use aws x- ray to post the patch status to aws systems manage opscenter to generate patch compliance reports.



23, a company has a web application that allows users to upload short videos. the videos are stored on amazon ebs volumes and analyzed by custom recognition software for categorization. the website contains static content that has variable traffic with peaks in certain months. the architecture consists of amazon ec2 instances running in an auto scaling group for the web application and ec2 instances running in an auto scaling group to process an amazon sqs queue. the company wants to re- architect the application to reduce operational overhead using aws managed services.where possible and remove dependencies on third-party software.which solution meets these requirements?

A. use amazon ecs containers for the web application and spot instances for the auto scaling group that processes the sqs queue. replace the custom software with amazon rekognition to categorize the videos.
B. store the uploaded videos in amazon efs and mount the file system to the ec2 instances for the web application. process the sqs queue with an aws lambda function that calls the amazon rekognition api to categorize the videos.
C. host the web application in amazon s3. store the uploaded videos in amazons s3. use s3 event notifications to publish events to the sqs queue. process the sqs queue with an aws lambda function that calls the amazon rekognition api to categorize the videos.
D. use aws elastic beanstalk to launch ec2 instances in an auto scaling group for the web application and launch a worker environment to process the sqs queue. replace the custom software with amazon rekognition to categorize the videos.



24, a company is migrating its development and production workloads to a new organization in aws organizations. the company has created a separate member account for development and a separate member account for production. consolidated billing is linked to the management account. in the management account, a solutions architect needs to create an iam user that can stop or terminate resources in both member accounts.which solution will meet this requirement?
A. create an iam user and a cross-account role in the management account. configure the cross-account role with least privilege access to the member accounts.
B. create an iam user in each member account. in the management account, create a cross­ account role that has least privilege access. grant the iam users access to the cross-account role by using a trust policy.
C. create an iam user in the management account in the member accounts, create an iam group that has least privilege access.add the iam user from the management account to each iam group in the member accounts.
D. create an iam user in the management account. in the member accounts, create cross­ account roles that have least privilege access. grant the iam user access to the roles by using a trust policy.


25, a life sciences company is using a combination of open source tools to manage data analysis workflows and docker containers running on servers in its on-premises data center to process genomics data. sequencing data is generated and stored on a local storage area network (san), and then the data is processed. the research and development teams are running into capacity issues and have decided to re- architect their genomics analysis platform on aws to scale based on workload demands and reduce the turnaround time from weeks to days.the company has a high-speed aws direct connect connection. sequencers will generate around 200 gb of data for each genome;and individual jobs can take several hours to process the data with ideal compute capacity. the end result will be stored in amazon s3. the company is expecting 10-15 job requests each day.which solution meets these requirements?
A. use regularly scheduled aws snowball edge devices to transfer the sequencing data into aws. when aws receives the snowball edge device and the data is loaded into amazon s3, use s3 events to trigger an aws lambda function to process the data.
B. use aws data pipeline to transfer the sequencing data to amazon s3. use s3 events to trigger an amazon ec2 auto scaling group to launch custom-aml ec2 instances running the docker containers to process the data.
C. use aws datasync to transfer the sequencing data to amazon s3. use s3 events to trigger an aws lambda function that starts an aws step functions workflow. store the docker images in amazon elastic container registry (amazon ecr) and trigger aws batch to run the container and process the sequencing data.
D. use an aws storage gateway file gateway to transfer the sequencing data to amazon s3. use s3 events to trigger an aws batch job that executes on amazon ec2 instances running the docker containers to process the data.



26, a company is in the process of implementing aws organizations to constrain its developers to use only amazon ec2, amazon s3,and amazon dynamodb. the developers account resides in a dedicated organizational unit (ou). the solutions architect has implemented the following scp on the developers account:when this policy is deployed. iam users in the developers account are still able to use aws services that are not listed in the policy?what should the solutions architect do to eliminate the developers' ability to use services outside the scope of this poll.


	

{



SU 	11





 ,
	

-s•.-":1•
.,J	;





 s 

ictcert


''s

r
p I	II



A. create an explicit deny statement for each aws service that should be constrained
B. remove the fullawsaccess scp from the developer account's ou
C. modify the fullawsaccess scp to explicitly deny all services
A. 
D. add an explicit deny statement using a wildcard to the end of the scp.

Mi-Jr:
https:/ / docs.aws.amazon.com/organizations /latest/userguide/ orgs_manage_policies_scps_ strategies.html#orgs_policies_denylist


27, a company with global offices has a single 1 gbps aws direct connect connection to a single aws region. the company's on-premises network uses the connection to communicate with the company's resources in the aws cloud. the connection has a single private virtual interface that connects to a single vpc.a solutions architect must implement a solution that adds a redundant direct connect connection in the sameregion. the solution also must provide connectivity to other regions through the same pair of direct connect connections as the company expands into other regions.which solution meets these requirements?
A. provision a direct connect gateway. delete the existing private virtual interface from the existing connection. create the second direct connect connection. create a new private virtual interface on each connection, and connect both private virtual interfaces to the direct connect gateway. connect the direct connect gateway to the single vpc.
B. keep the existing private virtual interface. create the second direct connect connection. create a new private virtual interface on the new connection, and connect the new private virtual interface to the single vpc.
C. keep the existing private virtual interface. create the second direct connect connection. create anew public virtual interface on the new connection, and connect the new public virtual interface to the single vpc.
D. provision a transit gateway delete the existing private virtual interface from the existing connection.create the second direct connect connection create a new private virtual interface on each connection, and connect both private virtual interfaces to the transit gateway. associate the transit gateway with the single vpc



28, a company needs to architect a hybrid dns solution. this solution will use an amazon route 53 private hosted zone for the domain cloud.example.com for the resources stored within vpcs. the company has the following dns resolution requirements:--on-premises systems should be able to resolve and connect to cloud .example.com.--all vpcs should be able to resolve cloud.example .com.there is already an aws direct connect connection

between the on-premises corporate network and aws transit gateway.which architecture should the company use to meet these requirements with the highest performance?
A. associate the private hosted zone to all the vpcs. create a route 53 inbound resolver in the shared services vpc.attach all vpcs to the transit gateway and create forwarding rules in the on-premises dns server for cloud. example .com that point to the inbound resolver.
B. associate the private hosted zone to all the vpcs. deploy an amazon ec2 conditional forwarder in the sharedservices vpc. attach all vpcs to the transit gateway and create forwarding rules in the on- premises dns server for cloud.example.com that point to the conditional forwarder.
C. associate the private hosted zone to the shared services vpc. create a route 53 outbound resolver in the shared services vpc. attach all vpcs to the transit gateway and create forwarding rules in the on- premises dns server for cloud. example .com that point to the outbound resolver.
D. associate the private hosted zone to the shared services vpc. create a route 53 inbound resolver in the shared services vpc. attach the shared services vpc to the transit gateway and create forwarding rules in the on-premises dns server for cloud example .com that point to the inbound resolver.

 It-tfr: all vpcs should be able to resolve the private domain, so all vpcs should be associated with the hosted zone, not just the shared services vpc 2) resources are stored within vpcs, not just the shared services vpc and on-premises systems can connect to the whole domain. hence, a is correct, not d the question asks about accessing resources with in aws vpc from onprem.on prem > inbound end point> route 53 resolver > resources in connected vpcs<above happens in shared services vpc>onpremaws vpca


29, a company is hosting a three-tier web application in an on-premises environment. due to a recent surge in traffic that resulted in downtime and a significant financial impact, company management has ordered that the application be moved to aws. the application is written in net and has a dependency on a mysql database. a solutions architect must design a scalable and highly available solution to meet the demand of 200, 000 daily userswhich steps should the solutions architect take to design an appropriate solution?
A. use aws elastic beanstalk to create a new application with a web server environment and an amazon rds mysql multi-az db instance. the environment should launch a network load balancer(nlb)in front of an amazon ec2 auto scaling group in multiple availability zones. use an amazon route 53 alias record to route traffic from the company's domain to the nib.
A. 
B. use aws cloudformation to launch a stack containing an application load balancer(alb)in front of an amazon ec2 auto scaling group spanning three availability zones. the stack should launch a multi-az deployment of an amazon aurora mysql db cluster with a retain deletion policy. use an amazon route 53 alias record to route traffic from the company's domain to the alb.
C. use aws elastic beanstalk to create an automatically scaling web server environment that spans two separate regions with an application load balancer(alb)in each region. create a multi-azz deployment of an amazon aurora mysql db cluster with a cross-region read replica. use amazon route 53 with a geoproximity routing policy to route traffic between the two regions.
D. use aws cloudformation to launch a stack containing an application load balancer(alb)in front of an amazon ecs cluster of spot instances spanning three availability zones. the stack should launch an amazon rds mysql db instance with a snapshot deletion policy. use an amazon route 53 alias record to route traffic from the company s domain to the alb.


 tfr: web app needs alb multi-az deployment should address ha retain deletion policy to not delete the db with the stack.c is wrong because it also mention a cross-region read replica, so the solution will mostly be unusable in read-only in one of the region.


30, a company is running several workloads in a single aws account. a new company policy stales that engineers can provision only approved resources and that engineers must use aws cloudformation to provision these resources. a solutions architect needs to create a solution to enforce the new restriction on the iam role that the engineers use for access.what should the solutions architect do to create the solution?
A. upload aws cloudformation templates that contain approved resources to an amazon s3 bucket.update the iam policy for the engineers' iam role to only allow access to amazon s3 and aws cloudformation. use aws cloudformation templates to provision resources.
B. update the iam policy for the engineers" iam role with permissions to only allow provisioning of approved resources and aws cloudformation.use aws cloudformation templates to create stacks with approved resources.
C. update the iam policy for the engineers' iam role with permissions to only allow aws cloudformation actions.create a new iam policy with permission to provision approved resources, and assign the policy to a new iam service role.using the iam service role to aws cloudformation during stack creation.
D. provision resources in aws cloudformation stacks.update the iam policy for the engineers' iam role to only allow access to their own aws cloudformation stack
A. 




3L a company has a microsoft sql server database in its data center and plans to migrate data to amazon aurora mysql. the company has already used the aws schema conversion tool migrate triggers, stored procedures, and other schema objects to aurora mysql. the database contains 1 tb of data and grows less than 1 mb per day. the company's data center is connected to aws through a dedicated 1 gbps aws direct connect connection.the company would like to migrate data to aurora mysql and perform reconfigurations with minimal downtime to the applications.which solution meets the company's requirements?
A. shut down applications over the weekend. create an aws dms replication instance and task to migrate existing data from sql server to aurora mysql. perform application testing and migrate the data to the new database endpoint.
B. create an aws dms replication instance and task to migrate existing data and ongoing replication from sql server to aurora mysql. perform application testing and migrate the data to the new database endpoint.
C. create a database snapshot of sql server on amazon s3. restore the database snapshot from amazon s3 to aurora mysql. create an aws dms replication instance and task for ongoing replication from sql server to aurora mysql.perform application testing and migrate the data to the new database endpoint.
D. create a sql server native backup file on amazon s3. create an aws dms replication instance and task to restore the sql server backup file to aurora mysql.another aws dms task for ongoing replication from sql server to aurora mysql. perform application testing and migrate the data to the new database endpoint.




32, a company is using an existing orchestration tool to manage thousands of amazon ec2 instance. a recent penetration test found a vulnerability in the company's software stack this vulnerability has prompted the company to perform a full evaluation of its current production environment. the analysis determined that the following vulnerability exist within the environment-operating systems with outdated libraries and known vulnerability are being used in production. -relational databases hosted and managed by the company
are running unsupported versions with known vulnerability.-data stored in databases is not encrypted.the solutions architect intends to use aws config to continuously audit and assess the compliance of the company's aws resource configurations with the company's policies and guidelines. what additional steps will enable the company to secure its environments and track resources while adhering to best practices?

A. use aws application discovery service to evaluate all running ec2 instances. use the aws di to modify each instance, and use ec2 user data to install aws systems manager agent during boot.schedule patching to run as a systems manager maintenance windows task. migrate all relational databases to amazon rds and enable aws kms encryption.
B. create an aws doudformation template for the ec2 instances. use ec2 user data in the doudformation template to install the aws systems manager agent, and enable aws kms encryption on all amazon ebs volumes. have doudformation replace all running instances. use systems manager patch manager to establish a patch baseline and deploy a systems manager maintenance windows task to execute aws-runpatchbaseline using the patch baseline.
C. install the aws systems manager agent on all existing instances using the company's current orchestration tool. use the systems manager run command to execute a list of commands to upgrade software on each instance using operatingsystem-specific tools. enable aws kms encryption on all amazon ebs volumes.
D. install the aws systems manager agent on all existing instances using the company's current orchestration tool. migrate all relational databases to amazon rds and enable aws kms encryption.use systems manager patch manager to establish a patch baseline and deploy a systems manager maintenance windows task to execute aws-runpatchbaseline using the patch baseline.



33,  a company has an application that runs on a fleet of amazon ec2 instances and stores 70 gb of device data for each instance in amazon s3. recently, some of the s3 uploads have been failing. at the same time, the company is seeing an unexpected increase in storage data costs. the application code cannot be modified.what is the most efficient way to upload the device data to amazon s3 while managing storage costs?
A. upload device data using a multipart upload. use the aws di to list incomplete parts to address the failed s3 uploads. enable the lifecyde policy for the incomplete multi part uploads on the s3 bucket to delete the old uploads and prevent new failed uploads from accumulating.
B. upload device data using s3 transfer acceleration: use the aws management console to address the failed s3 uploads. use the multi-object delete operation nightly to delete the old uploads
C. upload device data using a multipart upload. use the aws management console to list incomplete parts to address the failed s3 uploads. configure a lifecyde policy to archive continuously to amazon s3 glacier.
A. 
D. upload device data using s3 transfer acceleration.use the aws management console to list incomplete parts to address the failed s3 uploads. enable the lifecycle policy for the incomplete multi part uploads on the s3 bucket to delete the old uploads and prevent new failed uploads from accumulating.




34, a solutions architect is designing a web application on aws that requires 99.99% availability. the application will consist of a three-tier architecture that supports 300,000 web requests each minute when experiencing peak traffic. the application will use amazon route 53 for dns resolution, amazon cloudfront as the content delivery network(cdn),an elastic load balancer for load balancing. amazon ec2 auto scaling groups to scale the application tier, and amazon aurora mysql as the backend database. the backend database load will average 90% reads and 10% writes. the company wants to build a cost-effective solution, but reliability is critical.which set of strategies should the solutions architect use?
A. build the application in a single aws region. deploy the ec2 application layer to three availability zones using an auto scaling group with dynamic scaling based on request metrics. use a multi-az amazon aurora mysql db cluster with two aurora replicas. each aurora replica must have enough capacity to support 50% of the peak read queries.
B. build the application in a single aws region. deploy the ec2 application layer to three availability zones using an auto scaling group with a minimum desired capacity sufficient to process 450,000 requests each minute. use a multi- az amazon aurora mysql db cluster with two aurora replicas.each aurora replica must have enough capacity to support 100% of the peak read queries.
C. build the application in a single aws region. deploy the ec2 application layer to two availability zones using an auto scaling group with a minimum desired capacity sufficient to process 30.0000 requests each minute use a multi-az amazon aurora mysql db cluster with one aurora replica. the aurora replica must have enough capacity to support 50% of the peak read and write queries.
D. build the application in two aws regions. deploy the ec2 application layer to two availability zones using an auto scaling group with dynamic scaling based on the request metrics in each region in the second region,deploy an amazon aurora mysql cross-region replica,use amazon route 53 to distribute traffic between regions and configure failover if a region becomes unavailable.


35, a company hes several development teams collaborating on multiple projects.developers frequently move between projects, and each project requires access to a different set of aws resources. there are current projects for web,mobile, and database development however,the set of projects may change over time.developers should have full control over the resources for the project to which they are assigned, and read-only access to resources for all other projects.when developers are assigned to a different project or new aws resources are added. the company wants to maintenance.what type of control policy should a solutions architect recommend?
A. create a policy document for each project with specific project tags and allow full control of the resources with a matching tag. allow read only access for all other resources. attach the project- specific policy document to the iam role for that project. change the role assigned to the developer's iam user when they change projects. assign a specific project tag to new resources when they are created.
B. create an iam role for each project that requires access to aws resources. attach an inline policy document to the role,that specifies the iam users that are allowed to assume the role, with full control of the resources that belong to a project and read only access for all other resources within the account.update the policy document when the set of resources changes or developers change projects.
C. create a customer manage specify full control policy document for each project that requires access to aws resources. specify full control of the resources that belong to a projects and read -only access for all other resources within the account.attach the project­ specific policy document to the developer' s iam user when they change projects.update the policy document when the set of resources changes.
D. create a customer manage policy document for each project that requires access to aws resources specify full control of the resources that belong to a project and read-only access for all other resources within the account.attach the project-specific policy document to an iam group change the group membership when developer' s change projects update the policy document when the set to resources changes.



36, a company has a mobile app with users in europe. when the app is used, it downloads a configuration file that is device and app version-specific. the company has the following architecture:--configuration files are stored in amazon s3 in the eu-west-1 region and served to the users using amazon cloudfront.-- lambda@edge is used to extract the device and version information from the app requests. it then updates the requests to load the correct configuration.the company uses the configuration file load time as a key performance metric, and targets a response time of 100 ms or less. the app recently launched in the ap-southeast-2 region, and the latency for requests from users in australia is

significantly above the 100 ms target. a solutions architect needs to recommend a solution.which solution will reduce latency for users in australia?
A. create an s3 bucket in the ap-southeast-2 region. use cross-region replication to synchronize from the bucket in the eu-west-1 region. modify lambda@edge to access amazon s3 in the region that is closest to the user.
B. configure s3 transfer acceleration on the bucket.modify lambda@edge to access amazon s3 using the transfer acceleration endpoint in the region that is closest to the user.
C. configure s3 transfer acceleration on the bucket. add the transfer acceleration edge endpoints for australia and europe as cloudfront origins. modify lambda@edge to update the origin of the request to be the transfer acceleration endpoint in the region that is closest to the user.
D. create an s3 bucket in the ap-southeast-2 region. use cross-region replication to synchronize from the bucket in the eu-west-1 region.create an amazon route 53 hosted zone with latency-based routing configured for both buckets.modify lambda@edge to update the origin of the request to be the route 53 hosted zone that is closest to the user.



37, a company has a web application that allows users to upload short videos. the videos are stored on amazon ebs volumes and It-tfr:d by custom recognition software for categorization. the website contains static content that has variable traffic with peaks in certain months. the architecture consists of amazon ec2 instances running in an auto scaling group for the web application and ec2 instances running in an auto scaling group to process an amazon sqs queue. the company wants to re- architect the application to reduce
operational overhead using aws managed services.where possible and remove dependencies on third-party software.which solution meets these requirements?
A. use amazon ecs containers for the web application and spot instances for the auto scaling group that processes the sqs queue. replace the custom software with amazon rekognition to categorize the videos.
B. store the uploaded videos in amazon efs and mount the file system to the ec2 instances for the web application. process the sqs queue with an aws lambda function that calls the amazon rekognition api to categorize the videos.
C. host the web application in amazon s3. store the uploaded videos in amazons s3. use s3 event notifications to publish events to the sqs queue. process the sqs queue with an aws lambda function that calls the amazon rekognition api to categorize the videos.
A. 
D. use aws elastic beanstalk to launch ec2 instances in an auto scaling group for the web application and launch a worker environment to process the sqs queue. replace the custom software with amazon rekognition to categorize the videos.



38, an ecommerce company has an order processing application it wants to migrate to aws. the application has inconsistent data volume patterns, but needs to be available at all times. orders must be processed as they occur and in the order that they are received.which set of steps should a solutions architect take to meet these requirements?
A. use aws transfer for sftp and upload orders as they occur.use on- demand instances in multiple availability zones for processing.
B. use amazon sns with fifo and send orders as they occur.use a single large reserved instance for processing.
C. use amazon sqs with fifo and send orders as they occur.use reserved instances in multiple availability zones tor processing.
D. use amazon sqs with fifo and send orders as they cc use spot instances in multiple availability zones for processing.



39, the company user aws organizations with a single ou named production to manage multiple accounts.all accounts are members of the production ou administrators use deny list scps in the root of the organization to manage access to restricted services.the company recently acquired a new business unit and invited the new unit' s existing aws account to the organization once on boarded,the administrators of the new business unit discovered that they are not able to update existing aws config rules to meet the company' s policies.which option will allow administrators to make changes and continue to enforce the current policies without introducing additional long-term maintenance?
A. remove the organization's root scps that limit access to aws config. create aws service catalog products tor the company's standard aws config rules and deploy them throughout the organization, including the new account.
B. create a temporary ou named onboarding for the new account apply an scp to the onboarding ou to allow aws config actions. move the new account to the production ou when adjustments to aws config are complete.
A. 
C. convert the organization's root scps from deny list scps to allow list scps to allow the required services only. temporarily apply an scp to the organization's root that allows aws config actions for principals only in the new account.
D. create a temporary ou named onboarding for the new account apply an scp to the onboarding ou to allow aws config actions. move the organization's root scp to the production ou move the new account to the production ou when adjustments to aws config are complete.




40, a financial services company receives a regular data feed from its credit card servicing partner.approximately 5,000 records are sent every 15 minutes in plaintext, delivered over https directly into an amazon s3 bucket with server-side encryption .this feed contains sensitive credit card primary account number (pan) data. the company needs to automatically mask the pan before sending the data to another s3 bucket for additional internal processing.the company also needs to remove and merge specific fields, and then transform the record into json format.additionally,extra feeds are likely to be added in the future ,so any design needs to be easily expandable. which solutions will meet these requirements?
A. trigger an aws lambda function on file delivery that extracts each record and writes it to an amazon sqs queue.trigger another lambda function when new messages arrive in the sqs queue to process the records, writing the results to a temporary location in amazon
s3 .trigger a final lambda function once the sqs queue is empty to transform the records into json format and send the results to another s3 bucket tor internal processing.
B. trigger an aws lambda function on file delivery that extracts each record and writes it to an amazon sqs queue configure an aws fargate container application to automatically scale to a single instance when the sqs queue contains messages. have the application process each record. and transform the record into json format. when the queue is empty, send the results to another s3 bucket for internal processing and scale down the aws fargate instance.
C. create an aws glue crawler and custom classifier based on the data feed formats and build a table definition to match.trigger an aws lambda function on file delivery to start an aws glue etl job to transform the entire record according to the processing and transformation requirements. define the output format as json .once complete, have the etl job send the results to another s3 bucket for internal processing.
D. create an aws glue crawler and custom classifier based upon the data feed formats and build a table definition to match. perform an amazon athena query on file delivery to start an amazon emr etl job to transform the entire record according to the processing and transformation requirements. define the output format as json .once complete, send the results to another s3 bucket for internal processing and scale down the emr cluster.
A. 




4L a company plans to refactor a monolithic application into a modern application design deployed on aws. the ci/cd pipeline needs to be upgraded to support the modern design for the application with the following requirements:--it should allow changes to be released several times every hour. --it should be able to roll back the changes as quickly as possible.which design will meet these requirements?
A. deploy a ci/cd pipeline that incorporates amis to contain the application and their configurations.deploy the application by replacing amazon ec2 instances.
B. specify aws elastic beanstalk to stage in a secondary environment as the deployment target for the ci/ cd pipeline of the application. to deploy, swap the staging and production environment urls.
C. use aws systems manager to re-provision the infrastructure for each deployment update the amazon ec2 user data to pull the latest code artifact from amazon s3 and use amazon route 53 weighted routing to point to the new environment.
D. roll out the application update as part of an auto scaling event using prebuilt amis.use new versions of the amis to add instances,and phase out all instances that use the previous ami version with version with the configured termination policy during a deployment event.




42, a company requires that all internal application connectivity use private ip addresses. to facilitate this policy, a solutions architect has created interface endpoints to connect to aws public services. upon testing, the solutions architect notices that the service names are resolving to public ip addresses, and that internal services cannot connect to the interface endpoints. which step should the solutions architect take to resolve this issue?
A. update the subnet route table with a route to the interface endpoint
B. enable the private dns option on the vpc attributes.
C. configure the security group on the interface endpoint to allow connectivity to the aws services.
D. configure an amazon route 53 private hosted zone with a conditional forwarder for the internal application.


43, a company operates pipelines across north america and south america.the company assesses pipeline inspection gauges with imagery and ultrasonic sensor data to monitor the condition of its pipelines.the pipelines are in areas with intermittent or unavailable internet connectivity.the imagery data at each site requires terabytes of storage each month.the company wants a solution to collect the data at each site in monthly intervals and to store the data with high durability.the imagery captured must be preprocessed and uploaded to a central location for persistent storage.which actions should a solutions architect take to meet these requirements?
A. deploy aws snowball devices at local sites in a cluster configuration.configure aws lambda for preprocessing.ship the devices back to the closest aws region and store the data in amazon s3 buckets.
B. deploy aws snowball edge devices at local sites in a cluster configuration. configure aws lambda for preprocessing. ship the devices back to the closest aws region and store the data in amazon s3 buckets.
C. deploy aws iot greengrass on eligible hardware across the sites. configure aws lambda on the devices for preprocessing. upload the processed data to amazon s3 buckets in aws regions closest to the sites.
D. deploy aws iot greengrass on eligible hardware across the sites. configure aws lambda on the devices for preprocessing. ship the devices back to the closest aws region and store the data in amazon s3 buckets.




44, a company uses a load balancer to distribute traffic to amazon ec2 instances in a single availability zone.the company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements.-inbound requests must be filtered for common vulnerability attacks. -rejected requests must be sent to a third- party auditing application.-all resources should be highly available. which solution meets these requirements?
A. configure a multi-az auto scaling group using the application's ami create an application load balancer (alb) and select the previously created auto scaling group as the target. use amazon inspector to monitor traffic to the alb and ec2 instances. create a web acl in waf. create an aws waf using the web acl and alb. use an aws lambda function to frequently push the amazon inspector report to the third-party auditing application.
B. configure an application load balancer (alb) and add the ec2 instances as targets, create a web acl in waf create an aws wafusing the web acl and alb name and enable logging with amazon cloudwatch logs. use an aws lambda function to frequently push the logs to the third-party auditing application.
A. 
C. configure an application load balancer (alb) along with a target group adding the ec2 instances as targets. create an amazon kinesis data firehose with the destination of the
third-party auditing application. create a web acl in waf create an aws waf using the web,acl and alb then enable logging by selecting the kinesis data firehose as the destination. subscribe to aws managed rules in aws marketplace, choosing the waf as the subscriber.
D. configure a multi-az auto scaling group using the application's ami.create an application load balancer (alb) and sect the previously created auto scaling group as the target .create an amazon kinesis data firehose with a destination of the third-party auditing application.create a web acl in waf create an aws waf using the webacl and alb then enable logging by selecting the kinesis data firehose as the destination subscribe to aws managed rules in aws marketplace,choosing the waf as the subscriber.




45, a solutions architect is migrating an existing workload to aws fargate.the task can only run in a private subnet within the vpc where there is no direct connectivity from outside the system to the application.when the fargate task is launched. the task fails with the following error:cannotpullcontainererror: api error (500): get http://111122223333.dkr.ecr.us-east- 1.amazonaws.com/v2/: net/http: request canceled while waiting for connection.how should the solutions architect correct this error?
A. ensure the task is set to enabled for the auto-assign public ip setting when launching the task
B. ensure the task is set to disabled for the auto-assign public ip setting when launching the task.configure a nat gateway in the public subnet in the vpc to route requests to the internet
C. ensure the task is set to disabled for the auto-assign public ip setting when launching the task.configure a nat gateway in the private subnet in the vpc to route requests to the internet
D. ensure the network mode is set to bridge in the fargate task definition.

Mi-fr: there is no difference between configuring and creating as far as this question is concerned. and we all know that nat gateway needs to be created in a public subnet. it needs to be accessed from the private subnet via a route table attached to it that routes outbound traffic to the nat gateway which is in the public subnet and from there to the internet via the internet gateway attached to the vpc.

46, a company is planning to migrate an application from on-premises to aws. the application currently uses an oracle database and the company can tolerate a brief downtime of 1 hour when performing the switch to the new infrastructure. as part of the migration, the database engine will be changed to mysql. a solutions architect needs to determine which aws services can be used to perform the migration while minimizing the amount of work and time required.which of the following will meet the requirements?
A. use aws set to generate the schema scripts and apply them on the target prior to migration. use aws dms to analyse the current schema and provide a recommendation for the optimal database engine. then, use aws dms to migrate to the recommended enginer.use aws set to identify what embedded sql code in the application can be converted and what has to be done manually.
B. use aws set to generate the schema scripts and apply them on the target prior to migration. use aws dms to begin moving data from the on-premises database to aws.after the initial copy, continue to use aws dms to keep the databases insync until cutting over to the new database. use aws set to identify what embedded sql code in the application can be converted and what has to be done manually.
C. use aws dms to help identify the best target deployment between installing the database engine on amazon ec2 dierctly or moving to amazon rds. then, use aws dms to migrate to the platform. use aws application discovery service to identify what embedded sql code in the application can be converted and what has to be done manually.
D. use aws dms to begin moving data from the on-premises database to aws. after the intitial copy, continue to use aws dms to keep the databases in sync until cutting over to the new database. use aws application discovery service to identify what embedded sql code in the application can be converted and what has to be done manually.


tfr: a by default the engine will always be
innodbhttps:/ / docs.aws.amazon.com/dms/latest/userguide/chap_target.mysql.html b https:// docs.aws.amazon.com/schemaconversiontool/latest/userguide/chap_converting.app.html https:// aws.amazon.com/dms/faqs/c application discovery service is just not used for this....


47, a mobile app developer just made an app in both ios and android that has a feature to count step numbers. he has used aws cognito to authorize users with a user pool and identity pool to provide access to aws dynamodb table. the app uses the dynamodb table to store user subscriber data and number of steps. now the developer also needs cognito to integrate with google to provide federated authentication for the mobile application users

so that user does not need to remember extra login access.what should the developer do to make this happen for the ios and android app?
A. amazon cognito identity pools (federated identities) support user authentication through federated identity providers-including amazon, facebook, google, and saml identity providers. the developer just needs to set up the federated identities for google access
B. only android works for federated identities if google access is required for aws cognito.this can be done by configuring cognito identity pools with a google client id.
C. amazon cognito user pools support user authentication through federated identity providersincluding amazon, facebook, google, and saml identity providers. the developer just needs to set up the federated identities for google access in cognito user pool.
D. only ios (objective-c and swift) works for federated identities if google access is required for aws cognito. thiscan be done by configuration cognito identity pools with a google client id. google federated access does not work for android app.




48, a photo-sharing and publishing company receives 10,000 to 150,000 images daily. the company receives the images from multiple suppliers and users registered with the service. the company is moving to aws and wants to enrich the existing metadata by adding data using amazon rekognition. the following is an example of the additional data:as part of the cloud migration program, the company uploaded existing image data to amazon s3 and told users to upload images directly to amazon s3. what should the solutions architect do to support these requirements?
A. trigger aws lambda based on an s3 event notification to create additional metadata using amazon rekognition. use amazon dynamodb to store the metadata and amazon es to create an index. use a web front-end to provide search capabilities backed by amazon es.
B. use amazon kinesis to stream data based on an s3 event. use an application running in amazon ec2 to extract metadata from the images. then store the data on amazon dynamodb and amazon cloudsearch and create an index. use a web front-end with search capabilities backed by cloudsearch.
C. start an amazon sqs queue based on s3 event notifications. then have amazon sqs send the metadata information to amazon dynamodb. an application running on amazon ec2 extracts data from amazon rekognition using the api and adds data to dynamodb and amazon es. use a web front-end to provide search capabilities backed by amazon es.
D. trigger aws lambda based on an s3 event notification to create additional metadata using amazon rekognition. use amazon rds mysql multi-az to store the metadata information and
A. 
use lambda to create an index. use a web front-end with search capabilities backed by lambda.

i tfr: b kinesis cannot stream data from s3 event directly, and ec2 by itself is not the best scalable solution c sqs cannot send data to dynamodb directly, too many missing pieces here d lambda cannot back a search, and mysql is not the best to store metadata for search


49, a company has a legacy application running on servers on premises. to increase the application's reliability, the company wants to gain actionable insights using application logs. a solutions architect has been given following requirements for the solution:-aggregate logs using aws.-automate log analysis for errors.-notify the operations team when errors go beyond a specified threshold. what solution meets the requirements?
A. install amazon kinesis agent on servers, send logs to amazon kinesis data streams and use amazon kinesis dataanalytics to identify errors, create an amazon cloudwatch alarm to notify the operations team of errors
B. install an aws x-ray agent on servers, send logs to aws lambda and infr: them to identify errors, use amazon cloudwatch events to notify the operations team of errors.
C. install logstash on servers, send logs to amazon s3 and use amazon athena to identify errors, use sendmail to notify the operations team of errors.
D. install the amazon cloudwatch agent on servers, send logs to amazon cloudwatch logs and use metric filters to identify errors, create a cloudwatch alarm to notify the operations team of errors.

Mtfr: https:/ / docs.aws.amazon.com/amazoncloudwatch/latest/monitoring/install­ cloudwatch-agent-on- premise.html


50, a big company has a service to process gigantic clickstream data sets which are often the result of holiday shopping traffic on a retail website, or sudden dramatic growth on the data network of a media or social networking site. it is becoming more and more expensive
to Mtfr: these clickstream datasets for its on- premise infrastructure. as the sample data set
keeps growing, fewer applications are available to provide a timely response. the service is using a hadoop cluster with cascading. how can they migrate the applications to aws in the best way?

A. put the source data to s3 and migrate the processing service to an aws emr hadoop cluster with cascading. enable emr to directly read and query data from s3 buckets. write the output to rds database
B. put the source data to a kinesis stream and migrate the processing service to aws lambda to utilize its scaling feature. enable lambda to directly read and query data from kinesis stream. write the output to rds database
C. put the source data to a s3 bucket and migrate the processing service to aws ec2 with auto scaling.ensure that the auto scaling configuration has proper maximum and minimum number of instances.monitor the performance in cloudwatch dashboard. write the output to dynamodb table for downstream to process.
D. put the source data to a kinesis stream and migrate the processing service to an aws emr cluster with cascading. enable emr to directly read and query data from kinesis streams. write the output to redshift.


 tfr: for large dataset, it makes sense to use kinesis with emr. if we just store the dataset in s3, it makes not much differences than the on prem solution.


51, a solutions architect must update an application environment within aws elastic beanstalk using a blue/ green deployment methodology. the solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.what should be done next to complete the update?
A. redirect to the new environment using amazon route 53
B. select the swap environment urls option
C. replace the auto scaling launch configuration
D. update the dns records to point to the green environment


Mtfr: https:// docs.aws.amazon.com/elasticbeanstalk/latest/dg/using­ features.cnameswap.html


52, a solutions architect is redesigning an image-viewing and messaging platform to be delivered as saas. currently, there is a farm of virtual desktop infrastructure (vdi) that runs a desktop imageviewing application and a desktop messaging application. both applications use a shared database to manage user accounts and sharing. users log in from a web portal

that launches the applications and streams the view of the application on the user's machine. the development operations team wants to move away from using vdi and wants to rewrite the application.what is the most cost-effective architecture that offers both security and
ease of management?
A. run a website from an amazon s3 bucket with a separate s3 bucket for images and messaging data.call aws lambda functions from embedded javascript to manage the dynamic content, and use amazon cognito for user and sharing management.
B. run a website from amazon ec2 linux servers, storing the images in amazon s3, and use amazon cognito for user accounts and sharing. create aws cloudformation templates to launch the application by using ec2 user data to install and configure the application.
C. run a website as an aws elastic beanstalk application, storing the images in amazon s3, and using an amazon rds database for user accounts and sharing. create aws cloudformation templates to launch the application and perform blue/green deployments.
D. run a website from an amazon s3 bucket that authorizes amazon appstream to stream applications for a combined image viewer and messenger that stores images in amazon s3.have the website use an amazon rds database for user accounts and sharing.

 tfr: a - correct - solution will work and with low cost and management. no infrastructure to manage. b - incorrect - cost of running and managing infrastructure expensive - not easy to maintain c - incorrect - cost of running and managing infrastructure expensive - blue/green more so which requires the database to be external to the environment or data will be lost. d - incorrect - rds for authentication/authorisation to provide secure access to s3? possible? plus cost of running infrastructure, and appstream is the same tech as the current streaming solutionlight readinghttps://stackoverflow.com/questions/49782492/cognito-user-authorization-to­ access-an-s3-object
https:/ / docs.aws.amazon.com/iam/latest/ userguide / reference_policies_examples_s3_cogni to-bucket.html


53, an artificial intelligence startup company has used lots of ec2 instances. some instances use sql server database while the others use oracle. as the data needs to be kept secure, regular snapshots are required. theywant sql server ebs volume to take snapshot every 12 hours. however for oracle, it only needs a snapshot every day. which option below is the best one that the company should choose without extra charge?
A. use free third-party tool such as dive to manage ec2 instance lifecycle. it can design various backup policies for ec2 ebs volumes. add a 12 hours backup policy to sql server ebs volumes and a 24 hours backup policy to oracle ebs volumes.
A. 
B. add a prefix to the name of both sql server and oracle ebs volumes. in aws data lifecycle management console, create two management policies based on the name prefix.for example, add a 12 hours backup schedule to ebs volumes with a name starting with sql and add a 24 hours backup schedule to ebs volumes with a name starting with oracle?
C. create a dedicate lambda function to differentiate ec2 ebs volumes and take snapshots.set up cloudwatch events rules to call the lambda so that the function runs every 12 hours for sql server and 24 hours for oracle.
D. add different tags for sql server and oracle ebs volumes. in aws data lifecycle management console, create two management policies based on the tags. add a 12 hours schedule to sql server lifecycle policy and a 24 hours schedule to oracle lifecycle policy




54, a company has implemented aws organizations. it has recently set up a number of new accounts and wants to deny access to a specific set of aws services in these new accounts.how can this be controlled most efficiently?
A. create an iam policy in each account that denies access to the services. associate the policy with an iam group, and add all iam users to the group.
B. create a service control policy that denies access to the services. add all of the new accounts to a single organizations unit (ou), and apply the policy to that ou.
C. create an iam policy in each account that denies access to the service. associate the policy with an iam role, and instruct users to log in using their corporate credentials and assume the iam role.
D. create a service control policy that denies access to the services, and apply the policy to the root of the organization.




55, a bank is re-architecting its mainframe-based credit card approval processing application to a cloud- native application on the aws cloud. the new application will receive up to 1,000 requests per second at peak load. there are multiple steps to each transaction, and each step must receive the result of the previous step. the entire request must return an authorization response within less than 2 seconds with zero data loss. every request must receive a response. the solution must be payment card industry data security standard (pci dss)-compliant.which option will meet all of the bank's objectives with the least complexity and lowest cost while also meeting compliance requirements?

A. create an amazon api gateway to process inbound requests using a single aws lambda task that performs multiple steps and returns a json object with the approval status. open a support case to increase the limit for the number of concurrent lambdas to allow room for bursts of activity due to the new application.
B. create an application load balancer with an amazon ecs cluster on amazon ec2 dedicated instances in a target group to process incoming requests. use auto scaling to scale the cluster out/in based on average cpu utilization. deploy a web service that processes all of the approval steps and returns a json object with the approval status.
C. deploy the application on amazon ec2 on dedicated instances. use an elastic load balancer in front of a farm of application servers in an auto scaling group to handle incoming requests. scale out/in based on a custom amazon cloudwatch metric for the number of inbound requests per second after measuring the capacity of a single instance.
D. create an amazon api gateway to process inbound requests using a series of aws lambda processes, each with an amazon sqs input queue. as each step completes, it writes its result to the next step's queue. the final step returns a json object with the approval status. open a support case to increase the limit for the number of concurrent lambdas to allow room for bursts of activity due to the new application.


tfr: this is the only one that guarantee O data loss



56,  a company is migrating an application to aws. it wants to use fully managed services as much as possible during the migration. the company needs to store large, important documents within the application with the following requirements:-the data must be highly durable and available.-the data must always be encrypted at rest and in transit-the encryption key must be managed by the company and rotated periodically. which of the following solutions should the solutions architect recommend?
A. deploy the storage gateway to aws in file gateway mode. use amazon ebs volume encryption using an aws kms key to encrypt the storage gateway volumes.
B. use amazon s3 with a bucket policy to enforce https for connections to the bucket and to enforce server-side encryption and aws kms for object encryption.
C. use amazon dynamodb with ssl to connect to dynamodb. use an aws kms key to encrypt dynamodb objects at rest.
D. deploy instances with amazon ebs volumes attached to store this data. use ebs volume encryption using an aws kms key to encrypt the data.


 tfr: a file gateway link to s3, need to encrypt s3 as well d you may not be able to rotate the key


57, a solutions architect is designing a highly available and reliable solution for a cluster of amazon ec2 instances.the solutions architect must ensure that any ec2 instance within the cluster recovers automatically aftert a system failure. the solution must ensure that the recovered instance maintains the same ip address.how can these requirements be met?
A. create an aws lambda script to restart any ec2 instances that shut down unexpectedly.
B. create an auto scaling group for each ec2 instance that has a minimum and maximum size of 1.
C. create a new t2.micro instance to monitor the cluster instances. configure the t2.micro instance to issue an aws ec2 reboot-instances command upon failure.
D. create an amazon cloudwatch alarm for the statuscheckfailed_system metric, and then configure an ec2 action to recover the instance.


 tfr: b private ip could changed statuscheckfailed_system is for system wise problem, and this is what the question is asking for https:/ / docs.aws.amazon.com/awsec2/latest/userguide/monitoring-system-instance-status­ check.html#types- of-instance- status-checks


58, a large company starts to use aws organizations with consolidated billing feature to manage its separate departments. the aws operation team has just created 3 ous (organization units) with 2 aws accounts each. to be compliant with company-wide security policy, cloudtrail is required for all aws accounts which is already been set up. however after some time, there are cases that users in certain ou have turned off the cloudtrail of their accounts. what is the best way for the aws operation team to prevent this from happening again?
A. update the aws organizations feature sets to features?and then create a service control policies (scp) to prevent users from disabling aws cloudtrail. this can be achieved by a deny policy with cloudtrail:stoplogging denied.
B. this can be achieved by service control policies (scp) in features?set. the team needs to delete and recreate the aws organizations with features?enabled and then use a proper control policy to limit the operation of cloudtrail:stoplogging.
A. 
C. in each aws account in this organization, create an iam policy to deny cloudtrail:stoplogging for all users including administrators.
D. use a service control policies (scp) to prevent users from disabling aws cloudtrail. this can be done by a allow policy which denies cloudtrail:stoplogging

Mi-fr: organisation do not need to be recreated to enable all features set.
https:/ / docs.aws.amazon.com/ organizations/latest/userguide/ orgs_manage_org_support­ all-features.html


59, a solutions architect is migrating a 10 tb postgresql database to amazon rds for postgresql. the company's internet link is 50 mb with a vpn in the amazon vpc, and the solutions architect needs to migrate the data and synchronize the changes before the cutover. the cutover must take place within an 8- day period. what is the least complex method of migrating the database securely and reliably?
A. order an aws snowball device and copy the database using the aws dms. when the database is available in amazon 3, use aws dms to load it to amazon rds, and configure a job to synchronize changes before the cutover.
B. create an aws dms job to continuously replicate the data from on premises to aws. cutover to amazon rds after the data is synchronized.
C. order an aws snowball device and copy a database dump to the device. after the data has been copied to amazon s3, import it to the amazon rds instance. set up log shipping over a vpn to synchronize changes before the cutover.
D. order an aws snowball device and copy the database by using the aws schema conversion tool.when the data is available in amazon s3, use aws dms to load it to amazon rds, and configure a job to synchronize changes before the cutover.

 tfr: the internet connection could take more than 8 days to migrate https:/ / docs.aws.amazon.com/dms/latest/  userguide / chap_largedbs.html


60, your company has a logging microservice which is used to generate logs when users have entered certain commands in another application. this logging service is implemented via an sqs standard queue that an ec2 instance is listening to. however, you have found that on some occasions, the order of the logs are not maintained. as a result, it becomes harder to use this service to trace users' activities. how should you fix this issue in a simple way?

A. convert the existing standard queue into a fifo queue. add a deduplication id for the messages that are sent to the queue.
B. delete the existing standard queue and recreate it as a fifo queue. as a result, the order for the messages to be received is ensured.
C. migrate the whole microservice application to swf so that the operation sequence is guaranteed.
D. the wrong order of timestamps is a limitation of sqs, which does not have a fix.

It-tfr: a can't do
thishttps:/ / docs.aws.amazon.com/awssimplequeueservice/latest/sqsdeveloperguide/ fifo­ queues.html#fifo-queues- moving


6L	a company that provides wireless services needs a solution to store and ntr: log files about user activities. currently, log files are delivered daily to amazon linux on amazon ec2 instance. a batch script is run once a day to aggregate data used for analysis by a third-party tool. the data pushed to the thirdparty tool is used to generate a visualization for end users. the batch script is cumbersome to maintain, and it takes several hours to deliver the ever­ increasing data volumes to the third-party tool. the company wants to lower costs, and is open to considering a new tool that minimizes development effort and lowers administrative overhead. the company wants to build a more agile solution that can store and perform the analysis in near-real time, with minimal overhead. the solution needs to be cost effective and scalable to meet the company's end-user base growth.which solution meets the company's requirements?
A. develop a python script to failure the data from amazon ec2 in real time and store the data in amazon s3. use a copy command to copy data from amazon s3 to amazon redshift.connect a business intelligence tool running on amazon ec2 to amazon redshift and create the visualizations.
B. use an amazon kinesis agent running on an ec2 instance in an auto scaling group to collect and send the data to an amazon kinesis data forehose delivery stream. the kinesis data firehose delivery stream will deliver the data directly to amazon es. use kibana to visualize the data.
C. use an in-memory caching application running on an amazon ebs-optimized ec2 instance to capture the log data in near real-time. install an amazon es cluster on the same ec2 instance to store the log files as they are delivered to amazon ec2 in near real-time.install a kibana plugin to create the visualizations.
A. 
D. use an amazon kinesis agent running on an ec2 instance to collect and send the data to an amazon kinesis data firehose delivery stream. the kinesis data firehose delivery stream will deliver the data to amazon s3. use an aws lambda function to deliver the data from amazon s3 to amazon es. use kibana to visualize the data.


 tfr: a python script will be become the hard part to maintain c too many ec2s, very expensived firehose can deliver to es direclyhttps://docs.aws.amazon.com/elasticsearch­ service/latest/developergui de/ es-aws-integrations.h tml


62, a company runs a public-facing application that uses a java-based web service via a restful api. it is hosted on apache tomcat on a single server in a data center that runs consistently at 30% cpu utilization. use of the api is expected to increase by 10 times with a new product launch. the business wants to migrate theapplication to aws with no disruption, and needs it to scale to meet demand. the company has already decided to use amazon
route 53 and cname records to redirect traffic. how can these requirements be met with the least amount of effort?
A. use aws elastic beanstalk to deploy the java web service and enable auto scaling. then switch the application to use the new web service.
B. lift and shift the apache server to the cloud using aws sms. then switch the application to direct web service traffic to the new instance.
C. create a docker image and migrate the image to amazon ecs. then change the application code to direct web service queries to the ecs container.
D. modify the application to call the web service via amazon api gateway. then create a new aws lambda java function to run the java web service code. after testing, change api gateway to use the lambda function.

Mtfr: a this is best as replatform makes senseb re-host may not improve muchc will need load balancer and auto scaling..d a lot of work as this is re-architect


63,  a company is moving a business-critical, multi-tier application to aws. the architecture consists of a desktop client application and server infrastructure. the server infrastructure resides in an on-premises data center that frequently fails to maintain the application uptime sla of 99.95%. a solutions architect must re- architect the application to ensure that it can meet or exceed the sla. the application contains a postgresql database running on a single virtual machine. the business logic and presentation layers are load balanced

between multiple virtual machines. remote users complain about slow load times while using this latency- sensitive application.which of the following will meet the availability requirements with little change to the application while improving user experience and minimizing costs?
A. migrate the database to a postgresql database in amazon ec2. host the application and presentation layers in automatically scaled amazon ecs containers behind an application load balancer. allocate an amazon workspaces workspace for each end user to improve the user experience.
B. migrate the database to an amazon rds aurora postgresql configuration. host the application and presentation layers in an auto scaling configuration on amazon ec2 instances behind an application load balancer. use amazon appstream 2.0 to improve the user experience.
C. migrate the database to an amazon rds postgresql multi-az configuration. host the application and presentation layers in automatically scaled aws fargate containers behind a network load balancer.use amazon elasticache to improve the user experience.
D. migrate the database to an amazon redshift cluster with at least two nodes. combine and host the application and presentation layers in automatically scaled amazon ecs containers behind an application load balancer. use amazon cloudfront to improve the user experience.

Mi-fr: a database in ec2 may not be the best optionc using elasticache will require some changes to the application d redshift not design for this. even though it may work, the price is high.


64, a company currently uses a single 1 gbps aws direct connect connection to establish connectivity between an aws region and its data center. the company has five amazon vpcs, all of which are connected to the data center using the same direct connect connection. the network team is worried about the single point of failure and is interested in improving the redundancy of the connections to aws while keeping costs to a minimum. which solution would improve the redundancy of the connection to aws while meeting the cost requirements?
A. provision another 1 gbps direct connect connection and create new vifs to each of the vpcs.configure the vifs in a load balancing fashion using bgp.
B. set up vpn tunnels from the data center to each vpc. terminate each vpn tunnel at the virtual private gateway (vgw) of the respective vpc and set up bgp for route management.
A. 
C. set up a new point-to-point multiprotocol label switching (mpls) connection to the aws region that's being used. configure bgp to use this new circuit as passive, so that no traffic flows through this unless the aws direct connect fails.
D. create a public vif on the direct connect connection and set up a vpn tunnel which will terminate on the virtual private gateway (vgw) of the respective vpc using the public vif.use bgp to handle the failover to the vpn connection.


 tfr: a vif is not vgw, it is associated to direct connecthttps://aws.amazon.com/premiumsupport/knowledge-center/public-private­ interface-dx/ c mpls still go through direct
connecthttps: / / aws.amazon.com/answers /networking/ aws-network-connectivity-over­ mpls/ d you don't need a public vifunless you need to connect to aws public service, and you don't need public vif for vpn connection


65, a group ofresearch institutions and hospitals are in a partnership to study 2 pbs of genomic data. the institute that owns the data stores it in an amazon s3 bucket and updates it regularly. the institute would like to give all of the organizations in the partnership read access to the data. all members of the partnership are extremely cost-conscious, and the institute that owns the account with the s3 bucket is concerned about covering the costs for requests and data transfers from amazon s3. which solution allows for secure datasharing without causing the institute that owns the bucket to assume all the costs for s3 requests and data transfers?
A. ensure that all organizations in the partnership have aws accounts. in the account with the s3 bucket, create a cross- account role for each account in the partnership that allows read access to the data.have the organizations assume and use that read role when accessing the data.
B. ensure that all organizations in the partnership have aws accounts. create a bucket policy on the bucket that owns the data. the policy should allow the accounts in the partnership read access to the bucket. enable requester pays on the bucket. have the organizations use their aws credentials when accessing the data.
C. ensure that all organizations in the partnership have aws accounts. configure buckets in each of the accounts with a bucket policy that allows the institute that owns the data the ability to write to the bucket.periodically sync the data from the institute's account to the other organizations. have the organizations use their aws credentials when accessing the data using their accounts.
D. ensure that all organizations in the partnership have aws accounts. in the account with the s3 bucket, create a cross- account role for each account in the partnership that allows
A. 
read access to the data.enable requester pays on the bucket. have the organizations assume and use that read role when accessing the data.


 tfr: a the organization that owns the data will pay for everything c this will cause double charge: write and readd account that owns the assumed role will be charged with requester pays.. https://
docs.aws.amazon.com/amazons3/latest/ dev/requesterpaysbuckets.htmlhttps:/ / amazona ws-china.com/ en/premiumsu pport/knowledge-center / s3-cross-account-a ccess-denied/


66, a company is migrating its on-premises build artifact server to an aws solution. the current system consists of an apache http server that serves artifacts to clients on the local network, restricted by the perimeter firewall. the artifact consumers are largely build automation scripts that download artifacts via anonymous http, which the company will be unable to modify within its migration timetable. the company decides to move the solution to amazon s3 static website hosting. the artifact consumers will be migrated to amazon ec2 instances located within both public and private subnets in a virtual private cloud (vpc). which solution will permit the artifact consumers to download artifacts without modifying the existing automation scripts?
A. create a nat gateway within a public subnet of the vpc. add a default route pointing to the nat gateway into the route table associated with the subnets containing consumers.configure the bucket policy to allow the s3:listbucket and s3:getobject actions using the condition ipaddress and the condition key aws:sourceip matching the elastic ip address if the nat gateway.
B. create a vpc endpoint and add it to the route table associated with subnets containing consumers.configure the bucket policy to allow s3:listbucket and s3:getobject actions using the condition stringequals and the condition key aws:sourcevpce matching the identification of the vpc endpoint.
C. create an iam role and instance profile for amazon ec2 and attach it to the instances that consume build artifacts.configure the bucket policy to allow the s3:listbucket and s3:getobjects actions for the principal matching the iam role created.
D. create a vpc endpoint and add it to the route table associated with subnets containing consumers.configure the bucket policy to allow s3:listbucket and s3:getobject actions using the condition ipaddress and the condition key aws:sourceip matching the vpc cidr block


 tfr: a this will go through the public internet, apparently not the best option c instances in private subnet cannot access the bucketd for s3 with vpc endpoint, you cannot use
 
sourceip with vpc cidr block https://docs.aws.amazon.com/ vpc/latest/userguide/vpc­ endpoints-s3.html


67, an organization has two amazon ec2 instances:*the first is running an ordering application and an inventory application.*the second is running a queuing system.during certain times of the year, several thousand orders are placed per second. some orders were lost when the queuing system was down. also, the organization's inventory application has the incorrect quantity of products because some orders were processed twice. what should be done to ensure that the applications can handle the increasing number of orders?
A. put the ordering and inventory applications into their own aws lambda functions. have the ordering application write the messages into an amazon sqs fifo queue.
B. put the ordering and inventory applications into their own amazon ecs containers and create an auto scaling group for each application. then, deploy the message queuing server in multiple availability zones.
C. put the ordering and inventory applications into their own amazon ec2 instances, and create an auto scaling group for each application. use amazon sqs standard queues for the incoming orders, and implement idempotency in the inventory application.
D. put the ordering and inventory applications into their own amazon ec2 instances. write the incoming orders to anamazon kinesis data stream configure aws lambda to poll the stream and update the inventory application.


 tfr: a this looks like a good solution but it actually won't work as lambda has a concurrent limit for 1000 and we need to process thousands of orders per second. (although we could contact aws to increase the limit, but doesn't feel like a good answer for the exam).b distributed queueing system will probably have duplicate messages at some point. also, auto scale group is not a thing in ecs (it is for ec2 that backed the ecs though) d kinesis stream has no message level ack/fail, still will have duplicate or unprocessed items


68, a company's application is increasingly popular and experiencing latency because of high volume reads on the database server.the service has the following properties:*a highly available rest api hosted in one region using application load balancer (alb) with auto scaling.*a mysql database hosted on an amazon ec2 instance in a single availability zone.*the company wants to reduce latency, increase in-region database read performance, and have multi- region disaster recovery capabilities that can perform a live recovery automatically without any data or performance loss (ha/dr). which deployment strategy will meet these requirements?

A. use aws cloudformation stacksets to deploy the api layer in two regions. migrate the database to an amazon aurora with mysql database cluster with multiple read replicas in one region and a read replica in a different region than the source database cluster. use amazon route 53 health checks to trigger a dns failover to the standby region if the health checks to the primary load balancer fail. in the event of route 53 failover, promote the cross­ region database replica to be the master and build out new read replicas in the standby region.
B. use amazon elasticache for redis multi-az with an automatic failover to cache the database read queries. use aws opsworks to deploy the api layer, cache layer, and existing database layer in two regions. in the event of failure, use amazon route 53 health checks on the database to trigger a dns failover to the standby region if the health checks in the primary region fail. back up the mysql database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.
C. use aws cloudformation stacksets to deploy the api layer in two regions. add the database to an auto scaling group. add a read replica to the database in the second region.use amazon route 53 health checks in the primary region fail. promote the cross-region database replica to be the master and build out new read replicas in the standby region.
D. use amazon elasticache for redis multi-az with an automatic failover to cache the database read queries. use aws opsworks to deploy the api layer, cache layer, and existing database layer in two regions. use amazon route 53 health checks on the alb to trigger a dns failover to the standby region if the health checks in the primary region fail. back up the mysql database frequently, and in the event of a failure in an active region, copy the backup to the standby region and restore the standby database.


Mtfr: a aurora cluster is multi az by default, this is the best option b cannot live failover db c share data volume across ec2 will be painfuld same as b


69,  a company is using aws for production and development workloads. each business unit has its own aws account for production, and a separate aws account to develop and deploy its applications. the information security department has introduced new security policies that limit access for terminating certain amazon ecs instances in all accounts to a small group of individuals from the security team. how can the solutions architect meet these requirements?
A. create a new iam policy that allows access to those ec2 instances only for the security team. apply this policy to the aws organizations master account.
B. create a new tag-based iam policy that allows access to these ec2 instances only for the security team. tag the instances appropriately, and apply this policy in each account.
A. 
C. create an organizational unit under aws organizations. move all the accounts into this organizational unit and use scp to apply a whitelist policy to allow access to these ec2 instances for the security team only.
D. set up saml federation for all accounts in aws. configure saml so that it checks for the service api call before authenticating the user. block saml from authenticating api calls if anyone other than the security team accesses these instances.


 tfr: a iam policy will not be applied to sub accountc scp is not for granular access control. scp will not actually grant permission as well https:/ / docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html d this just won't work as saml will work like token base service and do not rely on the api calls


70, a development team is deploying new apis as serverless applications within a company. the team is currently using the aws management console to provision amazon api gateway, aws lambda, and amazon dynamodb resources. a solutions architect has been tasked with automating the future deployments of these serverless apis.how can this be accomplished?
A. use aws cloudformation with a lambda-backed custom resource to provision api gateway. use the aws::dynamodb::table and aws::lambda::function resources to create the amazon dynamodb table and lambda functions. write a script to automate the deployment of the cloudformation template.
B. use the aws serverless application model to define the resources. upload a yaml template and application files to the code repository. use aws codepipeline to connect to the code repository and to create an action to build using aws codebuild. use the aws cloudformation deployment provider in codepipeline to deploy the solution.
C. use aws cloudformation to define the serverless application. implement versioning on the lambda functions and create aliases to point to the versions. when deploying, configure weights to implement shifting traffic to the newest version, and gradually update the weights as traffic moves over.
D. commit the application code to the aws codecommit code repository. use aws codepipeline and connect to the codecommit code repository. use aws codebuild to build and deploy the lambda functions using aws codedeploy. specify the deployment preference type in codedeploy to gradually shift traffic over to the new version.


 tfr: a api gateway cloudformation is supported, custom resource is not necessary b sam deploy is just an alias of cloudformation deployhttps://docs.aws.amazon.com/serverless­ application-model /latest/ developergui de/ sam-cli-command- reference-sam- deploy.htmlcodebuild can put artifacts in s3 for later lambda deployment.
https:/ / docs.aws.amazon.com/codebuild/ latest/ apireference/ api_projectartifacts.html c we will need either codedeploy or step function to achieve traffic shift, this answer is too vague d api gateway and lambda resource is not deployed


71, a company currently uses amazon ebs and amazon rds for storage purposes. the company intends to use a pilot light approach for disaster recovery in a different aws region. the company has an rto of 6 hours and an rpo of 24 hours. which solution would achieve the requirements with minimal cost?
A. use aws lambda to create daily ebs and rds snapshots, and copy them to the disaster recovery region. use amazon route 53 with active-passive failover configuration. use amazon ec2 in an auto scaling group with the capacity set to O in the disaster recovery region.
B. use aws lambda to create daily ebs and rds snapshots, and copy them to the disaster recovery region. use amazon route 53 with active-active failover configuration. use amazon ec2 in an auto scaling group configured in the same way as in the primary region.
C. use amazon ecs to handle long-running tasks to create daily ebs and rds snapshots, and copy to the disaster recovery region. use amazon route 53 with active-passive failover configuration. use amazon ec2 in an auto scaling group with the capacity set to O in the disaster recovery region
D. use ebs and rds cross-region snapshot copy capability to create snapshots in the disaster recovery region. use amazon route 53 with active-active failover configuration. use amazon ec2 in an auto scaling group with the capacity set to O in the disaster recovery region.

Mtfr: a lambda should not be used for snapshotb ec2 configure the same way' will be multi-sited use built-in cross region copy will be the best solution, but ebs cannot take snapshot automatically, you will need aws data lifecycle


72, an online retailer needs to regularly process large product catalogs, which are handled in batches. these are sent out to be processed by people using the amazon mechanical turk service, but the retailer has asked its solutions architect to design a workflow orchestration system that allows it to handle multiple concurrent mechanical turk operations, deal with the result assessment process, and reprocess failures. which of the following options gives

the retailer the ability to interrogate the state of every workflow with the least amount of implementation effort?
A. trigger amazon cloudwatch alarms based upon message visibility in multiple amazon sqs queues (one queue per workflow stage) and send messages via amazon sns to trigger aws lambda functions to process the next step. use amazon es and kibana to visualize lambda processing logs to see the workflow states.
B. hold workflow information in an amazon rds instance with aws lambda functions polling rds for status changes. worker lambda functions then process the next workflow steps. amazon quicksight will visualize workflow states directly out of amazon rds.
C. build the workflow in aws step functions, using it to orchestrate multiple concurrent workflows. the status of each workflow can be visualized in the aws management console, and historical data can be written to amazon s3 and visualized using amazon quicksight.
D. use amazon swf to create a workflow that handles a single batch of catalog records with multiple worker tasks toextract the data, transform it, and send it through mechanical turk.use amazon es and kibana to visualize aws lambda processing logs to see the workflow states.


 tfr: c step function may not work really well with human intervention, and i don't think historical data can be easily pipe to s3d workflow is best to be dealt with by swf or step function, so a and b are excluded. as we are using mechanical turk hits, manual intervention will be needed (i.e. accessed for successful result). there is also a similar use case with swf in https:/ /aws.amazon.com/swf/faqs/


73, a company wants to move a web application to aws. the application stores session information locally on each web server, which will make auto scaling difficult. as part of the migration, the application will be rewritten to decouple the session data from the web servers. the company requires low latency, scalability, and availability. which service will meet the requirements for storing the session information in the most costeffective way?
A. amazon elasticache with the memcached engine
B. amazon s3
C. amazon rds mysql
D. amazon elasticache with the redis engine


 tfr: memcached is not really ha (no replication) https://docs.aws.amazon.com/amazonelasticache/latest/mem-ug/selectengine.html


74, an on-premises application will be migrated to the cloud. the application consists of a single elasticsearch virtual machine with data source feeds from local systems that will not be migrated, and a java web application on apache tomcat running on three virtual machines. the elasticsearch server currently uses 1 tb of storage out of 16 tb available storage, and the web application is updated every 4 months. multiple users access the web application from the internet. there is a 10gbit aws direct connect connection established, and the application can be migrated over a schedules 48-hour change window. which strategy will have the least impact on the operations staff after the migration?
A. create an elasticsearch server on amazon ec2 right-sized with 2 tb of amazon ebs and a public aws elastic beanstalk environment for the web application. pause the data sources, export the elasticsearch index from on premises, and import into the ec2 elasticsearch server.move data source feeds to the new elasticsearch server and move users to the web application.
B. create an amazon es cluster for elasticsearch and a public aws elastic beanstalk environment for the web application. use aws dms to replicate elasticsearch data. when replication has finished, move data source feeds to the new amazon es cluster endpoint and move users to the new web application.
C. use the aws sms to replicate the virtual machines into aws. when the migration is complete, pause the data source feeds and start the migrated elasticsearch and web application instances. place the web application instances behind a public elastic load balancer. move the data source feeds to the new elasticsearch server and move users to the new web application load balancer.
D. create an amazon es cluster for elasticsearch and a public aws elastic beanstalk environment for the web application. pause the data source feeds, export the elasticsearch index from on premises, and import into the amazon es cluster. move the data source feeds to the new amazon es cluster endpoint and move users to the new web application.

Mtfr: b es cannot be the source of dmshttps://docs.aws.amazon.com/dms/latest/userguide/chap_source.html  d by import and export, i think it means snapshot and restore, otherwise there is no export or import of an index in elasticsearch. if this is the case, this is the best answer, otherwise c will be the only viable solution....

75,	a company has an amazon ec2 deployment that has the following architecture:*an application tier that contains 8 m4.xlarge instances*a classic load balancer*amazon s3 as a persistent data storeafter one of the ec2 instances fails, users report very slow processing of their requests. a solutions architect must recommend design changes to maximize system reliability. the solution must minimize costs.what should the solution architect recommend?
A. migrate the existing ec2 instances to a serverless deployment using aws lambda functions
B. change the classic load balancer to an application load balancer
C. replace the application tier with m4.large instances in an auto scaling group
D. replace the application tier with 4 m4.2xlarge instances




76, a company is storing data on premises on a windows file server. the company produces 5 gb of new data daily.the company migrated part of its windows-based workload to aws and needs the data to be available on a file system in the cloud. the company already has established an aws direct connect connection between the on- premises network and aws.which data migration strategy should the company use?
A. use the file gateway option in aws storage gateway to replace the existing windows file server, and point the existing file share to the new file gateway
B. use aws datasync to schedule a daily task to replicate data between the on-premises windows file server and amazon fsx
C. use aws data pipeline to schedule a daily task to replicate data between the on-premises windows file server and amazon elastic file system (amazon efs)
D. use aws datasync to schedule a daily task to replicate data between the on-premises windows file server and amazon elastic file system (amazon efs)




77, a company maintains a restaurant review website. the website is a single-page application where files are stored in amazon s3 and delivered using amazon cloudfront. the company receives several fake postings every day that are manually removed.the security team has identified that most of the fake posts are from bots with Ip addresses that have a bad reputation within the same global region. the team needs to create a solution to help restrict the bots from accessing the website.which strategy should a solutions architect use?

A. use aws firewall manager to control the doudfront distribution security settings. create a geographical block rule and associate it with firewall manager.
B. associate an aws waf web ad with the doudfront distribution. select the managed amazon ip reputation rule group for the web ad with a deny action.
C. use aws firewall manager to control the doudfront distribution security settings. select the managed amazon ip reputation rule group and associate it with firewall manager with a deny action.
D. associate an aws waf web ad with the doudfront distribution. create a rule group for the web ad with a geographical match statement with a deny action.

Mi-fr: the amazon ip reputation list rule group contains rules that are based on amazon internal threat intelligence. this is useful if you would like to block ip addresses typically associated with bots or other threats. inspects for a list of ip addresses that have been identified as bots by amazon threat intelligence.


78, a company has multiple business units. each business unit has its own aws account and runs a single website within that account.the company also has a single logging account. logs from each business unit website are aggregated into a single amazon s3 bucket in the logging account. the s3 bucket policy provides each business unit with access to write data into the bucket and requires data to be encrypted. the company needs to encrypt logs uploaded into the bucket using a single aws key management service (aws kms) cmk. the cmk that protects the data must be rotated once every 365 days. which strategy is the most operationally efficient for the company to use to meet these requirements?
A. create a customer managed cmk in the logging account. update the cmk key policy to provide access to the loggingaccount only. manually rotate the cmk every 365 days.
B. create a customer managed cmk in the logging account. update the cmk key policy to provide access to the loggingaccount and business unit accounts. enable automatic rotation of the cmk.
C. use an aws managed cmk in the logging account. update the cmk key policy to provide access to the logging account and business unit accounts. manually rotate the cmk every 365 days.
D. use an aws managed cmk in the logging account. update the cmk key policy to provide access to the logging account only. enable automatic rotation of the cmk.


79, a large company has a business-critical application that runs in a single aws region. the application consists of multiple amazon ec2 instances and an amazon rds multi-az db instance. the ec2 instances run in an amazon ec2 scaling group across multiple availability zones. a solution architect is implementing a disaster recovery (dr) plan for the application. the solution architect has created a pilot light application deployments in a new region, which is referred as the rd region. the dr environment has an auto scaling group with a single ec2 instance and a read replica of the rds db instance. the solution architect must automate a failover from the primary application environment to the pilot light environment in the dr region.which solution meets the requirements with the most operational efficiency?
A. publish an application availability metric to amazon cloudwatch in the dr region from the application environment in the primary region.create a cloudwatch alarm in the dr region that is invoked when the application availability metric stops being delivered.configure the cloudwatch alarm to send a notification to an amazon simple notification service (amazon sns) topic in the dr region.add an email subscription to the sns topic that sends messages to the application owner.upon notification instruct a systems operator to sign in to the aws management console and initiate failover operations for the application.
B. create a cron task that runs every 5 minutes by using one of the applications ec2 instances in the primary region.configure the corn task to check whether the application is available.upon failure, the cron task notifies a systems operator and attempts to restart the application services.
C. create a cron task that runs every 5 minutes by using one of the application's ec2 instances in the primary region. configure the cron task to check whether the application is available. upon failure, the cron task modifies the dr environment by promoting the read replica and by adding ec2 instances to the auto scaling group.
D. publish an application available metric to amazon cloudwatch in the dr region from the application environment in the primary region.create a cloudwatch alarm in the dr region that in invoked when the application availability metric steps being delivered.configure the cloudwatch alarm to send a notification to an amazon simple and to add ec2 instances to the auto scaling group.


Mtfr: because if zone is down eventually ec2 will be down and cron will not work.others don't make sense! usually route53 with failover policy is used which is not an option here ec2crono

80, a large financial company is deploying applications that consist of amazon ec2 and amazon rds instances to the aws cloud using aws cloudformation. the cloudformation stack has the following stack policy:the company wants to ensure that developers do not lose data by accidentally removing or replacing rds instances when updating the cloudformation stack developers also still need to be able to modify or remove ec2 instances as needed.how should the company change the stack policy to meet these requirements?
A. modify the statement to specify "effect": "deny", "action":["update:*"] for all logical rds resources.
B. modify the statement to specify "effect": "deny", "action":["update:delete"] for all logical rds resources.
C. add a second statement that specifies "effect": "deny", "action":["update:delete", "update:replace"] for all logical rds resources.
D. add a second statement that specifies "effect": "deny", "action":["update:*"] for all logical rds resources.




8L	a company wants to refactor its retail ordering web application that currently has a load-balanced amazon ec2 instance fleet for web hosting, database api services, and business logic. the company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.which solution will meet these requirements?
A. use amazon s3 for web hosting with amazon api gateway for database api services. use amazon simple queue service (amazon sqs) for order queuing. use amazon elastic container service (amazon ecs) for business logic with amazon sqs long polling for retaining failed orders.
B. use aws elastic beanstalk for web hosting with amazon api gateway for database api services. use amazon mq for order queuing. use aws step functions for business logic with amazon s3 glacier deep archive for retaining failed orders.
C. use amazon s3 for web hosting with aws appsync for database api services. use amazon simple queue service(amazon sqs) for order queuing, use aws lambda for business logic with an amazon sqs dead- letter queue for retaining failed orders.
D. use amazon lightsail for web hosting with aws appsync for database api services. use amazon simple email service (amazon ses) for order queuing. use amazon elastic kubernetes service (amazon eks) for business logic with amazon elasticsearch service (amazon es) for retaining failed orders.
A. 


Mtfr: while appsync is no better than api gw in this context, dlq is better choice than sqs long polling for retaining failed orders.hints: refactoring app to use graphql apis (appsync) + serverless + dlq for failed orders.appsyncapi gwdlqsqsa graphql api (appsync) ++ dlq a


82,	a company has a three-tier application running on aws with a web server, an application server, and an amazon rds mysql db instance. a solutions architect is designing a disaster recovery (dr) solution with an rpo of 5 minutes.which solution will meet the company's requirements?
A. configure aws backup to perform cross-region backups of all servers every 5 minutes.reprovision the three tiers in the dr region from the backups using aws cloudformation in the event of a disaster.
B. maintain another running copy of the web and application server stack in the dr region using aws cloudformation drift detection.configure cross-region snapshots of the db instance to the dr region every 5 minutes. in the event of a disaster, restore the db instance using the snapshot in the dr region.
C. use amazon ec2 image builder to create and copy amis of the web and application server to both the primary and dr regions.create a cross-region read replica of the db instance in the dr region.in the event of a disaster, promote the read replica to become the master and reprovision the servers with aws cloudformation using the amis.
D. create amis of the web and application servers in the dr region.use scheduled aws glue jobs to synchronize the db instance with another db instance in the dr region.in the event of a disaster, switch to the db instance in the dr region and reprovision the servers with aws cloudformation using the amis.




83, a company runs an application in amazon vpc. the application requires that all traffic to three different third-party networks be encrypted. the network traffic between the application and the third-party networks is expected to be no more than 500 mbps for each connection. to facilitate network connectivity, a solutions architect has created a transit gateway and attached the application vpc. which set of actions should the solutions architect perform to complete the solution while minimizing costs?
A. use aws certificate manager (acm) to generate three public/private key pairs. install the private keys on a public - facing application load balancer (alb). have each third-party
A. 
network connect to the alb using httpsitls. update the transit gateway route table to route traffic between the application and the third-party networks through the alb.
B. create an aws direct connect connection between each third-party network and a direct connect gateway. associate the direct connect gateway with the transit gateway. encrypt the direct connect connection with each third-party network using a different encryption key.
C. use aws marketplace to deploy three different public-facing amazon ec2 instances running software vpn appliances.establish vpn connections between each appliance and the third­ party networks.update the transit gateway route table to send encrypted traffic to each third-party network using the appropriate vpn appliance.
D. create a transit gateway vpn attachment to each third-party network. use separate preshared keys for each vpnattachment. share those keys with the third- party networks. update the transit gateway route table by creating a separate route to each third-party network using the appropriate transit gateway attachment.




84, a large company runs workloads in vpcs that are deployed of aws accounts. each vpc consists of public subnets and private subnets that span across multiple availability zones nat gateway are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets. a solution architect is working on a hub-and-spoke design. all private subnets in the spoke vpcs must route traffic to the internal through an aggress vpc. the solutions architect already has deployed has deployed a nat gateway in an egress vpc in a cent al aws account. which set of additional steps should the solution architect take to meet these requirements?
A. create peering connections between the egress vpc and the spoke vpcs.configure the required routing to allow access to the internet.
B. create a transit gateway and share it with the existing aws accounts.attach existing vpcs to the transit gateway. configure routing to allow access to the internet.
C. create a transit gateway in every accountattach the nat gateway to the gateway configure the required routing to allow access to the internet.
D. create an aws privatelink connection between the egress vpc and the spoke vpcs.configure the require routing to allow access to he internet.


85, a financial company with multiple departments wants to expand its on-premises environment to the aws cloud. the company must retain centralized access control using an existing on- premises active directory (ad) service. each department should be allowed to create aws accounts with preconfigured networking and should have access to only a specific list of approved services. departments are not permitted to have account administrator permissions.what should a solutions architect do to meet these security requirements?
A. configure aws identity and access management (iam) with a saml identity provider (idp) linked to the on- premises active directory, and create a role to grant access.configure aws organizations with scps and create new member accounts.use aws cloudformation templates to configure the member account networking.
B. deploy an aws control tower landing zone. create an ad connector linked to the on­ premises active directory.change the identity source in aws single sign-on to use active directory.allow department administrators to use account factory to create new member accounts and networking. grant the departments aws power user permissions on the created accounts.
C. deploy an amazon cloud directory.create a two-way trust relationship with the on­ premises active directory, and create a role to grant access.set up an aws service catalog to use aws cloudformation templates to create the new member accounts and networking.use iam roles to allow access to approved aws services.
D. configure aws directory service for microsoft active directory with aws single sign-on.join the service to the on-premises active directory.use aws cloudformation to create new member accounts and networking. use iam roles to allow access to approved aws services.



86, a company manages an on-premises data ingestion application that receives metrics from lot devices in json format. the data is collected, transformed, and stored in a data warehouse for analysis. the current infrastructure has severe performance issues at peak loads due to insufficient compute capacity, causing some of the data ingestion to be dropped.the company wants to migrate the application to aws. the solution must support its current analytics tool that connects to the data warehouse with a java database connectivity (jdbc) driver. the company requires a resilient and cost. -effective solution that will address the performance issues.which solution will meet these requirements?
A. replatform the application. create an application load balancer and an amazon ec2 instance with auto scaling to host the application to ingest and transform the data. create an amazon rds postgresql multi-az db instance in a private subnet to store data. use amazon quicksight to generate reports and visualize data.
A. 
B. replatform the application. use amazon api gateway to handle data ingestion. use aws lambda to transform the data.create an amazon aurora postgresql db cluster with an aurora replica in two private subnets to store data. use amazon quicksight to generate reports and visualize data.
C. re-architect the application. load the data into amazon s3. use aws glue to transform the data. store the table schema in an aws glue data catalog. use amazon athena to query the data.
D. re-architect the application. load the data into amazon s3. use amazon emr to transform the data.create an external schema in an aws glue data catalog. use amazon redshift spectrum to query the data.



87, a financial company needs to create a separate aws account for a new digital wallet application. the company uses aws organizations to manage its accounts. a solutions architect uses the iam user supportl from the master account to create a new member account with finance1@example.com as the email address.what should the solutions architect do to create iam users in the new member account?
A. sign in to the aws management console with aws account root user credentials by using the 64- character password from the initial aws organizations email sent to finance1@example.com.set up the iam users as required.
B. from the master account, switch roles to assume the organizationaccountaccessrole role with the account id of the new member account. set up the iam users as required.
C. go to the aws management console sign-in page.choose "sign in using root account credentials."sign in by using the email address finance1@example.com and the master account's root password. set up the iam users as required.
D. go to the aws management console sign-in page.sign in by using the account id of the new member account and the supportl iam credentials. set up the iam users as required.



88, a company wants to provide desktop as a service (daas) to a number of employees using amazon workspaces. workspaces will need to access files and services hosted on premises with authorization based on the company's active directory. network connectivity will be provided through an existing aws direct connect connection.the solution has the following requirements:--credentials from active directory should he used to access on­ premises files and services --credentials from active directory should not be stored outside

the company--end users should have single sign-on (sso) to on- premises files and services once connected to workspaces.which strategy should the solutions architect use tor end user authentication'?
A. create an aws directory service for microsoft active directory (aws managed microsoft ad) directory within the workspaces vpc use the active directory migration tool (admt) with the password export server to copy users from the on-premises active directory to aws managed microsoft ad. set up a ore-way trust allowing users from aws managed microsoft ad to access resources in the on-premises active directory. use aws managed microsoft ad as the directory for workspaces.
B. create a service account in the on-premises active directory with the required permissions. create an ad connector in aws directory service to be deployed on premises using the service account to communicate with the on-premises active directory, ensure the required tcp ports are open from the workspaces vpc ta the on-premises ad connector. use tho ad connector as the directory for workspaces.
C. create a service account in the on-premises active directory with the required permissions. create an ad connector in aws directory service within the workspaces vpc using the service account to communicate with the on-premises active directory use the ad connector as the directory for workspaces.
D. create an aws directory service for microsoft active directory (aws managed microsoft ad) directory in the aws directory service within the workspaces vpc. set up a one-way trust allowing users from the on-premises active directory to access resources in the aws managed microsoft ad. use aws managed microsoft ad as the directory for workspaces. create an identity provider with aws identity and access management (iam) from an on­ premises adfs server. allow users from this identity provider to assume a role with a policy allowing them to run workspaces.



89, a solutions architect is troubleshooting an application that runs on amazon ec2 instances. the ec2 instances runs in an auto scaling group. the application needs to access user data in an amazon dynamodb table that has fixed provisioned capacity. to match the increased workload, the solutions architect recently doubled the maximum size of the auto scaling group. new, when many instances launch at the same time, some application components are throttled when the component scan the dynamodb table.the auto scaling group terminates the falling instances and starts new instances unit all applications are running.a solution architect must implement a solution to mitigate the throttling issue in the most cost- effective manner.which solution will these requirements?
A. doubles the provisioned read capacity of the dynamodb table.
A. 
B. duplicate the dynamodb tableconfigure the running copy of the application to select at random which table it accesses.
C. set the dynamodb table to on-demand mode.
D. add dynamodb accelerator (dax) to the table.

Mtfr: applications that are read-intensive, but are also cost-sensitive. with dynamodb, you provision the number ofreads per second that your application requires. ifread activity increases, you can increase your tables' provisioned read throughput (at an additional cost). or, you can offload the activity from your application to a dax cluster, and reduce the number of read capacity units that you need to purchase otherwise.


90, a company recently completed a large-scale migration to aws. development teams that support various business units have their own accounts in aws organizations. a central cloud team is responsible for controlling which services and resources can be accessed, and for creating operational strategies for all teams within tha company. some teams are approaching their account service quotas. the cloud team needs to create an automated and operationally efficient solution to proactively monitor service quotas. monitoring should occur every 15 minutes and send alerts when a team exceeds 80% utilization.which solution will meet these requirements?
A. create a scheduled aws config rule to trigger an aws lambda function to call the getservicequota api. if any service utilization is above 80%, publish a message to an amazon simple notification service (amazon sns! topic to alert the cloud team. create an aws cloudformation template and deploy the necessary resources to each account.
B. create an amazon eventbridge (amazon cloudwatch events) rule that triggers an aws lambda function to refresh the aws trusted advisor service limits checks and retrieve the most current utilization and service limit data. if the current utilization is above 80%, publish a message to an amazon simple notification service (amazon snsi topic to alert the cloud team. create aws cloudformation stacksets that deploy the necessary resources to all organizations accounts.
C. create an amazon cloudwatch alarm that triggers an aws lambda function to call the amazon cloudwatch getlnsightrulereport api to retrieve the most current utilization and service limit data. if the current utilization is above 80%, publish an amazon simple email service (amazon ses) notification to alert the cloud team. create aws cloudformation stacksets that deploy the necessary resources to all organizations accounts.
D. create an amazon eventbridge (amazon cloudwatch events) rule that triggers an aws lambda function to refresh the aws trusted advisor service limits checks and retrieve the
A. 
most current utilization and service limit data. it the current utilization is above 80%, use amazon pinpoint to send an alert to the cloud team. create an aws cloudformation template and deploy the necessary resources to each account.



9L	a company runs several workloads on a mix of amazon ec2 instances and amazon rds instances. the company is using an organization in aws organizations to manage multiple aws accounts. the company's security standards require evaluation of aws resources against the cis benchmarks and to automatically remediate issues where possible what should a solutions architect recommend to meet these requirements?
A. enable aws config in all accounts in the organization create an amazon s3 bucket in an account within the organization to store the aws config compliance reports deploy a conformance pack across the organization to implement the required cis controls and remediation.
B. use aws opsworks tor chef automate in all accounts in the organization create an amazon s3 bucket in an account within the organization deploy chef cookbooks to implement the required cis controls to the ec2 instances, and use opsworks for chef automate lo deploy the cookbooks
C. use aws systems manager configuration compliance to scan compliance of the resources in aws against the required cis controlsuse systems manager patch manager lo apply any needed remediation on noncompliant ec2 instances use aws lambda functions tor the other remediation on amazon rds
D. enable amazon inspector to audit the environment against the cis controls ingest results from amazon inspector into aws (amazon cloudwatch events) to schedule aws lambda functions to remediate issues.

fAIHfr: modify on d>c


92, a company that runs applications on aws recently subscribed to a new software-as-a­ service (saas) data vendor.the vendor provides the data by way of a rest api that the vendor hosts in its aws environment. the vendor offers multiple options for connectivity to the api and is working with the company to find the best way to connectthe company's aws account does not allow outbound internet access from its aws environment. the vendor's services run on aws in the same aws region as me company's applications. a solutions architect must

implement connectivity to the vendor's api so that the api is highly available in the company's vpc.which solution will meet these requirements?
A. connect o the vendor's public api address for the data service
B. connect o the vendor by way of a vpc peering connection between the vendor's vpc and the company's vpc
C. connect to the vendor by way of a vpc endpoint service that uses aws privatelink
D. connect to a public bastion host that the vendor provides. tunnel the api traffic.




93, a solutions architect is planning the migration of a complete on-premises data center to the aws cloud. the solutions architect must map all the dependencies between the on­ premises servers and must propose a migration order and timeline. the solutions architect will determine dependencies by using a combination of data that is taken from the on­ premises vms. this data will include network traffic and processes that are running.the on­ premises servers are in a vmware environment. what should the solutions architect do to collect dependency information from the on-premises servers to guide the order of migration?
A. deploy the aws agentless discovery connector ova file in the vmware environment.
B. deploy the aws application discovery agent to all servers in the vmware environment.
C. deploy the aws server migration connector ova file in the vmware environment.
D. deploy aws management portal for vcenter in the vmware environment.




94, a finance company hosts a data lake in amazon s3. the company receives financial data records over sftp each night from several third parties. the company runs its own sftp server on an amazon ec2 instance in a public subnet of a vpc. after the files are uploaded, they are moved to the data lake by a cron job that runs on the same instance.the sftp server is reachable on dns sftp.example.com through the use of amazon route 53. what should a solutions architect do to improve the reliability and scalability of the sftp solution?
A. move the ec2 instance into an auto scaling group.place the ec2 instance behind an application load balancer (alb). update the dns record sftp.example.com in route 53 to point to the alb.
A. 
B. migrate the sftp server to aws transfer for sftp.update the dns record sftp.example.com in
route 53 to point to the server endpoint hostname.
C. migrate the sftp server to a file gateway in aws storage gateway.update the dns record sftp.example.com in route 53 to point to the file gateway endpoint.
D. place the ec2 instance behind a network load balancer (nlb).update the dns record sftp.example.com in route 53 to point to the nib.


fAIHfr: modify on e>b



95, a company has two vpcs within the same aws account that are connected through a transit gateway. a solutions architect adds a new subnet to one of the vpcs. resources that are hosted in the new subnet are not able to communicate with resources in the other vpc. what should the solutions architect do to allow network traffic communication?
A. configure the new subnet to propagate to the appropriate transit gateway route table.associate the new subnet with the appropriate vpc route able.
B. create a new static route within the tran it gateway route table.associate the new subnet with the appropriate vpc route table.
C. update the transit gateway attachment to include the new subnet.define a new static route within the transit gateway route table.
D. update the transit gateway attachment to include the new subnet.associate the new subnet with the appropriate vpc route table.




96, a company is planning to host a web application on aws and wants to load balance the traffic across a group of amazon ec2 instances.one of the security requirements is to enable end-to-end encryption in transit between the client and the web server.which solution will meet this requirement?
A. place the ec2 instances behind an application load balancer (alb).provision an ssl certificate using aws certificate manager (acm), and associate the ssl certificate with the alb.export the ssl certificate and install it on each ec2 instance. configure the alb to listen on port 443 and to forward traffic to port 443 on the instances.
A. 
B. associate the ec2 instances with a target group.provision an ssl certificate using aws certificate manager (acm).create an amazon cloudfront distribution and configure it to use the ssl certificate. set cloudfront to use the target group as the origin server.
C. place the ec2 instances behind an application load balancer (alb).provision an ssl certificate using aws certificate manager (acm), and associate the ssl certificate with the alb. provision a third-party ssl certificate and install it on each ec2 instance.configure the alb to listen on port 443 and to forward traffic to port 443 on the instances.
D. place the ec2 instances behind a network load balancer (nlb).provision a third-party ssl certificate and install it on the nib and on each ec2 instance. configure the nib to listen on port 443 and to forward traffic to port 443 on the instances.




97, a company runs a proprietary stateless etl application on an amazon ec2 linux instance. the application is a linux binary, and the source code cannot be modified. the application is single-threaded, uses 2 gb of ram. and is highly cpu intensive. the application is scheduled to run every 4 hours and runs for up to 20 minutes. a solutions architect wants to revise the architecture for the solution.which strategy should the solutions architect use?
A. use aws lambda to run the application.use amazon cloudwatch logs to invoke the lambda function every 4 hours.
B. use aws batch to run the application.use an aws step functions state machine to invoke the aws batch job every 4 hours.
C. use aws fargate to run the application.use amazon eventbridge (amazon cloudwatch events) to invoke the fargate task every 4 hours.
D. use amazon ec2 spot instances to run the application.use aws codedeploy to deploy and run the application every 4 hours.




98, a medical company is running a rest api on a set of amazon ec2 instances. the ec2 instances run in an auto scaling group behind an application load balancer (alb). the alb runs in three public subnets, and the ec2 instances run in three private subnets. the company has deployed an amazon cloudfront distribution that has the alb as the only origin.which solution should a solutions architect recommend to enhance the origin security?

A. store a random string in aws secrets manager. create an aws lambda function for automatic secret rotation. configure cloudfront to inject the random string as a custom http header for the origin request create an aws waf web acl rule with a string match rule for the custom header. associate the web acl with the alb.
B. create an aws wafweb acl rule with an ip match condition of the cloudfront service ip address ranges. associate the web acl with the alb. move the alb into the three private subnets
C. store a random string in aws systems manager parameter store. configure parameter store automatic rotation for the string. configure cloudfront to inject the random string as a custom http header for the origin request. inspect the value of the custom http header, and block access in the alb.
D. configure aws shield advanced. create a security group policy to allow connections from cloudfront service ip address ranges add the policy to aws shield advanced, and attach the policy to the alb.


fAIHJr: modify on c->a



99, a company with global offices has a single 1 gbps aws direct connect connection to a single aws region. the company's on-premises network uses the connection to communicate with the company's resources in the aws cloud. the connection has a single private virtual interface that connects to a single vpc.a solutions architect must implement a solution that adds a redundant direct connect connection in the same region. the solution also must provide connectivity to other regions through the same pair of direct connect connections as the company expands into other regions.which solution meets these requirements?
A. provision a direct connect gateway. delete the existing private virtual interface from the existing connection. createthe second direct connect connection. create a new private virtual interface on each connection, and connect both private virtual interfaces to the direct connect gateway. connect the direct connect gateway to the single vpc.
B. keep the existing private virtual interface. create the second direct connect connection. create a new private virtual interface on the new connection, and connect the new private virtual interface to the single vpc.
C. keep the existing private virtual interface. create the second direct connect connection. create anew public virtual interface on the new connection, and connect the new public virtual interface to the single vpc.
D. provision a transit gateway delete the existing private virtual interface from the existing connection.create the second direct connect connection create a new private virtual
A. 
interface on each connection, and connect both private virtual interfaces to the transit gateway. associate the transit gateway with the single vpc


tfr: modify on d->a


100, a company is funning workloads that are deployed across hundreds of aws accounts. the company uses aws organizations with all features enabled. a solutions architect must implement a solution that will prevent the use of each member account's root user.which service control policy (scp) should the solutions architect apply to the organization root to meet this requirement?
A. option a
B. option b



10L	a large company in europe plans to migrate its applications to the aws cloud. the company uses multiple aws accounts for various business group. a data privacy law requires the company to restrict developers' access to aws european regions only.what should the solutions architect do to meet this requirement with the least amount of management overhead?
A. create iam users and iam groups in each account.create iam policies to limit access to non­ european regions. attach the iam policies to the iam groups.
B. enable aws organizations, attach the aws accounts, and create ous tor european regions and non- european regions.create scps to limit access to non-european regions and attach the policies to the ous.
C. set up aws single sign-on and attach aws accounts.create permission sets with policies to restrict access to non-european regions. create iam users and iam groups in each account.
D. enable aws organizations, attach the aws accounts, and create ous for european regions and non- european regions.create permission sets with policies to restrict access lo non­ european regions. create iam users and iam groups in the primary account.


102, a company is running a web application on amazon ec2 instances in a production aws account. the company requires all logs generated from the web application o be copied to a central aws account for analysis and archiving.the company's aws accounts are currently managed independently. logging agents are configured on the ec2 instances to upload the log files to an amazon s3 bucket in the central aws account. a solution architect needs to provide access for a solution that will allow the production account to store log files in the central account.the central account also needs to have read access to the log files. what should the solutions architect do to meet these requirements?
A. create a cross-account role in the central account.assume the role from the production account when the logs are being copies.
B. create a policy on the s3 bucket with the production account id as the principal.allow s3 access from a delegated user.
C. create a policy on the s3 bucket with access from only the cidr range of the ec2 instances in the production account.use the production account id as the principal.
D. create a cross-account role in the production account.assume the role from the production account when the logs are being copies



103, a software development company has multiple engineers who are working remotely. the company is running active directory domain services (adds) on an amazon ec2 instance. the company's security policy states that all internal, nonpublic services that are deployed in a vpc must be accessible through a vpn.multi-factor authentication (mfa) must be used for access to a vpn. what should a solutions architect do to meet these requirements?
A. create an aws site-to-site vpn connection.configure integration between a vpn and ad
<ls.use an amazon workspaces client with mfa support enabled to establish a vpn connection.
B. create an aws client vpn endpoint.create an ad connector directory for integration with ad ds. enable mfa for ad connector.use aws client vpn to establish a vpn connection.
C. create multiple aws site-to-site vpn connections by using aws vpn cloudhub.configure integration between aws vpn cloudhub and adds. use aws copilot to establish a vpn connection.
D. create an amazon worklink endpoint.configure integration between amazon worklink and ad <ls.enable mfa in amazon worklink. use aws client vpn to establish a vpn co nection.


104, a company is running several workloads in a single aws account. a new company policy stales that engineers can provision only approved resources and that engineers must use aws cloudformation to provision these resources. a solutions architect needs to create a solution to enforce the new restriction on the iam role that the engineers use for access.what should the solutions architect do to create the solution?
A. upload aws cloudformation templates that contain approved resources to an amazon s3 bucket.update the iam policy for the engineers' iam role to only allow access to amazon s3 and aws cloudformation. use aws cloudformation templates to provision resources.
B. update the iam policy for the engineers" iam role with permissions to only allow provisioning of approved resources and aws cloudformation.use aws cloudformation templates to create stacks with approved resources.
C. update the iam policy for the engineers' iam role with permissions to only allow aws cloudformation actions.create a new iam policy with permission to provision approved resources, and assign the policy to a new iam service role.using the iam service role to aws cloudformation during stack creation.
D. provision resources in aws cloudformation stacks.update the iam policy for the engineers' iam role to only allow access to their own aws cloudformation stack.


tfr: modify on b->c


105, a company runs a software-as-a-service (saas) application on aws. the application consists of aws lambda functions and an amazon rds for mysql multi-az database. during market events the application has a much higher workload than normal users notice slow response times during the peak period because of many database connections. the company needs to improve the scalable performance and availability of the database.which solution meets these requirements?
A. create an amazon cloudwatch alarm action that triggers a lambda function to add an amazon rds for mysql read replica when resource utilization hits a threshold
B. migrate the database to amazon aurora, and add a read replica add a database connection pool outside of the lambda handler function
C. migrate the database to amazon aurora, and add a read replica use amazon route 53 weighted records
D. migrate the database to amazon aurora, and add an aurora replica configure amazon rds proxy to manage database connection pools
A. 


Mtfr: modify on b>d



106, a solutions architect is designing a disaster recovery solution for a critical workload. the workload stores data in an amazon s3 bucket that uses server-side encryption and cmk. the company must comply with an rpo of 15 minutes or less for s3 stored objects across different geographic applications in the same aws account.objects stored in amazon s3 need to be at least 99.99% compliant with rpo requirements in the event of a disaster, the company needs to be able to recover individual files and objects in different regions.the solution should require minimal security changes.which combination of stops should the solutions architect take? (select three)
A. use amazon eventbridge to schedule an aws lambda function with an elevated iam role to copy objects to a different s3 bucket every 15 minutes.
B. grant additional permissions to the iam role for the s3getobjectversionforreplication and kms decrypt, kms encrypt actions to be able to replicate encrypted objects.
C. enable s3 cross region replication with s3 replication time control (s3 rte) to replicate objects to an s3 bucket in a different region.
D. in the destination configuration, add the asymmetric aws kms cmk and explicitly opt in by enabling replication of encrypted objects using the sourceselectioncriteria element.
E. in the destination configuration, add the symmetric aws kms cmk and explicitly opt in by enabling replication of encrypted objects using the sourcesolectioncriteria element.
F. grant additional permissions to the iam role for tho s3 getobjectversion and kms decrypt, kms encrypt actions to be able to replicate encrypted objects.




107, a company has developed a custom tool used in its workflow that runs within a docker container. the company must perform manual steps each time the container code is updated to make the container image available to new workflow executions. the company wants to automate this process to eliminate manual effort and ensure a new container image is generated every time the tool code is updated.which combination of actions should a solutions architect take to meet these requirements? (choose three.)
A. configure an amazon ecr repository for the tool. configure an aws codecommit repository containing code for the tool being deployed to the container image in amazon ecr.
A. 
B. configure an aws codedeploy application that triggers an application version update that pulls the latest tool container image from amazon ecr, updates the container with code from the source aws codecommit repository, and pushes the updated container image to amazon ecr.
C. configuration an aws codebuild project that pulls the latest tool container image from amazon ecr, updates the container with code from the source aws codecommit repository, and pushes the updated container image to amazon ecr.
D. configure an aws codepipeline pipeline that sources the tool code from the aws codecommit repository and initiates an aws codedeploy application update.
E. configure an amazon eventbridge rule that triggers on commits to the aws codecommit repository for the tool. configure the event to trigger an update to the tool container image in amazon ecr. push the updated container image to amazon ecr.
F. configure an aws codepipeline pipeline that sources the tool code from the aws codecommit repository andinitiates an aws codebuild build.


f@Hfr: acf is right.


108, a company uses aws organizations to manage more than 1,000 aws accounts. the company has created a new developer organization there are 540 developer member accounts that must be moved to the new developer organization. all accounts are set up with all the required information s0 that each account can be operated as a standalone account.which combination of steps should. a solutions architect take to move all of the developer accounts to the new developer organization? (select three.)
A. call the moveaccount operation in the organizations api from the old organization's master account to migrate the developer accounts to the new developer organization.
B. from the master account, remove each developer account from the old organization using the removeaccountfromorganization operation in the organizations api
C. from each developer account, remove the account from the old organization using the removeaccountfromorganization operation in the organizations api.
D. sign in to the new developer organization's master account and create a placeholder member account that acts as a target for the developer account migration.
E. call the inviteaccounttoorganization operation in the organizations api from the new developer organization's master account to send invitations to the developer accounts.
A. 
F. have each developer sign in to their account and confirm to join the new developer organization.


tfr:
https:/ / docs.aws.amazon.com/organizations /latest/apireference/ api_moveaccount.html https:/ / docs.aws.amazon.com/organizations/latest/apireference / api_removeaccountfromorganization.htmlh ttps:/ / docs.aws.amazon.com/organizations /late st/ apireference / api_inviteaccounttoorganization.html


109, a company's aws architecture currently uses access keys and secret access keys stored on each instance to access aws servicesdatabase credentials are hard-coded on each instance. ssh keys for command-line remote access are stored in a secured amazon s3bucket. the company has asked its solutions architect to improve the security posture of the architecture without adding operational complexity.which combination of steps should the solutions architect take to accomplish this? (select three)
A. use amazon ec2 instance profiles with an iam role.
B. use aws secrets manager to store access keys and secret access keys.
C. use aws systems manager parameter store to store database credentials
D. use a secure fleet of amazon ec2 bastion hosts for remote access.
E. use aws kms to store database credentials.
F. use aws systems manager session manager for remote access.

Mtfr: fm081658


110, a solutions architect needs to define a reference architecture for a solution for three­ tier applications with web, application, and nosql data layers. the reference architecture must meet the following requirements:-high availability within an aws region-able to fail over in 1 minute to another aws region for disaster recovery-provide the most efficient solution while minimizing the impact on the user experience which combination of steps will meet these requirements? (choose three.)
A. use an amazon route 53 weighted routing policy set to 100/0 across the two selected regions.set time to live (ttl) to 1 hour.
A. 
B. use an amazon route 53 failover routing policy for failover from the primary region to the disaster recovery region. set time to live (ttl) to 30 seconds.
C. use a global table within amazon dynamodb so data can be accessed in the two selected regions.
D. back up data from an amazon dynamodb table in the primary region every 60 minutes and then write the data to amazon s3.use s3 cross-region replication to copy the data from the primary region to the disaster recovery region. have a script import the data into dynamodb in a disaster recovery scenario.
E. implement a hot standby model using auto scaling groups for the web and application layers across multiple availability zones in the regions.use zonal reserved instances for the minimum number of servers and on-demand instances for any additional resources.
F. use auto scaling groups for the web and application layers across multiple availability zones in the regions.use spot instances for the required resources.


fAIHJr: bee a is incorrect.



11L	a company has multiple lines of business (lobs) that roll up to the parent company. the company has asked its solutions architect to develop a solution with the following requirements:-produce a single aws invoice for all of the aws accounts used by its lobs.-the costs for each lob account should be broken out on the invoice.-provide the ability to restrict services and features in the lob accounts, as defined by the company's governance policy.-each lob account should be delegated full administrator permissions, regardless of the governance policy. which combination of steps should the solutions architect take to meet these requirements? (choose two.)
A. use aws organizations to create an organization in the parent account for each lob. then, invite each lob account to the appropriate organization.
B. use aws organizations to create a single organization in the parent account.then, invite each lob's aws account to pin the organization.
(.implement service quotas to define the services and features that are permitted and apply the quotas to each lob as appropriate.
D. create an scp that allows only approved services and features, then apply the policy to the lob accounts.enable consolidated billing in the parent account's billing console and link the lob accounts.
D. 
E. enable consolidated billing in the parent account's billing console and link the lob accounts.

 tfr: b: with consolidated billingd: again consolidated billing, and ability to restrict services and features in the lob accounts.


112, a company's security compliance requirements state that all amazon ec2 images must be scanned for vulnerabilities and must pass a eve assessment. a solutions architect is developing a mechanism to create security-approved amis that can be used by developers.any new amls should go through an automated assessment process and be marked as approved before developers can use them. the approved images must be scanned every 30 days to ensure compliance. which combination of steps should the solutions architect take to meet these requirements while following best practices? (select two.)
A. use the aws systems manager ec2 agent to run the eve assessment on the ec2 instances launched from the amis that need to be scanned.
B. use aws lambda to write automatic approval rules. store the approved ami list in aws systems manager parameter store. use amazon eventbridge to trigger an aws systems manager automation document on all ec2 instances every 30 days.
C. use amazon inspector to run the eve assessment on the ec2 instances launched from the amis that need to be scanned.
D. use aws lambda to write automatic approval rules. store the approved ami list in aws systems manager parameter store. use a managed aws config rule for continuous scanning on all ec2 instances, and use aws systems manager automation documents for remediation.
E. use aws cloudtrail to run the eve assessment on the ec2 instances launched from the amis that need to be scanned.



113, a company has built a high performance computing (hpc) cluster in aws for a tightly coupled workload that generates a large number of shared files stored in amazon efs. the cluster was performing well when the number of amazon ec2 instances in the cluster was
100. however, when the company increased the cluster size to 1,000 ec2 instances, overall performance was well below expectations. which collection of design choices should a solutions architect make to achieve the maximum performance from the hpc cluster? (choose three.)

A. ensure the hpc cluster is launched within a single availability zone.
B. launch the ec2 instances and attach elastic network interfaces in multiples of four.
C. select ec2 instance types with an elastic fabric adapter (efa) enabled.
D. ensure the clusters is launched across multiple availability zones
E. replace amazon efs win multiple amazon ebs volumes in a raid array.
F. replace amazon efs with amazon fsx for lustre.


fAIHfr: acf is right.



114, a company has multiple aws accounts and manages these accounts with aws organizations. a developer was given iam user credentials to access aws resources. the developer should have read-only access to all amazon s3 buckets in the account. however,when the developer tries to access the s3 buckets from the console, they receive an access denied error message with no buckets listed.a solutions architect reviews the permissions and finds that the developer's iam user is listed as having read- only access to alls3 buckets in the account.which additional steps should the solutions architect take to troubleshoot the issue? (select two.)
A. check the bucket policies for all s3 buckets.
B. check the ads for all s3 buckets.
C. check the scps set at the organizational units (ous).
D. check for the permissions boundaries set for the iam user.
E. check if an appropriate iam role is attached to the iam user.


Mtfr: a is incorrect even though a bucket policy is a resource based policy and will be evaluated after organizations scps, if a deny is set in the policy you will list see it listed. you will see the word "error" in the access column.bis incorrect because even though ads are resource-based policies you use ads to grant basic read/write permissions on the objects in the bucket. you'll still be able to listbuckets if there is an ad on the bucket.c is correct because after the deny evaluation a organization scps are evaluated and take affect/ merged. (see link below)d is correct because a deny on the permission boundary will not allow the developer to listbuckets e is incorrect because this is a iam permission and applied after deny, org scp, and resource-based policy evaluation. in addition the solution architect

checked the developers iam user and it was listed as readonly.https://docs.aws.amazon.com/iam/latest/userguide/reference_policies_evaluatio   n-logic.html#policy-eval- denyallow


115, a healthcare company runs a production workload on aws that stores highly sensitive personal information. the security team mandates that, for auditing purposes, any aws api action using aws account root user credentials must automatically create a high-priority ticket in the company's ticketing system. the ticketing system has a monthly 3-hour maintenance window when no tickets can be created.to meet security requirements, the company enabled aws cloudtrail logs and wrote a scheduled aws lambda function that uses amazon athena to query api actions performed by the roo user. the lambda function submits any actions found to the ticketing system api. during a recent security audit, the security team discovered that several tickets were no created because the ticketing system was unavailable due to planned maintenance.which combination of steps should a solutions architect take to ensure that the incidents are reported to the ticketing system even during planned maintenance? (select two.)
A. create an amazon sns topic to which amazon cloudwatch alarms will be published configure a cloudwatch alarm to invoke the lambda function.
B. create an amazon sqs queue to which amazon cloudwatch alarms will be published configure a cloudwatch alarm to publish to the sqs queue.
C. modify the lambda function to be triggered by messages published to an amazon sns topic. update the existing application code to retry every 5 minutes if the ticketing system's api endpoint is unavailable.
D. modify the lambda function to be triggered when there are messages in the amazon sqs queue and to return successfully when the ticketing system api has processed the request.
E. create an amazon eventbridge rule that triggers on all api events where the invoking user identity is root configure the eventbridge rule to write the event to an amazon sqs queue.




116, a company is planning to migrate an existing high performance computing (hpc) solution to the aws cloud. the existing solution consists of a 12-node cluster running linux with high-speed interconnectivity deployed on a single rack.a solutions architect needs to optimize the performance of the hpc cluster. which combination of steps will meet these requirements? (select two.)
A. deploy instances across at least three availability zones.
A. 
B. deploy amazon ec2 instances in a placement group.
C. use amazon ec2 instances that support elastic fabric adapter (efa).
D. use amazon ec2 instances that support burstable performance.
E. enable cpu hyperthreading.




117, an enterprise company is using a multi-account aws strategy. there are separate accounts for development staging and production workloads. to control costs and improve governance the following requirements have been defined:-the company must be able to calculate the aws costs for each project-the company must be able to calculate the aws costs for each environment development staging and production.-commonly deployed it services must be centrally managed.-business units can deploy pre-approved it services only.-usage of aws resources in the development account must be limited.which combination of actions should be taken to meet these requirements? (choose three.)
A. apply environment, cost center, and application name tags to all taggable resources. B.configure custom budgets and define thresholds using cost explorer.
C.configure aws trusted advisor to obtain weekly emails with cost-saving estimates.
D.create a portfolio for each business unit and add products to the portfolios using aws cloudformation in aws service catalog.
E.configure a billing alarm in amazon cloudwatch.
F.configure scps in aws organizations to allow services available using aws.


fAIHfr: a- tagging & costingd- preapproved and control f- limited service cebudgettagging(a) service catalog portfolio(d)


118, a company hosts a game player-matching service on a public facing, physical, on­ premises instance that all users are able to access over the internet. all traffic to the instance uses udp. the company wants to migrate the service to aws and provide a high level of security. a solutions architect needs to design a solution for theplayer-matching service using aws.which combination of steps should the solutions architect take to meet these requirements? (choose three.)

A. use a network load balancer (nib) in front of the player-matching instance.use a friendly dns entry in amazon route 53 pointing to the nib's elastic ip address.
B. use an application load balancer (alb) in front of the player-matching instance.use a friendly dns entry in amazon route 53 pointing to the alb's internet-facing fully qualified domain name (fqdn).
C. define an aws waf rule to explicitly drop non-udp traffic, and associate the rule with the load balancer.
D. configure a network acl rule to block all non-udp traffic.associate the network acl with the subnets that hold the load balancer instances.
E. use amazon cloudfront with an elastic load balancer as an origin.
F. enable aws shield advanced on all public-facing resources.


f@Hfr: need nib here, optionb,c,e works with albifyour application is used only for tcp traffic, you can create a rule to deny all udp traffic, or vice versa" https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-
resiliency/security-groups-and- network- access-control-lists-nacls-bp5.htmladf is right.
using alb for the gaming industry, bdf>adf


119, a company wants to migrate its on-premises data center to the aws cloud. this includes thousands of virtualized linux and microsoft windows servers, san storage, java and php applications with mysql, and oracle databases. there are many dependent services hosted either in the same data center or externally. the technical documentation is incomplete and outdated. a solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. which tools or services should the solutions architect use to plan the cloud migration? (select three.)
A. aws application discovery service B.awssms
C.awsx-ray
D.aws cloud adoption readiness tool (cart)
E.amazon inspector
F.aws migration hub


120, a company runs a popular public-facing ecommerce website. its user base is growing quickly from a local market to a national market.the website is hosted in an on premises data center with web servers and a mysql database. the company wants to migrate its workload to aws. a solutions architect needs to create a solution to:--improve security-­ improve reliability--improve availability--reduce latency--reduce maintenancewhich combination of steps should the solutions architect take to meet these requirements? (select three)
A. use amazon ec2 instances in two availability zones for the web servers in an auto scaling group behind an application load balancer.
B. migrate the database to a multi-az amazon aurora mysql db cluster.
C. use amazon ec2 instances in two availability zones to host a highly available mysql database cluster.
D. host static website content in amazon s3. use s3 transfer acceleration to reduce latency while serving webpages. use aws waf to improve website security.
E. host static website content in amazon s3. use amazon cloudfront to reduce latency while serving webpages. use aws waf to improve website security.
F. migrate the database to a single az amazon rds for mysql db instance.



12L	a company has five physical data centers in specific locations around the world. each data center has hundreds of physical servers with a mix of windows and linux-based applications and database services. each data center also has an aws direct connect connection of 10 gbps to aws with a company- approved vpn solution to ensure that data transfer is secure. the company needs to shut down the existing data centers as quickly as possible and migrate the servers and applications to aws.which solution meets these requirements?
A. install the aws server migration service (aws sms) connector onto each physical machine. use the aws management console to select the servers from the server catalog, and start the replication.once the replication is complete, launch the amazon ec2 instances created by the service.
B. install the aws datasync agent onto each physical machine.use the aws management console to configure the destination to be an ami, and start the replication. once the replication is complete, launch the amazon ec2 instances created by the service.
A. 
C. install the cloudendure migration agent onto each physical machine.create a migration blueprint, and start the replication.once the replication is complete, launch the amazon ec2 instances in cutover mode.
D. install the aws application discovery service agent onto each physical machine. use the aws migration hub import option to start the replication. once the replication is complete, launch the amazon ec2 instances created by the service.


tfr: sms is currently not supported on physical machines. sms is for vms only.



122, an online retail company hosts its stateful web-based application and mysql database in an on-premises data center on a single server. the company wants to increase its customer base by conducting more marketing campaigns and. promotions. in preparation, the company wants to migrate its application and database to aws to increase the reliability of its architecture. which solution should provide the highest level ofreliability?
A. migrate the database to an amazon rds mysql multi-az db instance. deploy the application in an auto scaling group on amazon ec2 instances behind an application load balancer. store sessions in amazon neptune.
B. migrate the database to amazon aurora mysql. deploy the application in an auto scaling group on amazon ec2 instances behind an application load balancer. store sessions in an amazon elasticache for redis replication group.
C. migrate the database to amazon documentdb (with mongodb compatibility). deploy the application in an auto scaling group on amazon ec2 instances behind a network load balancer. store sessions in amazon kinesis data firehose.
D. migrate the database to an amazon rds mariadb multi-az db instance. deploy the application in an auto scalinggroup on amazon ec2 instances behind an application load balancer store sessions in amazon elasticache for memcached.




123, a company that tracks medical devices in hospitals wants to migrate its existing storage solution to the aws cloud. the company equips all of its devices with sensors that collect location and usage information. this sensor data is sent in unpredictable patterns with large spikes. the data is stored in a mysql database running on premises at each hospital. the company wants the cloud storage solution to scale with usage.the company's analytics team uses the sensor data to calculate usage by device type and hospital. the team needs to keep analysis tools running locally while fetching data from the cloud. the team

also needs to use existing java application and sql queries with as few changes as possible. how should a solutions architect meet these requirements while ensuring the sensor data is secure?
A. store the data in an amazon aurora serverless database. serve the data through a network load balancer (nlb).authenticate users using the nl b with credentials stored in aws secrets manager.
B. store the data in an amazon s3 bucket. serve the data through amazon quicksight using an iam user authorized with aws identity and access management (iam) with the s3 bucket as the data source.
C. store the data in an amazon aurora serverless database. serve the data through the aurora data api using an iam user authorized with aws identity and access management (iam) and the aws secrets manager arn
D. store the data in an amazon s3 bucket. serve the data through amazon athena using aws privatelink to secure the data in transit.

Mi9T: https:/ / docs.aws.amazon.com/amazonrds/latest/aurorauserguide/ data­ api.html#data-api.accesshe amazonrdsdatafullaccess policy also includes permissions for the user to get the value of a secret from aws secrets manager. users need to use secrets manager to store secrets that they can use in their calls to the data api. using secrets means that users don't need to include database credentials for the resources that they target in their calls to the data api.the team needs to keep analysis tools running locally while fetching data from the cloud.amazonrdsdatafullaccessawso secrets managerapio o
https:/ / aws.amazon.com/en/biogs/ security/rotate-amazon-rds-database-credentials­ automatically-with-aws- secrets-
manager/https:/  / aws.amazon.com/cn/rds/aurora/serverless/


124, a life sciences company is using a combination of open source tools to manage data analysis workflows and docker containers running on servers in its on-premises data center to process genomics data. sequencing data is generated and stored on a local storage area network (san), and then the data is processed. the research and development teams are running into capacity issues and have decided to re- architect their genomics analysis platform on aws to scale based on workload demands and reduce the turnaround time from weeks to days.the company has a high-speed aws direct connect connection. sequencers will generate around 200 gb of data for each genome;and individual jobs can take several hours to process the data with ideal compute capacity. the end result will be stored in amazon s3. the company is expecting 10-15 job requests each day.which solution meets these requirements?

A. use regularly scheduled aws snowball edge devices to transfer the sequencing data into aws. when aws receives the snowball edge device and the data is loaded into amazon s3, use s3 events to trigger an aws lambda function to process the data.
B. use aws data pipeline to transfer the sequencing data to amazon s3. use s3 events to trigger an amazon ec2 auto scaling group to launch custom-am! ec2 instances running the docker containers to process the data.
C. use aws datasync to transfer the sequencing data to amazon s3. use s3 events to trigger an aws lambda function that starts an aws step functions workflow. store the docker images in amazon elastic container registry (amazon ecr) and trigger aws batch to run the container and process the sequencing data.
D. use an aws storage gateway file gateway to transfer the sequencing data to amazon s3. use s3 events to trigger an aws batch job that executes on amazon ec2 instances running the docker containers to process the data.


tfr: modify on d->c



125, a financial company is building a system to generate monthly, immutable bank account statements for its users. statements are stored in amazon s3. users should have immediate access to their monthly statements for up to 2 years some users access their statements frequently, whereas others rarely access their statements. the company's security and compliance policy requires that the statements be retained for at least 7 years.what is the most cost-effective solution to meet the company's needs?
A. create an s3 bucket with object lock disabled. store statements in s3 standard. define an s3 lifecycle policy to transition the data to s3 standard-infrequent access (s3 standard-la) after 30 days.define another s3 lifecycle policy to move the data to s3 glacier deep archive after 2 years. attach an s3 glacier vault lock policy with deny delete permissions for archives less than 7 years old.
B. create an s3 bucket with versioning enabled. store statements in s3 intelligent-tiering. use same- region replication to replicate objects to a backup s3 bucket. define an s3 lifecycle policy for the backup s3 bucket to move the data to s3 glacier. attach an s3 glacier vault lock policy with deny delete permissions for archives less than 7 years old.
C. create an s3 bucket with object lock enabled. store statements in s3 intelligent-tiering. enable compliance mode with a default retention period of 2 years. define an s3 lifecycle policy to move the data to s3 glacier after 2 years. attach an s3 glacier vault lock policy with deny delete permissions for archives less than 7 years old.
A. 
D. create an s3 bucket with versioning disabled. store statements in s3 one zone-Infrequent access (s3 one zone- la).define an s3 lifecycle policy to move the data to s3 glacier deep archive after 2 years.attach an s3 glacier vault lock policy with deny delete permissions for archives less than 7 years old.

 tfr: s3 object lock protection is maintained regardless of which storage class the object resides in and throughout s3 lifecycle transitions between storage classes.


126, a company is building an image service on the web that will allow users to upload and search random photos. at peak usage, up to 10,000 users worldwide will upload their images. the service will then overlay text on the uploaded images, which will then be published on the company website.which design should a solutions architect implement?
A. store the uploaded images in amazon elastic file system (amazon efs). send application log information about each image to amazon cloudwatch logs. create a fleet of amazon ec2 instances that use cloudwatch logs to determine which images need to be processed. place processed images in another directory in amazon efs. enable amazon cloudfront and configure the origin to be the one of the ec2 instances in the fleet.
B. store the uploaded images in an amazon s3 bucket and configure an s3 bucket event notification to send a message to amazon simple notification service (amazon sns). create a fleet of amazon ec2 instances behind an application load balancer (alb) to pull messages from amazon sns to process the images and place them in amazon elastic file system (amazon efs). use amazon cloudwatch metrics for the sns message volume to scale out ec2 instances. enable amazon cloudfront and configure the origin to be the alb in front of the ec2 instances.
C. store the uploaded images in an amazon s3 bucket and configure an s3 bucket event notification to send a message to the amazon simple queue service (amazon sqs) queue. create a fleet of amazon ec2 instances to pull messages from the sqs queue to process the images and place them in another s3 bucket. use amazon cloudwatch metrics for queue depth to scale out ec2 instances.enable amazon cloudfront and configure the origin to be the s3 bucket that contains the processed images.
D. store the uploaded images on a shared amazon elastic block store (amazon ebs) volume mounted to a fleet of amazon ec2 spot instances. create an amazon dynamodb table that contains information about each uploaded image and whether it has been processed. use an amazon eventbridge (amazon cloudwatch events) rule to scale out ec2 instances. enable amazon cloudfront and configure the origin to reference an elastic load balancer in front of the fleet of ec2 instances.


127, a company has implemented an ordering system using an event-driven architecture. during initial testing, the system stopped processing orders. further log analysis revealed that one order message in an amazon simple queue service (amazon sqs) standard queue was causing an error on the backend and blocking all subsequent order messages. the visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. a solutions architect needs to	tfr: faulty order messages and ensure that the system continues to process subsequent messages. which step should the solutions architect take to meet these requirements?
A. increase the backend processing timeout to 30 seconds to match the visibility timeout.
B. reduce the visibility timeout of the queue to automatically remove the faulty message. C.configure a new sqs fifo queue as a dead-letter queue to isolate the faulty messages. D.configure a new sqs standard queue as a dead-letter queue to isolate the faulty messages.

Mtfr: the dead-letter queue of a fifo queue must also be a fifo queue. similarly, the dead­ letter queue of a standard queue must also be a standard queue.


128, a multimedia company needs to deliver its video-on-demand (vod) content to its subscribers in a cost- effective way. the video files range in size from 1-15 gb and are typically viewed frequently for the first 6 months after creation, and then access decreases considerably. the company requires all video files to remain immediately available for subscribers. there are now roughly 30,000 files, and the company anticipates doubling that number over time. what is the most cost-effective solution for delivering the company's vod content?
A. store the video files in an amazon s3 bucket using s3 intelligent-tiering use amazon cloudfront to deliver the content with the s3 bucket as the origin.
B. use aws elemental mediaconvert and store the adaptive bitrate video files in amazon s3. configure an aws elemental mediapackage endpoint to deliver the content from amazon s3
C. store the video files in amazon elastic file system (amazon efs) standard. enable efs lifecycle management to move the video files to efs infrequent access after 6 months. create an amazon ec2 auto scaling group behind an elastic load balancer to deliver the content from amazon efs.
D. store the video files in amazon s3 standard. create s3 lifecycle rules to move the video files to s3 standard- infrequent access (s3 standard-ia) after 6 months and to s3 glacier deep
A. 
archive after 1 year. use amazon cloudfront to deliver the content with the s3 bucket as the origin.


 tfr: intelligent tiering "stores objects in four access tiers, optimized for frequent, infrequent, archive, and deep archive access". so, only problem with intelligent tiering is that if an object has not been accessed for significant amount of time, it will move the object to glacier or glacier deep archive. i am not sure if in such a situation the object can be accessed immediately.if complete vod solution is required then b all the aws vod documents will redirect there. https://dl.awsstatic.com/whitepapers/amazon-cloudfront­ for-media.pdf https://aws.amazon.com/solutions/implementations/video-on-demand-on­ aws/


129, a company is creating a rest api to share information with six of its partners based in the united states. the company has created an amazon api gateway regional endpoint. each of the six partners will access the api once per day to post daily sales figures.after initial deployment, the company observes 1,000 requests per second originating from 500 different Ip addresses around the world. the company believes this traffic is originating from a botnet and wants to secure its api while minimizing cost.which approach should the company take to secure its api?
A. create an amazon cloudfront distribution with the api as the origin. create an aws waf web ad with a rule to block clients that submit more than five requests per day. associate the web ad with the cloudfront distribution. configure cloudfront with an origin access identity (oa) and associate it with the distribution. configure api gateway to ensure only the oai can execute the post method.
B. create an amazon cloudfront distribution with the api as the origin. create an aws waf web ad with a rule to block clients that submit more than five requests per day. associate the web ad with the cloudfront distribution. add a custom header to the cloudfront distribution populated with an api key, configure the api to require an api key on the post method.
C. create an aws waf web ad with a rule to allow access to the ip addresses used by the six partners.associate the web ad with the api. create a resource policy with a request limit and associate it with the api. configure the api to require an api key on the post method.
D. create an aws waf web ad with a rule to allow access to the Ip addresses used by the six partners.associate the web ad with the api. create a usage plan with a request limit and associate it with the api create an api key and add it to the i usage plan.


130, a company is using aws organizations to manage multiple aws accounts. for security purposes, the company requires the creation of an amazon simple notification service (amazon sns) topic that enables integration with a third- party alerting system in all the organizations member accounts. a solutions architect used an aws cloudformation template to create the sns topic and stack sets to automate the deployment of cloudformation stacks. trusted access has been enabled in organizations. what should the solutions architect do to deploy the cloudformation stacksets in all aws accounts?
A. create a stack set in the organizations member accounts. use service-managed permissions. set deployment options to deploy to an organization. use cloudformation stacksets drift detection.
B. create stacks in the organizations member accounts. use self-service permissions. set deployment options to deploy to an organization. enable the cloudformation stacksets automatic deployment.
C. create a stack set in the organizations master account. use service-managed permissions.set deployment options to deploy to the organization. enable cloudformation stacksets automatic deployment.
D. create stacks in the organizations master account. use service- managed permissions set deployment options to deploy to the organization. enable cloudformation stacksets drift detection.



13L	a company hosts a large on-premises mysql database at its main office that supports an issue tracking system used by employees around the world. the company already uses aws for some workloads and has created an amazon route 53 entry for the database endpoint that points to the on-premises database.management is concerned about the database being a single point of failure and wants a solutions architect to migrate the database to aws without any data loss or downtime . which set of actions should the solutions architect implement?
A. create an amazon aurora db cluster. use aws database migration service (aws dms) to do a full load from the on-premises database to aurora. update the route 53 entry for the database to point to the aurora cluster endpoint, and shut down the on-premises database.
B. during nonbusiness hours, shut down the on-premises database and create a backup. restore this backup to an amazon aurora db cluster. when the restoration is complete, update the route 53 entry for the database to point to the aurora cluster endpoint, and shut down the on-premises database.
A. 
C. create an amazon aurora db cluster. use aws database migration service (aws dms) to do a full load with continuous replication from the on-premises database to aurora. when the migration is complete, update the route 53 entry for the database to point to the aurora cluster endpoint, and shut down the on-premises database.
D. create a backup of the database and restore it to an amazon aurora multi-master cluster. this aurora cluster will be in a master-master replication configuration with the on­ premises database. update the route 53 entry for the database to point to the aurora cluster endpoint, and shut down the on-premises database.


f@Hfr: "around the world" eliminates possibility for the maintenance window at night. the other difference is ability to leverage continuous replication in mysql to aurora case.


132, a company wants to migrate its corporate data center from on premises to the aws cloud. the data center includes physical servers and vms that use vmware and hyper-v an administrator needs to select the correct services to collect data for the initial migration discovery process. the data format should be supported by aws migration hub. the company also needs the ability to generate reports from the data.which solution meets these requirements?
A. use the aws agentless discovery connector for data collection on physical servers and all vms, store the collected data in amazon s3. query the data with s3 select. generate reports by using kibana hosted on amazon ec2.
B. use the aws application discovery service agent for data collection on physical servers and all vms.store the collected data in amazon elastic file system (amazon efs). query the data and generate reports with amazon athena
C. use the aws application discovery service agent for data collection on physical servers and hyper-v.use the aws agentless discovery connector for data collection on vmware. store the collected data in amazon s3. query the data with amazon athena. generate reports by using amazon quicksight.
D. use the aws systems manager agent for data collection on physical servers. use the aws agentless discovery connector for data collection on all vms. store, query, and generate reports from the collected data by using amazon redshift.



133, a company has multiple aws accounts as part of an organization created with aws organizations. each account has a vpc in the us-east-2 region and is used for either

production or development workloads amazon ec2 instances across production accounts need to communicate with each other, and ec2 instances across development accounts need to communicate with each other, but production and development instances should not be able to communicate with each other. to facilitate connectivity, the company created a common network account. the company used aws transit gateway to create a transit gateway in the us. -east-2 region in the network account and shared the transit gateway with the entire organization by using aws resource access manager. network administrators then attached vpcs in each account to the transit gateway, after which the ec2 instances were able to communicate across accounts. however, production and development accounts were also able to communicate with one another.which set of steps should a solutions architect take to ensure production traffic and development traffic are completely isolated?
A. modify the security groups assigned to development ec2 instances to block traffic from production ec2 instances. modify the security groups assigned to production ec2 instances to block traffic from development ec2 instances.
B. create a tag on each vpc attachment with a value of either production or development, according to the type of account being attached. using the network manager feature of aws transit gateway, create policies that restrict traffic between vpcs based on the value of this tag.
C. create separate route tables for production and development traffic. delete each account's association and route propagation to the default aws transit gateway route table. attach development vpcs to the development aws transit gateway route table and production vpcs to the production route table, and enable automatic route propagation on each attachment.
D. create a tag on each vpc attachment with a value of either production or development, according to the type of account being attached. modify the aws transit gateway routing table to route production tagged attachments to one another and development tagged attachments to one another.




134, a solutions architect works for a government agency that has strict disaster recovery requirements. all amazon elastic block store (amazon ebs) snapshots are required to be saved in at least two additional aws regions the agency also is required to maintain the lowest possible operational overhead.which solution meets these requirements?
A. configure a policy in amazon data lifecycle manager (amazon dim) to run once daily to copy the ebs snapshots to the additional regions.
B. use amazon eventbridge (amazon cloudwatch events) to schedule an aws lambda function to copy the ebs snapshots to the additional regions.
A. 
C. set up aws backup to create the ebs snapshots. configure amazon s3 cross-region replication to copy the ebs snapshots to the additional regions.
D. schedule amazon ec2 image builder to run once daily to create an ami and copy the ami to the additional regions.




135, a company uses aws organizations to manage one parent account and nine member accounts. the number of member accounts is expected to grow as the business grows. a security engineer has requested consolidation of aws cloudtrail logs into the parent account for compliance purposes. existing logs currently stored in amazon s3 buckets in each individual member account should not be lost. future member accounts should comply with the logging strategy.which operationally efficient solution meets these requirements?
A. create an aws lambda function in each member account with a cross-account role. trigger the lambda functions when new cloudtrail logs are created and copy the cloudtrail logs to a centralized s3 bucket. set up an amazon cloudwatch alarm to alert if cloudtrail is not configured properly.
B. configure cloudtrail in each member account to deliver log events to a central s3 bucket ensure the central s3 bucket policy allows putobject access from the member accounts. migrate existing logs to the central s3 bucket. set up an amazon cloudwatch alarm to alert if cloud trail is not configured properly.
C. configure an organization-level cloudtrail in the parent account to deliver log events to a central s3 bucket. migrate the existing cloud trail logs from each member account to the central s3 bucket.delete the existing cloud trail and logs in the member accounts.
D. configure an organization-level cloudtrail in the parent account to deliver log events to a central s3 bucket. configure cloudtrail in each member account to deliver log events to the central s3 bucket.

Mi-fr: creating a trail for an organization:if an aws account is added to an organization, the organization trail and service-linked role will be added to that aws account, and logging will begin for that account automatically in the organization trail. if you have created an organization in aws organizations, you can create a trail that will log all events for all aws accounts in that
organizati on.h ttps:/ / docs.aws.amazon.com/awscloudtrail /latest/userguide/creating-trail­ organizati on.html

136, a company is moving its on-premises oracle database to amazon aurora postgresql. the database has several applications that write to the same tables. the applications need to be migrated one by one with a month in between each migration management has expressed concerns that the database has a high number of reads and writes. the data must be kept in sync across both databases throughout tie migration.what should a solutions architect recommend?
A. use aws datasync tor the initial migration.use aws database migration service (aws dms] to create a change data capture (cdc) replication task and a table mapping to select all cables.
B. useavvs datasync for the initial migration.use aws database migration service (aws dms) to create a full load plus change data capture (cdc) replication task and a table mapping to select ail tables.
C. use the aws schema conversion led with aws database migration service (aws dms) using a memory optimized replication instance.create a tui load plus change data capture (cdc) replication task and a table mapping lo select all tables.
D. use the aws schema conversion tool with aws database migration service (aws dms) using a compute optimized implication instance.create a full load plus change data capture (cdc) replication task and a table mapping to select the largest tables.




137, an adventure company has launched a new feature on its mobile app. users can use the feature to upload their hiking and rafting photos and videos anytime. the photos and videos are stored in amazon s3 standard storage in an s3 bucket and are served through amazon cloudfront. the company needs to optimize the cost of the storage.a solutions architect discovers that most of the uploaded photos and videos are accessed infrequently after 30 days. however, some of the uploaded photos and videos are accessed frequently after 30 days. the solutions architect needs to implement a solution that maintains millisecond retrieval availability of the photos and videos at the lowest possible cost.which solution will meet these requirements?
A. configure s3 intelligent-tiering on the s3 bucket.
B. configure an s3 lifecycle policy to transition image objects and video objects from s3 standard to s3 glacier deep archive after 30 days.
C. replace amazon s3 with an amazon elastic file system (amazon efs) file system that is mounted on amazon ec2 instances.
D. add a cache-control: max-age header to the s3image objects and s3video objects. set the header to 30 days.
A. 



138, a company runs an online marketplace web application on aws. the application serves hundreds of thousands of users during peak hours. the company needs a scalable, near-real­ time solution to share the details of millions of financial transactions with several other internal applications. transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval.what should a solutions architect recommend to meet these requirements?
A. store the transactions data into amazon dynamodb.set up a rule in dynamodb to remove sensitive data from every transaction upon write. use dynamodb streams to share the transactions data with other applications.
B. stream the transactions data into amazon kinesis data firehose to store data in amazon dynamodb and amazons3.use aws lambda integration with kinesis data firehose to remove sensitive data. other applications can consume the data stored in amazon s3.
C. stream the transactions data into amazon kinesis data streams.use aws lambda integration to remove sensitive data from every transaction and then store the transactions data in amazon dynamodb.other applications can consume the transactions data off the kinesis data stream.
D. store the batched transactions data in amazon s3 as files.use aws lambda to process every file and remove sensitive data before updating the files in amazon s3. the lambda function then stores the data in amazon dynamodb.other applications can consume transaction files stored in amazon s3.

Mtfr: kinesis ds with dynamodb. as kinesis datafirehos cant store in dynamodb. firehose doesn't store data to dynamodb


139, a company hosts its application in the aws cloud. the application runs on amazon ec2 instances behind an elastic load balancer in an auto scaling group and with an amazon dynamodb table. the company wants to ensure the application can be made available in another aws region with minimal downtime. what should asolutions architect do to meet these requirements with the 1 east amount of downtime?
A. create an auto scaling group and a load balancer in the disaster recovery region. configure the dynamodb table as a global table. configure dns failover to point to the now disaster recovery region's load balancer.
A. 
B. create an aws cloudformation template to create ec2 instances, load balancers, and dynamodb tables to be executed when needed. configure dns failover to point to the new disaster recovery region's load balancer.
C. create an aws cloudformaticn template to create ec2 instances and a load balancer to be executed when needed.configure the dynamodb table as a global table. configure dns failover to point to the new disaster recovery region's load balancer.
D. create an auto scaling group and load balancer in the disaster recovery region. configure the dynamodb table as a global table. create an amazon cloudwatch alarm to trigger an aws lambda function that updates amazon route 53 pointing to the disaster recovery load balancer.

 It-tfr: let's think, using cloudformation nothing is created until necessary so how to configure route 53 to point to something that doesn't still exist? then how much time cloudformation take time to create and fullfill the dynamodb (especially if the db is big)?. this point to rule out b & cd is clearly wrong.a make sense because infrastructure is already ready, only to switch dns. you will not need to make any effort if some disaster happens. the system will automatically handle everything without launching cf templates (manually or automatically). just curious, you as an architect, how are you going without any downtime to understand that cf template should be run? sitting in front of the monitor and refreshing the web page with aws console? even in your case you will have to wait for failing health checks (or other triggers). but with option "a" if health checks are failed system will switch to failover configuration and that's it. no need to wait for resources being deployed


140, a company has a media catalog with metadata for each item in the catalog. different types of metadata are extracted from the media items by an application running on aws lambda. metadata is extracted according to a number of rules, with the output stored in an amazon elasticache for redis cluster. the extraction process is done in batches and takes around 40 minutes to complete. the update process is triggered manually whenever the metadata extraction rules change. the company wants to reduce the amount of time it takes to extract metadata from its media catalog. to achieve this, a solutions architect has split the single metadata extraction lambda function into a lambda function for each type of metadata.which additional steps should the solutions architect take to meet the requirements?
A. create an aws step functions workflow to run the lambda functions in parallel.create another step functions workflow that retrieves a list of media items and executes a metadata extraction workflow for each one.
A. 
B. create an aws batch compute environment for each lambda function.configure an aws batch job queue for the compute environment.create a lambda function to retrieve a list of media items and write each item to the job queue.
C. create an aws step functions workflow to run the lambda functions in parallel.create a lambdafunction to retrieve a list of media items and write each item to an amazon sqs queue. configure the sqs queue as an input to the step functions workflow.
D. create a lambda function to retrieve a list of media items and write each item to an amazon sqs queue.subscribe the metadata extraction lambda functions to the sqs queue with a large batch size.



141, a company has an automobile sales website that stores its listings in an database on amazon rds when an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.which design should a solutions architect recommend?
A. create an aws lambda function triggered when the database on amazon rds is updated to send the information to an amazon simple queue service (amazon sqs) queue for the targets to consume.
B. create an aws lambda function triggered when the database on amazon rds is updated to send the information to an amazon simple queue service (amazon sqs) fifo queue for the targets to consume.
C. subscribe to an rds event notification and send an amazon simple queue service (amazon sqs) queue fanned out to multiple amazon simple notification service (amazon sns) topics. use aws lambda functions to update the targets.
D. subscribe to an rds event notification and send an amazon simple notification service (amazon sns) topic fanned out to multiple amazon simple queue service (amazon sqs) queues use aws lambda functions to update the targets.


 tfr: a you can't use lambda directly with rds, rds sends the notification to sns which then can trigger a lambda. take a look https://docs.aws.amazon.com/lambda/latest/dg/services­ rds.html b same as a c the rds event notifications sends the notification using sns not sqs. d sounds about right. you subscribe to an rds event notification which sends to sns topic, which is fanned out to multiple amazon sqs queues.subscribing to amazon rds event notificationyou can create an amazon rds event notification subscription so you can be notified when an event occurs for a given db instance, db snapshot, db security group, or db
 
parameter group. the simplest way to create a subscription is with the rds console. if you choose to create event notification subscriptions using the di or api, you must create an amazon simple notification service topic and subscribe to that topic with the amazon sns console or amazon sns api


142, a company has a popular gaming platform running on aws. the application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. the application is deployed in every aws region. it runs on amazon ec2 instances that are part of auto scaling groups configured behind application load balancers (albs). a solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints.which solution meets these requirements?
A. configure an accelerator in aws global accelerator. add a listener for the port that the application listens on, and attach it to a regional endpoint in each region. add the alb as the endpoint.
B. create an amazon cloudfront distribution and specify the alb as the origin server. configure the cache behavior to use origin cache headers. use aws lambda functions to optimize the traffic.
C. create an amazon cloudfront distribution and specify amazon s3 as the origin server. configure the cache behavior to use origin cache headers. use aws lambda functions to optimize the traffic.
D. configure an amazon dynamodb database to serve as the data store for the application. create a dynamodb accelerator (dax) cluster to act as the in-memory cache for dynamodb hosting the application data.



143, one of the criteria for a new deployment is that the customer wants to use aws storage gateway. however you are not sure whether you should use gateway-cached volumes or gateway-stored volumes or even what the differences are. which statement below best describes those differences?
A. gateway-cached lets you store your data in amazon simple storage service (amazon s3) and retain a copy of frequently accessed data subsets locally.gateway-stored enables you to configure your on-premises gateway to store all your data locally and then asynchronously back up point-in-time snapshots of this data to amazon s3.
B. gateway-cached is free whilst gateway-stored is not.
A. 
C. gateway-cached is up to 10 times faster than gateway-stored.
D. gateway-stored lets you store your data in amazon simple storage service (amazon s3) and retain a copy of frequently accessed data subsets locally.gateway-cached enables you to configure your on-premises gateway to store all your data locally and then asynchronously back up point-in-time snapshots of this data to amazon s3.

,ff i'fr: volume gateways provide cloud-backed storage volumes that you can mount as internet small computer system interface (iscsi) devices from your on-premises application servers. the gateway supports the following volume configurations:gateway-cached volumes ?you store your data in amazon simple storage service (amazon s3) and retain a copy of frequently accessed data subsets locally. gateway-cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on­ premises. you also retain low- latency access to your frequently accessed data. gateway­ stored volumes ?if you need low-latency access to your entire data set, you can configure your on- premises gateway to store all your data locally and then asynchronously back up point-in-time snapshots of this data to amazon s3. this configuration provides durable and inexpensive off-site backups that you can recover to your local data center or amazon ec2. for example, if you need replacement capacity for disaster recovery, you can recover the backups to amazon ec2.


144, a company wants to improve the availability and performance of its stateless udp based workload. the workload is deployed on amazon ec2 instances in multiple aws regions. what should a solutions architect recommend to accomplish this?
A. place the ec2 instances behind network load balancers (nibs) in each region. create an accelerator using aws global accelerator. use the nibs as endpoints for the accelerator.
B. place the ec2 instances behind application load balancers (albs) in each region. create an accelerator using awsglobal accelerator. use the albs as endpoints for the accelerator.
C. place the ec2 instances behind network load balancers (nibs) in each region. create an amazon cloudfront distribution with an origin that uses amazon route 53 latency-based routing to route requests to the nibs.
D. place the ec2 instances behind application load balancers (albs) in each region. create an amazon cloudfront distribution with an origin that uses amazon route 53 latency-based routing to route requests to the albs


145, a media company has an application that tracks user clicks on its websites and performs analytics to provide near-real time recommendations. the application has a fleet of amazon ec2 instances that receive data from the websites and send the data to an amazon rds db instance. another fleet of ec2 instances hosts the portion of the application that is continuously checking changes in the database and executing sql queries to provide recommendations. management has requested a redesign to decouple the infrastructure.the
solution must ensure that data analysts are writing sql to Mi-fr: the data only. no data can be
lost during the deployment.what should a solutions architect recommend?
A. use amazon kinesis data streams to capture the data from the websites, kinesis data firehose to persist the data on amazon s3, and amazon athena to query the data.
B. use amazon kinesis data streams to capture the data from the websites, kinesis data analytics to query the data, and kinesis data firehose to persist the data on amazon s3.
C. use amazon simple queue service (amazon sqs) to capture the data from the websites, keep the fleet of ec2 instances, and change to a bigger instance type in the auto scaling group configuration.
D. use amazon simple notification service (amazon sns) to receive data from the websites and proxy the messages to aws lambda functions that execute the queries and persist the data.change amazon rds to amazon aurora serverless to persist the data.




146, a solutions architect is creating a new vpc design. there are two public subnet for the load balancer, two private subnets for web servers, and two private subnets for mysql. the web serves use only https. the solutions architect has already created a security group for the load balancer allowing port 443 from 0.0 0.0/0.company policy requires that each resource has the least access required to still be able to perform its tasks. which additional configuration strategy should the solution architect use to meet these requirements?
A. create a security group far the web servers and allow port 443 from0.0.0/0.create a security group tor the mysql serve's aid allow port 3306 from the web servers security group.
B. create a network acl for the web servers and allow port 443 from0.0.0/0.create a network acl for the mysql servers and allow port 3306 from the web servers security group
C. create a security group for the web servers and allow port 443 from the load balancer.create a security group tor the mysql servers and allow port 3306 from the web sewers security group
A. 
D. create a network ad for the web servers and allow port 443 from the web balancer.create a network ad for the mysql servers and allow port 3306 from the web servers security group.




147, a company that operates a web application on premises is preparing to launch a newer version of the application on aws. the company needs to route requests to either the aws-hosted or the on-premises- hosted application based on the url query string. the on­ premises application is not available from the internet, and a vpn connection is established between amazon vpc and the company's data center. the company wants to use an application load balancer (alb) for this launch.which solution meets these requirements?
A. use two albs: one for on premises and one for the aws resource. add hosts to each target group of each alb.route with amazon route 53 based on the url query string.
B. use two albs: one for on premises and one for the aws resource. add hosts to the target group of each alb.create a software router on an ec2 instance based on the url query string.
C. use one alb with two target groups: one for the aws resource and one for on premises. add hosts to each target group of the alb.configure listener rules based on the url query string.
D. use one alb with two aws auto scaling groups: one for the aws resource and one for on premises.add hosts to each auto scaling group. route with amazon route 53 based on the url query string.

Mi-fr: "the host-based routing feature allows you to write rules that use the host header to route traffic to the desired target group. today we are extending and generalizing this feature, giving you the ability to write rules (and route traffic) based on standard and custom http headers and methods, the query string, and the source ip address. "but a seems to be if you have both internal and external facing services then you'll need at least one alb for each. you can't mix internal and external services in the same alb. please clarify the doubt.


148, a company is moving its on-premises oracle database to amazon aurora postgresql. the database has several applications that write to the same tables. the applications need to be migrated one by one with a month in between each migration. management has expressed concerns that the database has a high number of reads and writes. the data must be kept in sync across both databases throughout the migration.what should a solutions architect recommend?

A. use aws datasync for the initial migration. use aws database migration service (aws dms) to create a change data capture (cdc) replication task and a table mapping to select all tables.
B. use aws datasync for the initial migration. use aws database migration service (aws dms) to create a full load plus change data capture (cdc) replication task and a table mapping to select all tables.
C. use the aws schema conversion tool with aws database migration service (aws dms) using a memory optimized replication instance. create a full load plus change data capture (cdc) replication task and a table mapping to select all tables.
D. use the aws schema conversion tool with aws database migration service (aws dms) using a compute optimized replication instance. create a full load plus change data capture (cdc) replication task and a table mapping to select the largest tables.




149, a company is designing an internet-facing web application. the application runs on amazon ec2 for linux- based instances that store sensitive user data in amazon rds mysql multi-az db instances. the ec2 instances are in public subnets, and the rds db instances are in private subnets. the security team has mandated that the db instances be secured against web-based attacks.what should a solutions architect recommend?
A. ensure the ec2 instances are part of an auto scaling group and are behind an application load balancer.configure the ec2 instance iptables rules to drop suspicious web traffic. create a security group for the db instances.configure the rds security group to only allow port 3306 inbound from the individual ec2 instances.
B. ensure the ec2 instances are part of an auto scaling group and are behind an application load balancer.move db instances to the same subnets that ec2 instances are located in. create a security group for the db instances.configure the rds security group to only allow port 3306 inbound from the individual ec2 instances.
C. ensure the ec2 instances are part of an auto scaling group and are behind an application load balancer.use aws waf to monitor inbound web traffic for threats.create a security group for the web application servers and a security group for the db instances.configure the rds security group to only allow port 3306 inbound from the web application server security group.
D. ensure the ec2 instances are part of an auto scaling group and are behind an application load balancer.use aws waf to monitor inbound web traffic for threats.configure the auto scaling group lo automatically create new db instances under heavy traffic.create a security group for the rds db instances. configure the rds security group to only allow port 3306 inbound.
A. 



150, a company has a custom application with embedded credentials that retrieves information from an amazon rds mysql db instance management says the application must be made more secure with the least amount of programming effort.what should a solutions architect do to meet these requirements?
A. use aws key management service (aws kms) customer master keys (cmks) to create keys.configure the application to load the database credentials from aws kms. enable automatic key rotation.
B. create credentials on the rds for mysql database for the application user and store the credentials in aws secrets manager.configure the application to load the database credentials from secrets manager. create an aws lambda function that rotates the credentials in secret manager.
C. create credentials on the rds for mysql database for the application user and store the credentials in aws secrets manager.configure the application to load the database credentials from secrets manager.set up a credentials rotation schedule for the application user in the rds for mysql database using secrets manager.
D. create credentials on the rds for mysql database for the application user and store the credentials in aws systems manager parameter.store configure the application to load the database credentials from parameter store.set up a credentials rotation schedule for the application user in the rds for mysql database using parameter store.



15L	a company plans to deploy an application in the aws cloud. the company will use amazon cloudfront to improve the user experience and overall application performance. incoming requests require http header manipulation for the origin to process the requests and provide a more customized user experience. which solution meets these requirements most cost-effectively?
A. create a lambda@edge function with a python runtime for viewer requests.
B. create a cloudfront function with a python runtime for origin requests.
C. create a lambda@edge function with a javascript runtime for origin requests.
D. create a cloudfront function with a javascript runtime for viewer requests.


152, a company has an amazon s3 bucket that contains millions of unencrypted objects. to comply with a recent security audit, a solutions architect needs to ensure that all objects are encrypted and needs to compile a list of objects that contain sensitive data. many applications access objects in the s3 bucket, and the development team has limited resources.which solution will meet these requirements with the least development effort?
A. run an amazon inspector report on the s3 bucket to identify sensitive data. create anew s3 bucket with default encryption enabled. transfer the unencrypted objects to the new s3 bucket update the applications to access the new s3 bucket.
B. run an amazon macie report on the s3 bucket to identify sensitive data. create a new s3 bucket with default encryption enabled. transfer the unencrypted objects to the new s3 bucket. update the applications to access the new s3 bucket.
C. run an amazon inspector report against the s3 bucket to identify sensitive data. modify the s3 bucket to enable default encryption. use an amazon s3inventory report and amazon s3 batch operations to encrypt the existing unencrypted objects in the same s3 bucket.
D. run an amazon macie report on the s3 bucket to identify sensitive data.modify the s3 bucket to enable default encryption. use an s3 inventory report and s3 batch operations to encrypt the existing unencrypted objects in the same s3 bucket.




153, to be fips 140-2 level 3 compliant, a company needs to store encryption keys. the keys need to be available between the hours of 8:00am and 6:00pm us eastern time, monday through friday. the keys must not be available outside of these hours. the company needs a highly available solution to store the encryption keys.which solution will meet these requirements most cost-effectively?
A. create an aws cloudhsm cluster with instances in multiple availability zones. store the cloudhsm metadata as standard parameters in aws systems manager parameter store. create and schedule aws lambda functions to delete and recreate the cloudhsm cluster from a cluster backup. configure the lambda functions to retrieve the necessary metadata from parameter store.
B. store the encryption keys in aws key management service (aws kms). create and schedule aws lambda functions to disable and enable the stored encryption keys as necessary.
C. create an aws cloudhsm cluster with instances in multiple availability zones. create an amazon dynamodb table. store the cloudhsm metadata in the dynamodb table. create and schedule aws lambda functions to delete and recreate the cloudhsm cluster from a cluster
A. 
backup. configure the lambda functions to retrieve the necessary metadata from the dynamodb table.
D. store the encryption keys in aws key management service (aws kms) create and schedule aws lambda functions to remove and add key access policies as necessary.



154, a company provides a three-tier web application to its customers. each customer has an aws account in which the application is deployed, and these accounts are members of the company's organization in aws organizations. to protect its customers' aws accounts and applications the company wants to monitor them for unusual and unexpected behavior.the company needs to analyze and monitor customer vpc flow logs. aws cloudtrail logs, and dns logs. what should a solutions architect do to meet these requirements?
A. designate an account in the organization as the aws shield master account enable shield and shield logs in every account and invite the accounts to join the shield master account analyze shield findings m the shield master account
B. designate an account in the organization as the amazon guardduty master account enable guardduty in every account and invite the accounts to join the guardduty master account analyze guardduty finding in the guardduty master account
C. designate an account in the organization as the aws waf master account enable aws waf and aws waf logs in every account and invite the accounts to join the aws waf master account analyze aws waf logs in the aws waf master account
D. designate an account in the organization as the aws resource access manager (aws ram) master accountenable aws ram in every account, and invite the accounts to join the aws ram master account analyze aws ram logs in the aws ram master account



155, a company is hosting a monolithic rest-based api for a mobile app on five amazon ec2 instances in public subnets of a vpc. mobile clients connect to the api by using a domain name that is hosted on amazon route 53. the company has created a route 53 multivalue answer routing policy with the ip addresses of all the ec2 instances. recently, the app has been overwhelmed by large and sudden increases to traffic. the app has not been able to keep up with the traffic. a solutions architect needs to implement a solution so that the app can handle the new and varying load. which solution will meet these requirements with the least operational overhead?

A. separate the api into individual aws lambda functions. configure an amazon api gateway restapi with lambda integration for the backend. update the route 53record to point to the api gateway api.
B. containerize the api logic. create an amazon elastic kubernetes service (amazon eks) cluster. run the containers in the cluster by using amazon ec2. create a kubernetes ingress. update the route 53record to point to the kubernetes ingress.
C. create an auto scaling group. place all the ec2 instances in the auto scaling group. configure the auto scaling group to perform scaling actions that are based on cpu utilization. create an aws lambda function that reacts to auto scaling group changes and updates the route 53 record.
D. create an application load balancer (alb) in front of the api. move the ec2instances to private subnets in the vpc.add the ec2 instances as targets for the alb. update the route 53record to point to the alb.




156, a company stores a large amount of customer data in an amazon s3 bucket that uses the s3 standard storage class. the amount of data will continue to grow. the company needs to reduce the cost of the data storage. the company needs to accesssome customer data frequently within the first 6 months. however, the company rarely needs to access other customer data during that period. the company rarely needs to access data that is 180 days or older, but the company must have the ability to access this old data within 6 hours. which storage strategy will meet these requirements most cost-effectively?
A. implement s3 intelligent-tiering on the existing s3 bucket. activate the deep archive access tier. set the days until transition to 180.
B. implement s3 intelligent-tiering on the existing s3 bucket. activate the archive access tier. set the days until transition to 180.
C. create a new s3lifecycle configuration for the existing s3 bucket. configure the lifecycle rule for all objects to move data to s3 glacier flexible retrieval after 180 days.
D. create anew s3lifecycle configuration for the existing s3bucket. configure the lifecycle rule for all objects to move data to s3 glacier deep archive after 180 days.




157, a company has migrated a legacy application to the aws cloud. the application runs on three amazon ec2 instances that are spread across three availability zones. one ec2instance

is in each availability zone. the ec2 instances are running in three private subnets of the vpc and are set up as targets for an application load balancer (alb) that is associated with three public subnets. the application needs to communicate with on- premises systems. only traffic from ip addresses in the company's ip address range are allowed to access the on­ premises systems. the company's security team is bringing only one ip address from its internal ip address range to the cloud. the company has added this ip address to the allow list for the company firewall. the company also has created an elastic ip address for this ip address.a solutions architect needs to create a solution that gives the application the ability to communicate with the on- premises systems. the solution also must be able to mitigate failures automatically.which solution will meet these requirements?
A. deploy three nat gateways, one in each public subnet. assign the elastic ip address to the nat gateways.turn on health checks for the nat gateways. if a nat gateway fails a health check, recreate the nat gateway and assign the elastic ip address to the new nat gateway.
B. replace the alb with a network load balancer (nib). assign the elastic ip address to the nib. turn on health checks for the nib. in the case of a failed health check, redeploy the nib in different subnets.
C. deploy a single nat gateway in a public subnet.assign the elastic ip address to the nat gateway. use amazon cloudwatch with a custom metric to monitor the nat gateway. if the nat gateway is unhealthy, invoke an aws lambda function to create a new nat gateway in a different subnet. assign the elastic ip address to the new nat gateway.
D. assign the elastic ip address to the alb. create an amazon route 53simple record with the elastic ip address as the value. create a route 53 health check. in the case of a failed health check, recreate the alb in different subnets.




158, a company hosts historical weather records in amazon s3. the records are downloaded from the company's website by a way of a url that resolves to a domain name. users all over the world access this content through subscriptions. a third-party provider hosts the company's root domain name, but the company recently migrated some of its services to amazon route 53. the company wants to consolidate contracts, reduce latency for users, and reduce costs related to serving the application to subscribers.which solution meets these requirements?
A. create a web distribution on amazon cloudfront to serve the s3 content for the application. create a cname record in a route 53 hosted zone that points to the cloudfront distribution, resolving to the application's url domain name.
A. 
B. create a web distribution on amazon cloudfront to serve the s3 content for the application. create an alias record in the amazon route 53 hosted zone that points to the cloudfront distribution, resolving to the application's url domain name.
C. create an a record in a route 53 hosted zone for the application. create a route 53 traffic policy for the web application, and configure a geolocation rule. configure health checks to check the health of the endpoint and route dns queries to other endpoints if an endpoint is unhealthy.
D. create an a record in a route 53 hosted zone for the application. create a route 53 traffic policy for the web application, and configure a geoproximity rule. configure health checks to check the health of the endpoint and route dns queries to other endpoints if an endpoint is unhealthy.

Mi-fr: cname is for the real dns servers we all know and love.alias is dns but for aws specifically. special record for aws. create a web distribution on amazon cloudfront to serve the s3 content for the application. create an alias record in the amazon route 53 hosted zone that points to the cloudfront distribution, resolving to the application's url domain name.


159, a company uses aws organizations to manage its aws accounts. the company needs a list of all its amazon ec2 instances that have underutilized cpu or memory usage. the company also needs recommendations for how to downsize these underutilized instances. which solution will meet theserequirements with the least effort?
A. install a cpu and memory monitoring tool from aws marketplace on all the ec2 instances. store the findings in amazon s3.implement a python script to identify underutilized instances. reference ec2 instance pricing information for recommendations about downsizing options.
B. install the amazon cloudwatch agent on all the ec2 instances by using aws systems manager.retrieve the resource optimization recommendations from aws cost explorer in the organization's management account. use the recommendations to downsize underutilized instances in all accounts of the organization.
C. install the amazon cloudwatch agent on all the ec2 instances by using aws systems manager.retrieve the resource optimization recommendations from aws cost explorer in each account of the organization. use the recommendations to downsize underutilized instances in all accounts of the organization.
D. install the amazon cloudwatch agent on all the ec2 instances by using aws systems manager.create an aws lambda function to extract cpu and memory usage from all the ec2instances. store the findings as files in amazon s3.use amazon athena to find
A. 
underutilized instances. reference ec2 instance pricing information for recommendations about downsizing options.



160, a company has resources in on-premises networks and in multiple vpcs on aws. the company needs a networking solution that will give the on-premises resources the ability to communicate with the vpc resources. the vpc resources also must be able to communicate with each other.the company has configured a customer gateway device on premises and has attached virtual private gateways to each vpc.the company wants a scalable solution so that the company can easily add more vpcs in the future.how should a solutions architect configure the network connectivity to meet these requirements with the least operational overhead?
A. create an individual aws site-to-site vpn connection between the customer gateway device and each of the virtual private gateways. configure vpc peering connections between all the vpcs.
B. create a transit gateway. create an aws site-to-site vpn attachment between the customer gateway device and the transit gateway. attach the vpcs to the transit gateway. remove the virtual private gateways.
C. set up a connection through aws direct connect. create vifs to provide access to the virtual private gateways. configure vpc peering connections between all the vpcs.
D. create a transit vpc. create vpn connections from the transit vpc to each virtual private gateway.create aws site-to-site vpn connections between the customer gateway device and the transit vpc.



16L	a company is designing a new application that uses amazon ec2 instances for compute and an amazon aurora mysql database for its data store. the application will allow users to access sensitive information.the company is concerned about the potential of database credential leakage. the company needs a solution that uses short-lived credentials that rotate as frequently as possible.which solution will meet these requirements?
A. save the database credentials as a secret in aws secrets manager. configure secrets manager to rotate the credentials every 24 hours. create an ec2 instance profile that contains an iam role that allows the ec2 instances to access the secret in secrets manager. attach the instance profile to the ec2 instances. configure the application to retrieve the credentials from secrets manager.
A. 
B. save the database credentials as a parameter in aws systems manager parameter store. create an aws lambda function to rotate the credentials every 24hours and to update the parameter. create an ec2 instance profile that contains an iam role that allows the ec2 instances to access the parameter in parameter store. attach the instance profile to the ec2 instances. configure the application to retrieve the credentials from parameter store.
C. implement iam database authentication in the application data layer. set up an iam role that grants the compute resources access to the rds-db:connect action.
D. save the database credentials in a file on the ec2 instances configure the application to retrieve the credentials from the file. create a cron job to rotate the credentials every 24 hours.




162, a company is migrating an application from on-premises infrastructure to the aws cloud. during migration design meetings, the company expressed concerns about the availability and recovery options for its legacy windows file server. the file server contains sensitive business-critical data that cannot be recreated in the event of data corruption or data loss. according to compliance requirements, the data must not travel across the public internet. the company wants to move to aws managed services where possible.the company decides to store the data in an amazon fsx for windows file server file system.a solutions architect must design a solution that copies the data to another aws region for disaster recovery (dr) purposes.which solution will meet these requirements?
A. create a destination amazon s3 bucket in the dr region. establish connectivity between the fsx for windows file server file system in the primary region and the s3 bucket in the dr region by using amazon fsx file gateway. configure the s3 bucket as a continuous backup source in fsx file gateway.
B. create an fsx for windows file server file system in the dr region.establish connectivity between the vpc in the primary region and the vpc in the dr region by using aws site-to-site vpn. configure aws datasync to communicate by using vpn endpoints.
C. create an fsx for windows file server file system in the dr region.establish connectivity between the vpc in the primary region and the vpc in the dr region by using vpc peering. configure aws datasync to communicate by using interface vpc endpoints with aws privatelink.
D. create an fsx for windows file server file system in the dr region. establish connectivity between the vpc in the primary region and the vpc in the dr region by using aws transit gateway in each region. use aws transfer family to copy files between the fsx for windows file server file system in the primary region and the fsx for windows file server file system in the dr region over the private aws backbone network
A. 



163, a company needs to gather data from an experiment in a remote location that does not have internet connectivity. during the experiment, sensors that are connected to a local network will generate 6 tb of data in a proprietary format over the course of 1 week. the sensors can be configured to upload their data files to an ftp server periodically, but the sensors do not have their own ftp server. the sensors also do not support other protocols. the company needs to collect the data centrally and move the data to object storage in the aws cloud as soon as possible after the experiment.which solution will meet these requirements?
A. order an aws snowball edge compute optimized device. connect the device to the local network.configure aws datasync with a target bucket name, and unload the data over nfs to the device. after the experiment, return the device to aws so that the data can be loaded into amazon s3.
B. order an aws snowcone device, including an amazon linux 2aml. connect the device to the local network. launch an amazon ec2instance on the device. create a shell script that periodically downloads data from each sensor.after the experiment, return the device to aws so that the data can be loaded as an amazon elastic block store (amazon ebs) volume.
C. order an aws snowcone device, including an amazon linux 2aml. connect the device to the local network. launch an amazon ec2instance on the device. install and configure an ftp server on the ec2instance. configure the sensors to upload data to the ec2 instance.after the experiment, return the device to aws so that the data can be loaded into amazon s3.
D. order an aws snowcone device. connect the device to the local network. configure the device to use amazon fsx. configure the sensors to upload data to the device. configure aws datasync on the device to synchronize the uploaded data with an amazon s3 bucket. return the device to aws so that the data can be loaded as an amazon elastic block store (amazon ebs) volume.



164, a company has an on-premises volume backup solution that has reached its end oflife. the companywants to use aws as part of a new backup solution and wants to maintain local access to all the data while it is backed up on aws.the company wants to ensure that the
data backed up on aws is automatically and securely transferred. which solution meets these requirements?
A. use aws snowball to migrate data out of the on-premises solution to amazon s3. configure on- premises systems to mount the snowball s3 endpoint to provide local access to the data.
A. 
B. use aws snowball edge to migrate data out of the on-premises solution to amazon s3. use the snowball edge file interface to provide on-premises systems with local access to the data.
C. use aws storage gateway and configure a cached volume gateway. run the storage gateway software appliance on premises and configure a percentage of data to cache locally.mount the gateway storage volumes to provide local access to the data.
D. use aws storage gateway and configure a stored volume gateway.run the storage gateway software appliance on premises and map the gateway storage volumes to on-premises storage. mount the gateway storage volumes to provide local access to the data.

It-tfr:  https:/ / docs.aws.amazon.com/snowball/latest/developer-guide/bestpractices.html


165, a company recently migrated a message processing system to aws. the system receives messages into an activemq queue running on an amazon ec2 instance. messages are processed by a consumer application running on amazon ec2. the consumer application processes the messages and writes results to a mysql database running on amazon ec2. the company wants this application to be highly available with low operational complexity.which architecture offers the highest availability?
A. add a second activemq server to another availability zone. add an additional consumer ec2 instance in another availability zone. replicate the mysql database to another availability zone.
B. use amazon mq with active/standby brokers configured across two availability zones. add an additional consumer ec2 instance in another availability zone. replicate the mysql database to another availability zone.
C. use amazon mq with active/standby brokers configured across two availability zones. add an additional consumer ec2 instance in another availability zone. use amazon rds for mysql with multi- az enabled.
D. use amazon mq with active/standby brokers configured across two availability zones. add an auto scaling group for the consumer ec2 instances across two availability zones use amazon rds for mysql with multi-az enabled.



166, a company prefers to limit running amazon ec2 instances to those that were launched from amis pre- approved by the information security department. the development team has an agile continuous integration and deployment process that cannot be stalled by the

solution. which method enforces the required controls with the least impact on the development process? (choose two.)
A. use iam policies to restrict the ability of users or other automated entities to launch ec2 instances based on a specific set of pre-approved amis, such as those tagged in a specific way by information security.
B. use regular scans within amazon inspector with a custom assessment template to determine if the ec2 instance that the amazon inspector agent is running on is based upon a pre-approved ami. if it is not, shut down the instance and inform information security by email that this occurred.
C. only allow launching of ec2 instances using a centralized devops team, which is given work packages via notifications from an internal ticketing system. users make requests for resources using this ticketing tool, which has manual information security approval steps to ensure that ec2 instances are only launched from approved amis.
D. use aws config rules to spot any launches of ec2 instances based on non-approved amis, trigger an aws lambda function to automatically terminate the instance, and publish a message to an amazon sns topic to inform information security that this occurred.
E. use a scheduled aws lambda function to scan through the list of running instances within the virtual private cloud (vpc) and determine if any of these are based on unapproved amis.publish a message to an sns topic to inform information security that this occurred and then shut down the instance.


tfr: b aws inspector is used to find security vulnerability, not used to find ami c not agile. e scheduled lambda is not athing, you need cloudwatch event to trigger lambda


16 7, a company has a security event whereby an amazon s3 bucket with sensitive information was made public. company policy is to never have public s3 objects, and the compliance team must be informed immediately when any public objects are identified. how can the presence of a public s3 object be detected, set to trigger alarm notifications, and automatically remediated in the future? (choose two.)
A. turn on object-level logging for amazon s3. turn on amazon s3 event notifications to notify by using an amazon sns topic when a putobject api call is made with a public-read permission.
B. configure an amazon cloudwatch events rule that invokes an aws lambda function to secure the s3 bucket.
A. 
C. use the s3 bucket permissions for aws trusted advisor and configure a cloudwatch event to notify by using amazon sns.
D. turn on object-level logging for amazon s3. configure a cloudwatch event to notify by using an sns topic when a putobject api call with public-read permission is detected in the aws cloudtrail logs.
E. schedule a recursive lambda function to regularly change all object permissions inside the s3 bucket.

Mi-fr: triggering the remediation lambda function with cloudwatch event is more efficient.a s3 event may be lost in some cases, and could take up to minutes to arrive https:/ / docs.aws.amazon.com/amazons3/latest/ dev/notificationhowto.html s3 event message does not contain information regarding permission
https:/ / docs.aws.amazon.com/amazons3/latest/dev/notification-content- structure.htmlc we could take advice from trust advisor, but use a policy for trust advisor not going to help...


168, anycompany has acquired numerous companies over the past few years. the cio for anycompany would like to keep the resources for each acquired company separate. the cio also would like to enforce a chargeback model where each company pays for the aws services it uses the solutions architect is tasked with designing an aws architecture that allows anycompany to achieve the following:*implementing a detailed chargeback mechanism to ensure that each company pays for the resources it uses.*anycompany can pay for aws services for all its companies through a single invoice.*developers in each acquired company have access to resources in their company only.*developers in an acquired company should not be able to affect resources in their company only.*a single identity store is used to authenticate developers across all companies. which of the following approaches would meet these requirements? (choose two.)
A. create a multi-account strategy with an account per company. use consolidated billing to ensure that anycompany needs to pay a single bill only.
B. create a multi-account strategy with a virtual private cloud (vpc) for each company.reduce impact across companies by not creating any vpc peering links. as everything is in a single account, there will be a single invoice. use tagging to create a detailed bill for each company.
C. create iam users for each developer in the account to which they require access. create policies that allow the users access to all resources in that account. attach the policies to the iam user.
A. 
D. create a federated identity store against the company's active directory. create iam roles with appropriate permissions and set the trust relationships with aws and the identity store.use aws sts to grant users access based on the groups they belong to in the identity store.
E. create a multi-account strategy with an account per company. for billing purposes, use a tagging solution that uses a tag to identify the company that creates each resource.


 tfr: https://aws.amazon.com/blogs/security/how-to-set-up-uninterrupted-federated­ user-access-to-aws-using- ad-fs/


169, a company runs a three-tier application in aws. users report that the application performance can varygreatly depending on the time of day and functionality being accessed. the application includes the following components:*eight t2.large front-end web servers that serve static content and proxy dynamic content from the application tier.*four t2.large application servers.*one db.m4.large amazon rds mysql multi-az db instance. operations has determined that the web and application tiers are network constrained. which of the following should cost effective improve application performance? (choose two.)
A. replace web and app tiers with t2.xlarge instances
B. use aws auto scaling and m4.large instances for the web and application tiers
C. convert the mysql rds instance to a self-managed mysql cluster on amazon ec2
D. create an amazon cloudfront distribution to cache content
E. increase the size of the amazon rds instance to db.m4.xlarge


Mtfr: as the constraint is network, t2.xlarge has the same network performance as m4.large, but more expensive https://aws.amazon.com/ec2/pricing/on­ demand/https://aws.amazon.com/ec2/instance-types/


170, a company wants to migrate its website from an on-premises data center onto aws. at the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. the company's security policy states that privileges and network permissions must be configured according to best practice, using least privilege. a solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an amazon ecs

cluster.what steps are required after the deployment to meet the requirements? (choose two.)
A. create tasks using the bridge network mode.
B. create tasks using the awsvpc network mode.
C. apply security groups to amazon ec2 instances, and use iam roles for ec2 instances to access other resources.
D. apply security groups to the tasks, and pass iam credentials into the container at launch time to access other resources.
E. apply security groups to the tasks, and use iam roles for tasks to access other resources.


 tfr: a as in bridge mode all containers in the same instance share the same security group of the instance, we could open ports that are not necessary. this is not good for least privilege. b as each task gets its own eni and security group, we could do fine grained permission here c if we don't picka, this is not necessary d pass iam credential is bad practicee https:/ / docs.aws.amazon.com/amazonecs/latest/developerguide/task-iam­ roles.html


171, a company has detected to move some workloads onto aws to create a grid environment to run market analytics. the grid will consist of many similar instances, spun­ up by a job-scheduling function. each time a large analytics workload is completed, a new vpc is deployed along with job scheduler and grid nodes.multiple grids could be running in parallel. key requirements are:*grid instances must communicate with amazon s3 retrieve data to be processed.*grid instances must communicate with amazon dynamodb to track intermediate data,*the job scheduler need only to communicate with the amazon ec2 api to start new grid nodes. a key requirement is that the environment has no access to the internet, either directly or via the onpremises proxy. however, the application needs to be able to seamlessly communicate to amazon s3, amazon dynamodb, andamazon ec2 api, without the need for reconfiguration for each new deployment. which of the following should the solutions architect do to achieve this target architecture? (choose three.)
A. enable vpc endpoints for amazon s3 and dynamodb.
B. disable private dns name support.
C. configure the application on the grid instances to use the private dns name of the amazon s3 endpoint.
D. populate the on-premises dns server with the private ip addresses of the ec2 endpoint.
A. 
E. enable an interface vpc endpoint for ec2.
F. configure amazon s3 endpoint policy to permit access only from the grid nodes.

 It-tfr: a https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html b private dns is needed for interface vpc endpointc s3 endpoint is gateway endpointprivate dns name of s3 endpoint is not a thing. you will need to use prefix list id for the route table and continue to use the s3 dns name for the servicehttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#vpc­ endpoints-routing https:/ / docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints­ s3.html d this is necessary as the default ec2 endpoint route is only populated in vpc route table, and private ip for interface endpoint dns name is required in the dns server.e
https:/ / docs.aws.amazon.com/vpc/latest/userguide/vpce-interface.html f s3  is gateway endpoint and you cannot limit principal in the endpoint policy https://docs.aws.amazon.com/vpc/ latest/userguide/vpc-endpoints-access.html#vpc­ endpoint-policies


172, a company has a web service deployed in the following two aws regions: us-west-2 and us-east-1. each aws region runs an identical version of the web service. amazon route 53 is used to route customers to the aws region that has the lowest latency. the company wants to improve the availability of the web service in case an outage occurs in one of the two aws regions. a solutions architect has recommended that a route 53 health check be performed. the health check must detect a specific text on an endpoint. what combination of conditions should the endpoint meet to pass the route 53 health check? (choose two.)
A. the endpoint must establish a tcp connection within 10 seconds.
B. the endpoint must return an http 200 status code.
C. the endpoint must return an http 2xx or 3xx status code.
D. the specific text string must appear within the first 5,120 bytes of the response.
E. the endpoint must respond to the request within the number of seconds specified when creating the health check


 It-tfr: a the limit is 4 secondsb 2xx or 3xx is good.e must response within 2 secondshttps: / / docs.aws.amazon.com/route53/latest/developerguide/dns-failover­ determining-health-of- endpoints.html#dns- failover-determining-health-of-endpoints­ monitor-endpoint
 
173, a company needs to cost-effectively persist small data records (up to 1 kib) for up to 30 days. the data is read rarely. when reading the data, a 5-minute delay is acceptable. which of the following solutions achieve this goal? (choose two.)
A. use amazon s3 to collect multiple records in one s3 object. use a lifecycle configuration to move data to amazon glacier immediately after write. use expedited retrievals when reading the data.
B. write the records to amazon kinesis data firehose and configure kinesis data firehose to deliver the data to amazon s3 after 5 minutes. set an expiration action at 30 days on the s3 bucket.
C. use an aws lambda function invoked via amazon api gateway to collect data for 5 minutes. write data to amazon s3 just before the lambda execution stops.
D. write the records to amazon dynamodb configured with a time to live (ttl) of 30 days.read data using the getitem or batchgetitem call.
E. write the records to an amazon elasticache for redis. configure the redis append-only file (aof) persistence logs to write to amazon s3. recover from the log if the elasticache instance has failed.


fAIHfr: modify on ad->bda glacier retrieval can be up to 1-5 mins, and glacier has a minimum size charge of 40kb, but the minimum storage time charge is 90 days, even though it is still much cheaper than standard s3 https:/ /
docs.aws.amazon.com/amazonglacier /latest/ dev/ downloading-an-archive-two-steps.html https:// aws.amazon.com/s3/storage-classes/b i think it means buffer interval for firehose here. the cost is tricky as each record round up to the nearest 5 kb for charging, as the record is all 1 kb, we could pay 5 times more in this case for firehose. https:// docs.aws.amazon.com/firehose/latest/dev/basic-deliver.html#frequency c this is not a really robust solution as we have long running lambda, which is not what lambda intend to do. it will be quite costly. api gateway will also timeout after 30s.d aof write to s3 is not supported https://docs.aws.amazon.com/amazonelasticache/latest/red-ug/redisaof.html


174, a company wants to replace its call system with a solution built using aws managed services. the company call center would like the solution to receive calls, create contact flows, and scale to handle growth projections.the call center would also like the solution to use deep learning capabilities to recognize the intent of the callers and handle basic tasks, reducing the need to speak an agent. the solution should also be able to query business

applications and provide relevant information back to calls as requested. which services should the solution architect use to build this solution? (choose three.)
A. amazon rekognition to identity who is calling.
B. amazon connect to create a cloud-based contact center.
C. amazon alexa for business to build conversational interface.
D. aws lambda to integrate with internal systems.
E. amazon lex to recognize the intent of the caller.
F. amazon sqs to add incoming callers to a queue.


 tfr: a recognition is for image and video b amazon connect is a call centre servicec alexa is for devicese lex is used to build conversational interfacef caller queue is manage by amazon connect as well, and sqs is not designed for this.


175, the company security team queries that all data uploaded into an amazon s3 bucket must be encrypted. the encryption keys must be highly available and the company must be able to control access on a per- user basis, with different users having access to different encryption keys. which of the following architectures will meet these requirements? (choose two.)
A. use amazon s3 server-side encryption with amazon s3-managed keys. allow amazon s3 to generate an aws/s3 master key, and use iam to control access to the data keys that are generated.
B. use amazon s3 server-side encryption with aws kms-managed keys, create multiple customer master keys, and use key policies to control access to them.
C. use amazon s3 server-side encryption with customer-managed keys, and use aws cloudhsm to manage the keys. use cloudhsm client software to control access to the keys that are generated.
D. use amazon s3 server-side encryption with customer-managed keys, and use two aws cloudhsm instances configured in high-availability mode to manage the keys. use the cloud hsm client software to control access to the keys that are generated.
E. use amazon s3 server-side encryption with customer-managed keys, and use two aws cloudhsm instances configured in high-availability mode to manage the keys. use iam to control access to the keys that are generated in cloudhsm.
A. 

Mtfr: -b & c-.a - s3 is managing the keys - so nob - we all agree becasue manageed by kms with multipel keys c - cloud hsm is a service - not to be deployed on instance. client get deployed on instance d - refer to c - there is no hsm instancee - refer to c - there is no hsm instance-"b" & "d".a: customer can not control the keys!b: aws-kms managed keys, allow the user to create master keys, and control them. it is high available as it is a managed service by aws.c: cloudhsm can be high available by including a second instance in different az. d: meet the requirement of management and high availability.e: managing the keys by cloudhsm client, not iam user!! dcloudhsm instancehigh availability mode. cloudhsmha clustermulti-azhsmyou can create a cluster that has from 1 to 28 hsms (the default limit is 6 hsms per aws account per aws region). you can place the hsms in different availability zones in an aws region. adding more hsms to a cluster provides higher performance. spreading clusters across availability zones provides redundancy and high availability.when you create an aws cloudhsm cluster with more than one hsm, you automatically get load balancing. when you create the hsms in different aws availability zones, you automatically get high availability.a s3 generated keys cannot be managed cone hsm is not hae cloudhsm cannot communicate with any aws services


176, a solutions architect is designing a multi-account structure that has 10 existing accounts. the design must meet the following requirements:*consolidate all accounts into one organization.*allow full access to the amazon ec2 service from the master account and the secondary accounts.*minimize the effort required to add additional secondary accounts. which combination of steps should be included in the solution? (choose two.)
A. create an organization from the master account. send invitations to the secondary accounts from the master account. accept the invitations and create an ou.
B. create an organization from the master account. send a join request to the master account from each secondary account. accept the requests and create an ou.
C. create a vpc peering connection between the master account and the secondary accounts. accept the request for the vpc peering connection.
D. create a service control policy (scp) that enables full ec2 access, and attach the policy to the ou.
E. create a full ec2 access policy and map the policy to a role in each account. trust every other account to assume the role.


tfr:
https:/ / docs.aws.amazon.com/organizations /latest/userguide/ orgs_manage_accounts_invi tes.html


177, a company plans to move regulated and security-sensitive businesses to aws. the security team is developing a framework to validate the adoption of aws best practice and industryrecognized compliance standards. the aws management console is the preferred method for teams to provision resources. which strategies should a solutions architect use to meet the business requirements and continuously assess, audit, and monitor the configurations of aws resources? (choose two.)
A. use aws config rules to periodically audit changes to aws resources and monitor the compliance of the configuration. develop aws config custom rules using aws lambda to establish a testdriven development approach, and further automate the evaluation of configuration changes against the required controls.
B. use amazon cloudwatch logs agent to collect all the aws sdk logs. search the log data using a pre- defined set of filter patterns that machines mutating api calls. send notifications using amazon cloudwatch alarms when unintended changes are performed. archive log data by using a batch export to amazon s3 and then amazon glacier for a long-term retention and auditability.
C. use aws cloud trail events to assess management activities of all aws accounts. ensure that cloudtrail is enabled in all accounts and available aws services. enable trails, encrypt
cloud trail event log files with an aws kms key,and monitor recorded activities with cloudwatch logs.
D. use the amazon cloudwatch events near-real-time capabilities to monitor system events patterns, and trigger aws lambda functions to automatically revert non-authorized changes in aws resources. also, target amazon sns topics to enable notifications and improve the response time of incident responses.
E. use cloud trail integration with amazon sns to automatically notify unauthorized api activities. ensure that cloudtrail is enabled in all accounts and available aws services.evaluate the usage oflambda functions to automatically revert non-authorized changes in aws resources.


 tfr: b management console do not go through sdk c d need cloudtrail to log resource change to cloudwatche cloudtrail to sns has no filtering so you will need to send all the logs. https:/ / docs.aws.amazon.com/ awscloudtrail/latest/userguide/configure-sns­ notifications-for- cloudtrail.html#configure-cloudtrail-to-send- notifications
 
178, a company deployed a three-tier web application in two regions: us-east-1 and eu­ west-1. the application must be active in both regions at the same time. the database tier of the application uses a single amazon rds aurora database globally, with a master in us-east- 1 and a read replica in eu-west-1. both regions are connected by a vpn. the company wants to ensure that the application remains available even in the event of a regionlevelfailure of all of the application's components. it is acceptable for the application to be in readonly mode for up to 1 hour. the company plans to configure two amazon route 53 record sets, one for each of the regions. how should the company complete the configuration to meet its requirements while providing the lowest latency for the application end-users? (choose two.)
A. use failover routing and configure the us-east-1 record set as primary and the eu-west-1 record set as secondary. configure an http health check for the web application in us-east-1, and associate it to the us-east-1 record set.
B. use weighted routing and configure each record set with a weight of 50. configure an http health check for each region, and attach it to the record set for that region.
C. use latency-based routing for both record sets. configure a health check for each region and attach it to the record set for that region.
D. configure an amazon cloudwatch alarm for the health checks in us-east-1, and have it invoke an aws lambda function that promotes the read replica in eu-west-1.
E. configure an amazon rds event notifications to react to the failure of the database in us­ east-1. by invoking an aws lambda function that promotes the read replica in eu-west-1.

Mi9T: a this will not ensure low latency for all requestsb this will not ensure low latencyc latency routing also supports
failoverhttps://  docs.aws.amazon.com/route53/latest/ developerguide/  dns-failover­ complex-configs.html d healthy check fails doesn't mean database fails


179, a large company is migrating its entire it portfolio to aws. each business unit in the company has a standalone aws account that supports both development and test environments. new accounts to support production workloads will be needed soon. the finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs. the security team requires a centralized mechanism to control iam usage in all the company's accounts. what combination of the following options meet the company's needs with least effort? (choose two.)

A. use a collection of parameterized aws cloudformation templates defining common iam permissions that are launched into each account. require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model.
B. use aws organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. invite the existing accounts to join the organization and create new accounts using organizations.
C. require each business unit to use its own aws accounts. tag each aws account appropriately and enable cost explorer to administer chargebacks.
D. enable all features of aws organizations and establish appropriate service control policies that filter iam permissions for sub-accounts.
E. consolidate all of the company's aws accounts into a single aws account. use tags for billing purposes and iam's access advice feature to enforce the least privilege model.


 tfr: a this will work, but the process will have a bit effort. c we will like consolidate billing as well...d https:/ jaws.amazon.com/biogs/security/how-to-use-service-control­ policies-to-set-permission-guardrails- across-  accounts-in-your-aws-organization/e single account is not a good option


180, a development team has created a series of aws cloudformation templates to help deploy services. they created a template for a network/virtual private (vpc) stack, a database stack, a bastion host stack, and a web application-specific stack. each service requires the deployment of at least:*a network/vpc stack*a bastion host stack*a web application stackeach template has multiple input parameters that make it difficult to deploy the services individually from the aws cloudformation console. the input parameters from one stack are typically outputs from other stacks. for example, the vpc id, subnet ids, and security groups from the network stack may need to be used in the application stack or database stack.which actions will help reduce the operational burden and the number of parameters passed into a service deployment? (choose two.)
A. create a new aws cloudformation template for each service. after the existing templates to use cross- stack references to eliminate passing many parameters to each template. call each required stack for the application as a nested stack from the new stack. call the newly created service stack from the aws cloudformation console to deploy the specific service with a subset of the parameters previously required.
B. create a new portfolio in aws service catalog for each service. create a product for each existing aws cloudformation template required to build the service. add the products to the portfolio that represents that service in aws service catalog. to deploy the service, select the
A. 
specific service portfolio and launch the portfolio with thenecessary parameters to deploy all templates
C. set up an aws codepipeline workflow for each service. for each existing template, choose aws cloudformation as a deployment action. add the aws cloudformation template to the deployment action. ensure that the deployment actions are processed to make sure that dependences are obeyed.use configuration files and scripts to share parameters between the stacks. to launch the service, execute the specific template by choosing the name of the service and releasing a change.
D. use aws step functions to define a new service. create a new aws cloudformation template for each service. after the existing templates to use cross-stack references to eliminate passing many parameters to each template. call each required stack for the application as a nested stack from the new service template. configure aws step functions to call the service template directly. in the aws step functions console, execute the step.
E. create a new portfolio for the services in aws service catalog. create a new aws cloudformation template for each service. after the existing templates to use cross-stack references to eliminate passing many parameters to each template. call each required stack for the application as a nested stack from the new stack. create a product for each application. add the service template to the product. add each new product to the portfolio.deploy the product from the portfolio to deploy the service with the necessary parameters only to start the deployment.

Mi-fr: a cloudformation console is not very good to handle multiple services b a service should not be a portfolio, it is more like a product. also, i don't think it is possible to launch a portfolio c use codepipeline is good, but i am not comfortable to share parameter with config files and scripts. we still could use cross-stack reference for this, but this is one of the best two d you can't deploy a cloudformation template directly from step function
https:/ / docs.aws.amazon.com/step-functions/latest/dg/ concepts-service­ integrations.html e a nested stack with cross stack reference example https://cloudacademy.com/blog/understanding-nested-cloudformation-stacks/


18L	a company's solutions architect is designing a disaster recovery (dr) solution for an application that runs on aws. the application uses postgresql 11.7as its database. the company has an rpo of 30 seconds. the solutions architect must design a dr solution with the primary database in the us-east-1 region and the failover database in the us-west-2 region.what should the solutions architect do to meet these requirements with minimum application change?
A. migrate the database to amazon rds for postgresql in us-east-1 set up a read replica in us­ west-2 set the managed rpo for the rds database to 30 seconds.
A. 
B. migrate the database to amazon rds for postgresql in us-east-1 set up a standby replica in an availability zone in us-west-2. set the managed rpo for the rds database to 30 seconds.
C. migrate the database to an amazon aurora postgresql global database with the primary region as us- east-1 and the secondary region as us-west-2. set the managed rpo for the aurora database to 30 seconds.
D. migrate the database to amazon dynamodb in us-east-1 set up global tables with replica tables that are created in us-west-2.




182, a company has a policy that all amazon ec2 instances that are running a database must exist within the same subnets in a shared vpc. administrators must follow security compliance requirements and are not allowed to directly log in to the shared account. all company accounts are members of the same organization in aws organizations. the number of accounts will rapidly increase as the company grows. a solutions architect uses aws resource access manager to create a resource share in the shared account. what is the most operationally efficient configuration to meet these requirements?
A. add the vpc to the resource share. add the account ids as principals.
B. add all subnets within the vpc to the resource share add the account ids as principals.
C. add all subnets within the vpc to the resource share. add the organization as a principal.
D. add the vpc to the resource share. add the organization as a principal.

Mi-fr: adding the organization as a principal ensure that current and future accounts will have access to the share. the question mentions that there will be many new accounts, that's the clue


183, a company's processing team has an aws account with a production application. the application runs on amazon ec2 instancesbehind a network load balancer (nib). the ec2instances are hosted in private subnets in a vpc in the eu- west-1 region. the vpc was assigned the cidr block of 10.0.0.0 /16. the billing team recently created a new aws account and deployed an application on ec2 instances that are hosted in private subnets in a vpc in the eu-central-1 region. the new vpc is assigned the cidr block of 10.0.0.0/16. the processing application needs to securely communicate with the billing application over a proprietary tcp port. what should a solutions architect do to meet this requirement with the least amount of operational effort?

A. in the billing team's account, create a new vpc and subnets in eu-central-1 that use the cidr block of 192.168.0.0/16. redeploy the application to the new subnets. configure a vpc peering connection between the two vpcs.
B. in the processing team's account, add an additional cidr block of168.0.0/16 to the vpc in eu- west-1. restart each of the ec2 instances so that they obtain a new ip address. configure an inter- region vpc peering connection between the two vpcs.
C. in the billing team's account, create a new vpc and subnets in eu-west-1 that use the cidr block of 192.168.0.0/16. create a vpc endpoint service (aws privatelink) in the processing team's account and an interface vpc endpoint in the new vpc. configure an inter-region vpc peering connection in the billing team's account between the two vpcs.
D. in each account, create a new vpc with the cidr blocks of168.0.0/16and 172.16.0.0/16. create inter-region vpc peering connections between the billing team's vpcs and the processing team's vpcs. create gateway vpc endpoints to allow traffic to route between the vpcs.

Mi-fr: b,c are not relevant.aleast amount of operational effort.the optimal variant is to plan networking ahead to avoid cidr clashes d why do we need multiple cidr as there is requirement as such


184, a company hosts a web application that runs on a group of amazon ec2 instances that are behind an application load balancer (alb) in a vpc. the company wants to tftfr: the network payloads to reverse- engineer a sophisticated attack of the application.which approach should the company take to achieve this goal?
A. enable vpc flow logs. store the flow logs in an amazon s3 bucket for analysis
8.enable traffic mirroring on the network interface of the ec2 instances send the mirrored traffic to a target for storage and analysis.
C.create an aws waf web ad, and associate it with the alb configure aws waf logging.
D.enable logging for the alb. store the logs in an amazon s3 bucket for analysis.




185, a company is serving files to its customers through an sftp server that is accessible over the internet. the sftp server is running on a single amazon ec2 instance with an elastic ip address attached. customers connectto the sftp server through its elastic ip address and

use ssh for authentication. the ec2 instance also has an attached security group that allows access from all customer ip addresses.a solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. the solution must not change the way customers connect.which solution will meet these requirements?
A.disassociate the elastic ip address from the ec2 instance create an amazon s3 bucket to be used for sftp file hosting. create an aws transfer family server configure the transfer family server with a publicly accessible endpoint associate the sftp elastic ip address with the new endpoint point the transfer family server to the s3 bucket sync all files from the sftp server to the s3 bucket.
B.disassociate the elastic ip address from the ec2 instance. create an amazon s3 bucket to be used for sftp file hosting. create an aws transfer family server. configure the transfer family server with a vpc-hosted, internet- facing endpoint. associate the sftp elastic ip address with the new endpoint.attach the security group with customer ip addresses to the new endpoint. point the transfer family server to the s3 bucket. sync all files from the sftp server to the s3 bucket.
C.disassociate the elastic ip address from the ec2 instance. create a new amazon elastic file system (amazon efs) file system to be used for sftp file hosting. create an aws fargate task definition to run an sftp server. specify the efs file system as a mount in the task definition. create a fargate service by using the task definition, and place a network load balancer (nib) in front of the service.when configuring the service, attach the security group with customer ip addresses to the tasks that run the sftp server. associate the elastic ip address with the nib. sync all files from the sftp server to the s3 bucket.
D.disassociate the elastic ip address from the ec2instance. create a multi-attach amazon elastic block store (amazon ebs) volume to be used for sftp file hosting. create a network load balancer (nib) with the elastic ip address attached. create an auto scaling group with ec2instances that run an sftp server. define in the auto scaling group that instances that are launched should attach the new multi- attach ebs volume. configure the auto scaling group toautomatically add instances behind the nib. configure the auto scaling group to use the security group that allows customer ip addresses for the ec2 instances that the auto scaling group launches. sync all files from the sftp server to the new multi-attach ebs volume




186, a company is running a data-intensive application on aws. the application runs on a cluster of hundreds of amazon ec2 instances. a shared file system also runs on several ec2 instances that store 200 tb of data. the application reads and modifies the data on the shared file system and generates a report. the job runs once monthly, reads a subset of the files from the shared file system, and takes about 72 hours to complete. the compute

instances scale in an auto scaling group, but the instances that host the shared file system run continuously. the compute and storage instances are all in the same aws region. a solutions architect needs to reduce costs by replacing the shared file system instances. the file system must provide high performance access to the needed data for the duration of the 72-hour run. which solution will provide the largest overall cost reduction while meeting these requirements?
A.migrate the data from the existing shared file system to an amazon s3 bucket that uses the s3 intelligent-tiering storage class. before the job runs each month, use amazon fsx for lustre to create anew file system with the data from amazon s3 by using lazy loading. use the new file system as the shared storage for the duration of the job. delete the file system when the job is complete.
B.migrate the data from the existing shared file system to a large amazon elastic block store (amazon ebs) volume with multi-attach enabled attach the ebs volume to each of the instances by using a user data script in the auto scaling group launch template. use the ebs volume as the shared storage for the duration of the job. detach the ebs volume when the job is complete.
C.migrate the data from the existing shared file system to an amazon s3 bucket that uses the s3 standard storage class before the job runs each month, use amazon fsx for lustre to create anew file system with the data from amazon s3 by using hatch loading use the new file system as the shared storage for the duration of the job. delete the file system when the job is complete
D.migrate the data from the existing shared file system to an amazon s3 bucket before the job runs each month, use aws storage gateway to create a file gateway with the data from amazon s3 use the file gateway as the shared storage for the job. delete the file gateway when the job is complete.




187, a company is planning to migrate an application from on premises to the aws cloud. the company will begin the migration by moving the application's underlying data storage to aws. the application data is stored on a shared file system on premises, and the application servers connect to the shared file system through smb.a solutions architect must implement a solution that uses an amazon s3 bucket for shared storage. until the
application is fully migrated and code is rewritten to use native amazon s3 apls the application must continue to have access to the data through smb.the solutions architect must migrate the application data to aws to its new location while still allowing the on­ premises application to access the data.which solution will meet these requirements?
A.create a new amazon fsx for windows file server file system. configure aws datasync with one location for the on-premises file share and one location for the new amazon fsx file

system. create a new datasync task to copy the data from the on-premises file share location to the amazon fsx file system.
B.create an s3 bucket for the application. copy the data from the on-premises storage to the s3 bucket.
C.deploy an aws server migration service (aws sms) vm to the on-premises environment use aws sms to migrate the file storage server from on premises to an amazon ec2 instance.
D.create an s3 bucket for the application. deploy a new aws storage gateway file gateway on an on- premises vm. create a new file share that stores data in the s3 bucket and is associated with the file gateway copy the data from the on-premises storage to the new file gateway endpoint



188, a company has an on-premises website application that provides real estate information for potential renters and buyers. the website uses a java backend and a nosql mongodb database to store subscriber data. the company needs to migrate the entire application to aws with a similar structure. the application must be deployed for high availability, and the company cannot make changes to the application.which solution will meet these requirements?
A.use an amazon aurora db cluster as the database for the subscriber data deploy amazon ec2 instances in an auto scaling group across multiple availability zones for the java backend application.
B.use mongodb on amazon ec2 instances as the database for the subscriber data deploy ec2 instances in an auto scaling group in a single availability zone for the java backend application
C.configure amazon documentdb (with mongodb compatibility) with appropriately sized instances in multiple availability zones as the database for the subscriber data. deploy amazon ec2 instances in an auto scaling group across multiple availability zones for the java backend application.
D.configure amazon documentdb (with mongodb compatibility) in on-demand capacity mode in multiple availability zones as the database for the subscriber data deploy amazon ec2 instances in an auto scaling group across multiple availability zones for the java backend application.


189, a company wants to migrate a 30 tb oracle data warehouse from on premises to amazon redshift. the company used the aws schema conversion tool (aws set) to convert the schema of the existing datawarehouse to an amazon redshift schema. the company also used a migration assessment report to identify manual tasks to complete.the company needs to migrate the data to the new amazon redshift cluster during an upcoming data freeze period of2 weeks. the only network connection between the on-premises data warehouse and aws is a 50 mbps internet connection.which migration strategy meets these requirements?
A.create an aws database migration service (aws dms)replication instance. authorize the public ip address of the replication instance to reach the data warehouse through the corporate firewall create a migration task to run at the beginning of the data freeze period.
B.install the aws set extraction agents on the on-premises servers. define the extract, upload, and copy tasks to send the data to an amazon s3 bucket copy the data into the amazon redshift cluster. run the tasks at the beginning of the data freeze period.
C.install the aws set extraction agents on the on-premises servers. create a site-to-site vpn connection create an aws database migration service (aws dms) replication instance that is the appropriate size. authorize the ip address of the replication instance to be able to access the on- premises data warehouse through the vpn connection.
D.create a job in aws snow ball edge to import data into amazon s3 install aws set extraction agents on the on- premises servers. define the local and aws database migration service (aws dms) tasks to send the data to the snowball edge device. whig the snowball edge device is returned to aws and the data is available in amazon s3, run the aws dms subtask to copy the data to amazon redshift.

Mtfr: since there is no online data to sync, snowball is best option. also with SOmbs you can able to transfer 7tb only in 14 days.transfer 30th over SOmbps will take around 53days that ruled out abc,aws database migration service (aws dms) can use snowball edge and amazon s3 to migrate large databases more quickly than by other
methodshttps:// docs.aws.amazon.com/dms/latest/userguide/ chap_largedbs.html


190, a company is running an application in the aws cloud. the application consists of microservices that run on a fleet of amazon ec2 instances in multiple availability zones behind an application load balancer. the company recently added a new restapi that was implemented in amazon api gateway. some of the older microservices that nun on ec2 instances need to call this new api. the company does not want the api to be accessible from the public internet and does not want proprietary data to traverse the public internet.what should a solutions architect do to meet these requirements?

A.create an aws site-to-site vpn connection between the vpc and the api gateway use api gateway to generate a unique api key for each microservice. configure the api methods to require the key.
B.create an interface vpc endpoint for api gateway, and set an endpoint policy to only allow access to the specific api add a resource policy to api gateway to only allow access from the vpc endpoint change the api gateway endpoint type to private.
C.modify the api gateway to use iam authentication. update the iam policy for the iam role that is assigned to the ec2 instances to allow access to the api gateway. move the api gateway into a new vpc deploy a transit gateway and connect the vpcs
D.create an accelerator in aws global accelerator, and connect the accelerator to the api gateway.update the route table for all vpc subnets with a route to the created global accelerator endpoint ip address. add an api key for each service to use for authentication.




191, a company has a new application that needs to run on five amazon ec2 instances in a single aws region. the application requires high-throughput, low-latency network connections between all of the ec2 instances where the application will run. there is no requirement for the application to be fault tolerant.which solution will meet these requirements?
A.launch five new ec2 instances into a cluster placement group ensure that the ec2 instance type supports enhanced networking.
B.launch five new ec2 instances into an auto scaling group in the same availability zone. attach an extra elastic network interface to each ec2 instance
C.launch five new ec2 instances into a partition placement group. ensure that the ec2 instance type supports enhanced networking.
D.launch five new ec2 instances into a spread placement group attach an extra elastic network interface to each ec2instance.




192, a company is running a distributed application on a set of amazon ec2 instances in an auto scaling group. the application stores large amounts of data on an amazon elastic file system (amazon efs) file system, and new data is generated monthly the company needs to back up the data in a secondary aws region to restore from in case of a performance problem in its primary region. the company has an rto of 1 hour. a solutions architect needs

to create a backup strategy while minimizing the extra cost. which backup strategy should the solutions architect recommend to meet these requirements?
A.create a pipeline in aws data pipeline. copy the data to an efs file system in the secondary region create a lifecycle policy to move files to the efs one zone-infrequent access storage class
B.set up automatic backups by using aws backup create a copy rule to copy backups to an amazon s3 bucket in the secondary region. create a lifecycle policy to move backups to the s3 glacier storage class.
C.set up aws datasync and continuously copy the files to an amazon s3 bucket in the secondary region create a lifecycle policy to move files to the s3 glacier deep archive storage class.
D.turn on efs cross-region replication and set the secondary region as the target create a lifecycle policy to move files to the efs infrequent access storage class in the secondary region.

Mi-fr: there is nothing called cross-region replication with efs. a - correctb - cannot be restored with an rto of 1 hour c - cannot be restored with an rto of 1 hourd - cross-region replication is an s3 concept. not valid for efs.using aws data pipeline to back up efs file systems is a legacy backup solution. in this backup solution, you create a data pipeline by using the aws data pipeline service. this pipeline copies data from your amazon efs file system (called the production file system) to another amazon efs file system (called the backup file system)efso


193, a team collects and routes behavioral data for an entire company. the company runs a multi-azvpc environment with public subnets, private subnets, and in internet gateway. each public subnet also contains a nat gateway. most of the company's applications read from and write to amazon kinesis data streams.most of the workloads run in private subnets.a solutions architect must review the infrastructure. the solutions architect needs to reduce costs and maintain the function of the applications. the solutions architect uses cost explorer and notices that the cost in the ec2- other category is consistently high. a further review shows that natgateway-bytes charges are increasing the cost in the ec2-other category.what should the solutions architect do to meet these requirements?
A.enable vpc flow logs. use amazon athena to analyze the logs for traffic that can be removed ensure that security groups are blocking traffic that is responsible for high costs.
B.add an interface vpc endpoint for kinesis data streams to the vpc ensure that applications have the correct iam permissions to use the interface vpc endpoint.

C.enable vpc flow logs and amazon detective. review detective findings for traffic that is not related to kinesis data streams. configure security groups to block that traffic.
D.add an interface vpc endpoint for kinesis data streams to the vpc ensure that the vpc endpoint policy allows traffic from the applications.




194, a company is creating a sequel fora popular online game. a large number of users from all over the world will play the game within the first week after launch. currently, the game consists of the following components deployed in a single aws region:-- amazon s3 bucket that stores game assets.-- amazon dynamodb table that stores player scores.a solutions architect needs to design a multi-region solution that will reduce latency, improve reliability, and require the least effort to implement.what should the solutions architect do to meet these requirements?
A.create an amazon cloudfront distribution to serve assets from the s3 bucket configure s3 cross- region replication.create a new dynamodb table in a new region. use the new table as a replica target for dynamodb global tables.
B.create an amazon cloudfront distribution to serve assets from the s3 bucket configure s3 same- region replication.create a new dynamodb table in a new region configure asynchronous replication between the dynamodb tables by using aws database migration service (aws dms) with change data capture (cdc).
C.create another s3 bucket in a new region, and configure s3 cross-region replication between the buckets. create an amazon cloudfront distribution and configure origin failover with two origins accessing the s3 buckets in each region.configure dynamodb global tables by enabling amazon dynamodb streams, and add a replica table in a new region.
D.create another s3 bucket in the same region, and configure s3 same-region replication between the buckets. create an amazon cloudfront distribution and configure origin failover with two origins accessing the s3 buckets. create a new dynamodb table in a new region use the new table as a replica target for dynamodb global tables.




195, a company is deploying a new cluster for big data analytics on aws. the cluster will run across many linux amazon ec2 instances that are spread across multiple availability zones all of the nodes in the clustermust have read and write access to common underlying file storage. the file storage must be highly available, must be resilient, must be compatible

with the portable operating system interface (poslx), and must accommodate high levels of throughput.which storage solution will meet these requirements?
A.provision an aws storage gateway file gateway nfs file share that is attached to an amazon s3 bucket mount the nfs file share on each ec2 instance in the cluster.
B.provision a new amazon elastic file system (amazon efs) file system that uses general purpose performance mode mount the efs file system on each ec2 instance in the cluster.
C.provision a new amazon elastic block store (amazon ebs) volume that uses the io2 volume type.attach the ebs volume to all of the ec2 instances in the cluster.
D.provision a new amazon elastic file system (amazon efs) file system that uses max i/o performance mode. mount the efs file system on each ec2 instance in the cluster.



196, a company wants to manage the costs associated with a group of 20 applications that are critical, by migrating to aws. the applications are a mix of java and node.js spread across different instance clusters. the company wants to minimize costs while standardizing by using a single deployment methodology. most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. average application memory consumption is less than 1 gb, though some applications use as much asS gb of memory during peak processing. the most important application in the group is a billing report written in java that accesses multiple data sources and often for several hours.which is the most cost-effective solution?
A.deploy a separate aws lambda function for each application. use aws cloudtrail logs and amazon cloudwatch alarms to verify completion of critical jobs.
B.deploy amazon ecs containers on amazon ec2 with auto scaling configured for memory utilization of 75%. deploy an ecs task for each application being migrated with ecs task scaling. monitor services and hosts by using amazon cloudwatch.
C.deploy aws elastic beanstalk for each application with auto scaling to ensure that all requests have sufficient resources. monitor each aws elastic beanstalk deployment with using cloudwatch alarms.
D.deploy a new amazon ec2 instance cluster that co-hosts all applications by using ec2 auto scaling and application load balancers. scale cluster size based on a custom metric set on instance memory utilization. purchase 3-year reserved instance reservations equal to the groupmaxsize parameter of the auto scaling group.


 tfr: c explanation is: "a group of 20 applications that are critical, by migrating to aws". so if you choose ecs you would have to handle these applications migration on your own ... so, i guess the best choice is to use beanstalk, response cd explanation is: it looks expensive at first sight, but if you read the question it says, "..a group of 20 applications that are critical", and the 3-year reserved instances will give you a discount and the porpouse of the application is critical so maybe it means to run 7d/24h in addition to the answer that says "co-host" that will give you ha, plus is the only answer that scalate correctly the instances "...scale cluster size based on a custom metric set on instance memory utilization"


197, a company runs an ordering system on aws using amazon sqs and aws lambda, with each order received as a json message. recently the company had a marketing event that led to a tenfold increase in orders. with this increase, the following undesired behaviors started in the ordering system:-lambda failures while processing orders lead to queue backlogs.-the same orders have been processed multiple times.a solutions architect has been asked to solve the existing issues with the ordering system and add the following resiliency
features:-retain problematic orders for analysis.-send notification if errors go beyond a threshold value. how should the solutions architect meet these requirements?
A.receive multiple messages with each lambda invocation, add error handling to message processing code and delete messages after processing, increase the visibility timeout for the messages, create a dead letter queue for messages that could not be processed, create an amazon cloudwatch alarm on lambda errors for notification.
B.receive single messages with each lambda invocation, put additional lambda workers to poll the queue, delete messages after processing, increase the message timer for the messages, use amazon cloudwatch logs for messages that could not be processed, create a cloudwatch alarm on lambda errors for notification.
C.receive multiple messages with each lambda invocation, use long polling when receiving the messages, log the errors from the message processing code using amazon cloudwatch logs, create a dead letter queue with aws lambda to capture failed invocations, create cloudwatch events on lambda errors for notification.
D.receive multiple messages with each lambda invocation, add error handling to message processing code and delete messages after processing, increase the visibility timeout for the messages, create a delay queue for messages that could not be processed, create an amazon cloudwatch metric on lambda errors for notification.

Mtfr: b - single message/lambda will increase concurrency requirements and increased failure rates. there is no "lambda workers" just increased concurrency limit.c - there is no long polling in lambdad is incorrect, the delay queue is used to throttle incoming messages and not handle messages that could not be processed.b is wrong single order per message

will increase more messages in the queue c delay will increase unprocessed msgs on the queue. exactly opposite of what is needed. we rather need a dead letter queue for messages that could not be handled.d is wrong becuase of delay queue a and c are possible choices.a is better as increasing the visibility timeout decreases the possibility of duplicate message processing.


198, you work for an e-commerce retailer as an aws solutions architect. your company is looking to improve customer loyalty programs by partnering with other third-parties to offer a more comprehensive selection of customer rewards. you plan to use amazon managed blockchain to implement a blockchain network that allows your company and third-parties to share and validate rewards information quickly and transparently.how do you add members for this blockchain?
A.when amazon managed blockchain is set up, there is an initial member in the aws account. then new members can be added in this aws account without having to send an invitation, or a network invitation can be created for a member in a different aws account
B.while amazon managed blockchain is configured, there is an initial member in the aws account. then new members can be added in this aws account without having to send an invitation. you cannot add new members for other aws accounts
C.when amazon managed blockchain is created, there is no any member in the aws account. then new members can be added in this aws account or other accounts by sending out an an invitation.
D.when amazon managed blockchain is firstly created, there is no any member in the aws account.then new members can be added in this aws account. for other accounts, they can join this net blockchain network by using the network id


f@Hfr: https:/ / docs.aws.amazon.com/managed-blockchain/latest/managementguide/get­ started-create- network.html


199, a company wants to allow its marketing team to perform sq) queries on customer records to identify market segments. the data is spread across hundreds of files. the records must be encrypted in transit and at rest. the team manager must have the ability to manage users and groups, but no team members should have access to services or resources not required for the sq) queries. additionally, administrators need to audit the queries made and receive notifications when a query violates rules defined by the security team.aws organizations has been used to create a new account and an aws iam user with administrator permissions for the team manager.which design meets these requirements?

A.apply a service control policy (scp) that allows access to iam, amazon rds, and aws cloudtrail.load customer records in amazon rds mysql and train users to execute queries using the aws di.stream the query logs to amazon cloudwatch logs from the rds database instance. use a subscription filter with aws lambda functions to audit and alarm on queries against personal data.
B.apply a service control policy (scp) that denies access to all services except iam, amazon athena, amazon s3, and aws cloudtrail. store customer record files in amazon s3 and train users to execute queries using the cli via athena. analyze cloud trail events to audit and alarm on queries against personal data.
C.apply a service control policy (scp) that denies to all services except iam, amazon dynamodb, and aws cloudtrail. store customer records in dynamodb and train users to execute queries using the aws cli. enable dynamodb streams to track the queries that are issued and use an aws lambda function for real-time monitoring and alerting.
D.apply a service control policy (scp) that allows to iam, amazon athena, amazon s3, and aws cloudtrail. store customer records as files in amazon s3 and train users to leverage the amazon s3 select feature and execute queries using the aws cli. enable s3 object-level logging and analyze cloudtrail events to audit and alarm on queries against personal data.

 tfr: scps use the aws identity and access management (iam) policy language; however, they do not grant permissions. scps enable you set permission guardrails by defining the maximum available permissions for iam entities in an account. if a scp denies an action for an account, none of the entities in the account can take that action, even if their iam permissions allow them to do so. the guardrails set in scps apply to all iam entities in the account, which include all users, roles, and the account root user.there seems to be some confusion between answers band dl)scp cannot be used to provide access. it can be used to deny access. this makes b correct answer.2)also athena is not being used actively for anything in option d this makes b correct answer


200, a company runs a memory-intensive analytics application using on-demand amazon ec2 compute optimized instance. the application is used continuously and application demand doubles during working hours. the application currently scales based on cpu usage. when scaling in occurs, a lifecycle hook is used because the instance requires 4 minutes to clean the application state before terminating.because users reported poor performance during working hours, scheduled scaling actions were implemented so additional instances would be added during working hours. the solutions architect has been asked to reduce the cost of the application.which solution is most cost-effective?
A.use the existing launch configuration that uses cS instances, and update the application ami to include the amazon cloudwatch agent. change the auto scaling policies to scale based

on memory utilization.use reserved instances for the number of instances required after working hours, and use spot instances to cover the increased demand during working hours.
B.update the existing launch configuration to use rS instances, and update the application ami to include ssm agent. change the auto scaling policies to scale based on memory utilization. use reserved instances for the number of instances required after working hours, and use spot instances with on- demand instances to cover the increased demand during working hours.
C.use the existing launch configuration that uses cS instances, and update the application ami to include ssm agent. leave the auto scaling policies to scale based on cpu utilization. use scheduled reserved instances for the number of instances required after working hours, and use spot instances to cover the increased demand during work hours.
D.create a new launch configuration using rS instances, and update the application ami to include the amazon cloudwatch agent. change the auto scaling policies to scale based on memory utilization.use reserved instances for the number of instances required after working hours, and use standard reserved instances with on-demand instances to cover the increased demand during working hours.

 It-tfr: standard reserved instances provide you with a significant discount (up to 72%) compared to on-demand instance pricing, and can be purchased for a 1-year or 3-year term. customers have the flexibility to change the availability zone, the instance size, and networking type of their standard reserved instances.as you cannot modify launch configuration you have to create new one. also, memory is not a default metric you can get for ec2 on cloudwatch.you need to install cloudwatch agent on ec2 to collect memory information.


20L	a company has deployed an application to multiple environments in aws, including production and testing. the company has separate accounts for production and testing, and users are allowed to create additional application users for team members or services, as needed. the security team has asked the operations team for better isolation between production and testing with centralized controls on security credentials and improved management of permissions between environments.which of the following options would most securely accomplish this goal?
A.create a new aws account to hold user and service accounts, such as an identity account. create users and groups in the identity account. create roles with appropriate permissions in the production and testing accounts. add the identity account to the trust policies for the roles.

B.modify permissions in the production and testing accounts to limit creating new iam users to members of the operations team. set a strong iam password policy on each account. create new iam users and groups in each account to limit developer access to just the services required to complete their job function.
C.create a script that runs on each account that checks user accounts for adherence to a security policy.disable any user or service accounts that do not comply.
D.create all user accounts in the production account. create roles for access in the production account and testing accounts. grant cross-account access from the production account to the testing account.

 tfr: modify on b->aa: by centralizing users to a single account, a user can access the prod and test using assume role. this ensures that all actions are properly logged and is the most secure. adapted from this article: https:/ / aws.amazon.com/blogs/security/how-to­ centralize-and-automate-iam-policy-creation-in-sandbox- development-and- test­ environments/b: this means the test users will still need to be created. the problem with test users is always security. who is the actual person behind the scene carrying out that specific actions? this is unlikely the most secure option.c: any answers that is asking you to write a script is very unlikely to be the answer.d: this seems to be able to work too which is similar to a but the security team already asked for "better isolation with centralized controls". hence chose ahttps://docs.aws.amazon.com/iam/latest/userguide/id_roles_common-scenarios_aws­ accounts.html


202, a company is currently using aws codecommit for its source control and aws codepipeline for continuous integration. the pipeline has a build stage for building the artifacts which is then staged in an amazon s3 bucket.the company has identified various improvement opportunities in the existing process, and a solutions architect has been given the following requirement:-create a new pipeline to support feature development-support feature development without impacting production applications-incorporate continuous testing with unit tests-isolate development and production artifacts-support the capability to merge tested code into production code. how should the solutions architect achieve these requirements?
A.trigger a separate pipeline from codecommit feature branches. use aws codebuild for running unit tests. use codebuild to stage the artifacts within an s3 bucket in a separate testing account.
B.trigger a separate pipeline from codecommit feature branches. use aws lambda for running unit tests. use aws codedeploy to stage the artifacts within an s3 bucket in a separate testing account.

C.trigger a separate pipeline from codecommit tags use jenkins for running unit tests. create a stage in the pipeline with s3 as the target for staging the artifacts with an s3 bucket in a separate testing account.
D.create a separate codecommit repository for feature development and use it to trigger the pipeline.use aws lambda for running unit tests. use aws codebuild to stage the artifacts within different s3 buckets in the same production account.


 tfr: aws codebuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.code build for unit tests.codebuild to verify against a feature branch before merging. store artifacts in s3. https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html


203, an organization has recently grown through acquisitions. two of the purchased companies use the same ip cidr range. there is a new short-term requirement to allow anycompany a (vpc- a) to communicate with a server that has the ip address 10.0.0.77 in anycompany b (vpc-b). anycompany a must also communicate with all resources in anycompany c (vpc-c). the network team has created the vpc peer links, but it is having issues with communications between vpc-a and vpc-b. after an investigation, the team believes that the routing tables in the vpcs are incorrect.what configuration will allow anycompany a to communicate with anycompany c in addition to the database in anycompany b?
A.on vpc-a, create a static route for the vpc-b cidr range (10.0.0.0/24) across vpc peer pcx­ ab. create a static route ofl0.0.0.0/16 across vpc peer pcx-ac.on vpc-b, create a static route for vpc-a cidr (172.16.0.0/24) on peer pcx-ab.on vpc-c, create a static route for vpc-a cidr (172.16.0.0/24) across peer pcx-ac.
B.on vpc-a, enable dynamic route propagation on pcx-ab and pcx-ac.on vpc-b, enable dynamic route propagation and use security groups to allow only the ip address
10.0.0.77 /32 on vpc peer pcx-ab.on vpc-c, enable dynamic route propagation with vpc-a on peer pcx-ac.
C.on vpc-a, create network access control lists that block the ip address 10.0.0.77/32 on vpc peer pcx- ac.on vpc-a, create a static route for vpc-b cidr (10.0.0.0/24) on pcx-ab and a static route for vpc-c cidr (10.0.0.0/24) on pcx-ac.on vpc-b, create a static route for vpc-a cidr (172.16.0.0/24) across peer pcx-ab. on vpc-c, create a static route for vpc-a cidr (172.16.0.0/24) across peer pcx-ac.
D.on vpc-a, create a static route for the vpc-b cidr (10.0.0.77 /32) database across vpc peer pcx- ab. create a static route for the vpc-c cidr on vpc peer pcx-ac.on vpc-b, create a static

route for vpc-a cidr (172.16.0.0/24) on peer pcx-ab.on vpc-c, create a static route for vpc-a cidr (172.16.0.0/24) across peer pcx-ac.


 tfr: dis ok,will work, /32 will be prioritized (routing prioritize smaller cider).however, it will not be perfect, a wont able to communicate with 10.0.0.77 in vpc-c, because it will always be routed to b for that destination ip. but it is "short-term requirement " in this question, so the solution is acceptable.


204, an online e-commerce business is running a workload on aws. the application architecture includes a web tier, an application tier for business logic, and a database tier for user and transactional data management. the database server has a 100 gb memory requirement. the business requires cost- efficient disaster recovery for the application with an rto of 5 minutes and an rpo of 1 hour. the business also has a regulatory for out-of region disaster recovery with a minimum distance between the primary and alternate sites of 250 miles.which of the following options can the solutions architect design to create a comprehensive solution for this customer that meets the disaster recovery requirements?
A.back up the application and database data frequently and copy them to amazon s3. replicate the backups using s3 cross-region replication, and use aws cloudformation to instantiate infrastructure for disaster recovery and restore data from amazon s3.
B.employ a pilot light environment in which the primary database is configured with mirroring to build a standby database on m4.large in the alternate region. use aws cloudformation to instantiate the web servers, application servers and load balancers in case of a disaster to bring the application up in the alternate region. vertically resize the database to meet the full production demands, and use amazon route 53 to switch traffic to the alternate region.
C.use a scaled-down version of the fully functional production environment in the alternate region that includes one instance of the web server, one instance of the application server, and a replicated instance of the database server in standby mode. place the web and the application tiers in an auto scaling behind a load balancer, which can automatically scale when the load arrives to the application.use amazon route 53 to switch traffic to the alternate region.
D.employ a multi-region solution with fully functional web, application, and database tiers in both regions with equivalent capacity. activate the primary database in one region only and the standby database in the other region. use amazon route 53 to automatically switch traffic from one region to another using health check routing policies.


 tfr: a - incorrect - too long to scale to receive workloadb - incorrect - too long to scale to receive workload. m4.large has 8gb of ram c - correct - rto of 5 minutes is met - at least one instance is running in the remote site. nowhere does it state the load requirements or whether the primarysite is configured in the same manner with 1 instance that auto scales. nothing states that the instance can't be a larger instance type.ct - incorrect - definitely possible, but expensive. c provides the same functionality at lower cost.warm standby (rpo in seconds, rto in minutes): maintain a scaled-down version ofa fully functional environment always running in the dr region. business-critical systems are fully duplicated and are always on, but with a scaled down fleet. when the time comes for recovery, the system is scaled up quickly to handle the production loadmulti-region active-active (rpo is none or possibly seconds, rto in seconds): your workload is deployed to, and actively serving traffic from, multiple aws regions. this strategy requires you to synchronize users and data across the regions that you are using. when the time comes for recovery, use services like amazon route 53 or aws global accelerator to route your user traffic to where your workload is healthy d5 mins is too short for any auto scaling, not to mention spin up new stacks.


205, to abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public aws regions, including in the united states, where the company's headquarters is located. the solutions architect is required to provide access to the data stored in aws to the company's global wan network. the security team mandates that no traffic accessing this data should traverse the public internet.how should the solutions architect design a highly available solution that meets the requirements and is cost- effective?
A.establish aws direct connect connections from the company headquarters to all aws regions in use.use the company wan to send traffic over to the headquarters and then to the respective dx connection to access the data.
B.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use inter-region vpc peering to access the data in other aws regions.
C.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use an aws transit vpc solution to access data in other aws regions.
D.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use direct connect gateway to access data in other aws regions.


 tfr: this feature also allows you to connect to any of the participating vpcs from any direct connect location, further reducing your costs for making using aws services on a cross-region basis. https:/ /aws.amazon.com/blogs/aws/new- aws-direct-connect-gateway­ inter-region-vpc-access/a: there is only a single de and hence is not highly available.b: vpc peering means there are additional cost charges when data transfer between region. also there is a 125 vpc peering limit. data transferred across inter-region vpc peering connections is charged at the standard inter-region data transfer rates.
https:/ /aws.amazon.com/about-aws/whats-new/2017/11/ announcing-support-for-inter­ region-vpc-peering/c: similar to bd: remember one caveat which the question did not state is if there are multiple accounts: the vpcs that reference a particular direct connect gateway must have ip address ranges that do not overlap. today, the vpcs must all be in the same aws account; we plan to make this more flexible in the future. https:/ /
aws.amazon.com/blogs/aws/ new-aws-  direct-connect-gateway-inter-region-vpc-access/


206, a company wants to launch an online shopping website in multiple countries and must ensure that customers are protected against potential "man-in-the-middle" attacks. which architecture will provide themost secure site access?
A.use amazon route 53 for domain registration and dns services. enable dnssec for all route 53 requests. use aws certificate manager (acm) to register tls/ssl certificates for the shopping website, and use application load balancers configured with those tls/ssl certificates for the site.use the server name identification extension in all client requests to the site.
B.register 2048-bit encryption keys from a third-party certificate service. use a third-party dns provider that uses the customer managed keys for dnssec. upload the keys to acm, and use acm to automatically deploy the certificates for secure web services to an ec2 front-end web server fleet by using nginx. use the server name identification extension in all client requests to the site.
C.use route 53 for domain registration. register 2048-bit encryption keys from a third-party certificate.use a third-party dns service that supports dnssec for dns requests that use the customer managed keys. import the customer managed keys to acm to deploy the certificates to classic load balancers configured with those tls/ssl certificates for the site. use the server name identification extension in all clients requests to the site.
D.use route 53 for domain registration, and host the company dns root servers on amazon ec2 instances running bind. enable dnssec for dns requests. use acm to register tls/ssl certificates for the shopping website, and use application load balancers configured with those tis/ ssl certificates for the site. use the server name identification extension in all client requests to the site.


 tfr: ait seems like there is some missing information in the answers or was poorly written. c is not correct as classic load balancers do not support server name indication or sni (every question labels this wrong as "server name identification"). question b seems like it's making an assumption that this is a load balanced fleet of ec2 instances. if this was using a application load balancer, this question would be right. but if it was using a network load balancer, then you would need the certificates to be on the ec2 instances, which acm cannot deploy to (acm is not integrated with ec2). a and dare just wrong. so this question has issues.ano need to host company root domain on ec2.host the company dns root servers on amazon ec2 instances running bind byou can use private certificates issued with acm private ca with ec2 instances, containers, and on your own servers. at this time, public acm certificates can be used only with specific aws services.https://aws.amazon.com/certificate­ manager/faqs/ cthe best choice is c as we need to register first the domain in aws route53, need ssl certification from 3rd part ca to be imported to acm, and finally, we need third party dns service to support dnssec in aws. route 53 supports dnssec for domain registration but does not support dnssec for dns service. you must deploy an ssljtls certificate on your load balancer. the load balancer uses the certificate to terminate the connection and then decrypt requests from clients before sending them to the instances. but you need to register 2048 bit encryption keys from third party ca for your certificate. as a result, third party dns service will support dnssec in aws.damazon route 53 supports dnssec for domain registration. however, route 53 does not support dnssec for dns service, regardless of whether the domain is registered with route 53. if you want to configure dnssec for a domain that is registered with route 53, you must either use another dns service provider or set up your own dns server.


207, a company is creating an account strategy so that they can begin using aws. the security team will provide each team with the permissions they need to follow the principle or least privileged access. teams would like to keep their resources isolated from other groups, and the finance team would like each team's resource usage separated for billing purposes.which account creation process meets these requirements and allows for changes?
A.create a new aws organizations account. create groups in active directory and assign them to roles in aws to grant federated access. require each team to tag their resources, and separate bills based on tags. control access to resources through iam granting the minimally required privilege.
B.create individual accounts for each team. assign the security as the master account, and enable consolidated billing for all other accounts. create a cross-account role for security to manage accounts, and send logs to a bucket in the security account.
C.create a new aws account, and use aws service catalog to provide teams with the required resources. implement a third-party billing to provide the finance team with the resource use for each team based on tagging. isolate resources using iam to avoid account sprawl. security will control and monitor logs and permissions.

D.create a master account for billing using organizations, and create each team's account from that master account. create a security account for logs and cross-account access. apply service control policies on each account, and grant the security team cross-account access to all accounts. security will create iam policies for each account to maintain least privilege access.

Mi-fr: use scp to control organizational level policies.


208, a company wants to follow its website on aws using serverless architecture design patterns for global customers. the company has outlined its requirements as follow-the website should be responsive.-the website should offer minimal latency.-the website should be highly available.-users should be able to authenticate through social identity providers such as google, facebook, and amazon.-there should be baseline ddos protections for spikes in traffic. how can the design requirements be met?
A.use amazon cloudfront with amazon ecs for hosting the website. use aws secrets manager for provide user management and authentication functions. use ecs docker containers to build an api.
B.use amazon route 53 latency routing with an application load balancer and aws fargate in different regions for hosting the website. use amazon cognito to provide user management and authentication functions. use amazon eks containers.
C.use amazon cloudfront with amazon s3 for hosting static web resources. use amazon cognito to provide user management authentication functions. use amazon api gateway with aws lambda to build an api.
D.use aws direct connect with amazon cloudfront and amazon s3 for hosting static web resource.use amazon cognito to provide user management authentication functions. use aws lambda to build an api.

Mi-fr: these combination of services are stateless and highly scaleable. although i like optionb, but its has both fargate and eks, this option needs bit more explaination. option d is a bad design, direct connect seriously?. option a is missing congito/user authentication method. which leaves only option c


209, the ciso of a large enterprise with multiple it departments, each with its own aws account, wants one central place where aws permissions for users can be managed and

users authentication credentials can be synchronized with the company's existing on­ premises solution.which solution will meet the ciso's requirements?
A.define aws iam roles based on the functional responsibilities of the users in a central account. create a saml- based identity management provider. map users in the on-premises groups to iam roles.establish trust relationships between the other accounts and the central account.
B.deploy a common set of aws iam users, groups, roles, and policies in all of the aws accounts using aws organizations. implement federation between the on-premises identity provider and the aws accounts.
C.use aws organizations in a centralized account to define service control policies (scps). create a saml-based identity management provider in each account and map users in the on-premises groups to aws iam roles.
D.perform a thorough analysis of the user base and create aws iam users accounts that have the necessary permissions. set up a process to provision and de provision accounts based on data in the on-premises solution.

It-tfr: https://docs.aws.amazon.com/iam/latest/userguide/tutorial_cross-account-with­ roles.html and search "trust" on the page.also, question asks about "aws permissions for users can be managed", scp won't help too much about that. it's more like iam's job.


210, a retail company is running an application that stores invoice files in an amazon s3 bucket and metadata about the files in an amazon dynamodb table. the application software runs in both us-east-1 and eu-west-1.the s3 bucket and dynamodb table are in us-east-1. the company wants to protect itself from data corruption and loss of connectivity to either region.which option meets these requirements?
A. create a dynamodb global table to replicate data between us-east-1 and eu-west-1. enable continuous backup on the dynamodb table in us-east-1. enable versioning on the s3 bucket.
B. create an aws lambda function triggered by amazon cloudwatch events to make regular backups of the dynamodb table. set up s3 cross-region replication from us-east-1 to eu­ west-1. set up mfa delete on the s3 bucket in us-east-1.
C. create a dynamodb global table to replicate data between us-east-1 and eu-west-1. enable versioning on the s3 bucket. implement strict ads on the s3 bucket.
D. create a dynamodb global table to replicate data between us-east-1 and eu-west-1. enable continuous backup on the dynamodb table in us-east-1. set up s3 cross-region replication from us- east-1 to eu- west-1.
A. 

Mtfr: the question is if ever the primary region is not available for dynamodb and s3, you need to use them as well in the secondary region to avoid loss connectivity and data corruption. the data should asynchronously be updated in both regions so it means we need cross-region replication for s3 and dynamodb. enable s3 versioning is unnecessary in terms of data replication as no one will delete the file.


21L	a company has an organization in aws organizations. the company has enabled trusted access between organizations and aws resource access manager (aws ram) the organization includes three aws accounts, one each for shared services, development, and production. the shared services account has a vpc.a solutions architect needs to meet the following requirements:-configure access between the shared services vpc and the development and production accounts-ensure that workloads in each account are deployed to at least three availability zones-ensure that there is no direct communication between the development and production workloads which combination of steps will meet these requirements? (select three)
A. in the shared services vpc. create three subnets for three availability zones. create one subnet in each availability zone.
B. in the shared services vpc, create six subnets for three availability zones. create two subnets in each availability zone
C. configure network ads to prevent connectivity between the subnets in the development account and the production account
D. configure vpc default security group outbound rules to prevent connectivity between the subnets in the development account and the production account
E. use aws ram to share three subnets in different availability zones with the development account.additionally, use aws ram to share the same three subnets with the production account
F. use aws ram to share three subnets in different availability zones with the development account.additionally, use aws ram to share three other subnets in different availability zones with the production account



212, a hedge fund company is developing a new web application to handle trades. traders around the world will use the application the application will handle hundreds of thousands of transactions especially during overlapping work hours between europe and the united

states. according to the company's disaster recovery plan, the data that is generated must be replicated to a second aws region. each transaction item will be less than 100 kb in size. the company wants to simplify the cl/cd pipeline as much as possible.which combination of steps will meet these requirements most cost-effectively?(select two.) 01-09-11
A. deploy the application in multiple regions. use amazon route 53latency-based routing to route users to the nearest deployment
B. provision an amazon aurora global database to persist data use amazon elasticache to improve response time
C. provision an amazon cloudfront domain with the website as an origin restrict access to geographies where the usage is expected
D. provision an amazon dynamodb global table. use dynamodb accelerator (dax) to improve response time
E. provision an amazon aurora multi-master cluster to persist data use amazon elasticache to improve response time




213, a company is running a multi-tier web application on premises. the web application is containerized andruns on a number of linux hosts connected to a postgresql database that contains user records. the operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. a solutions architect must improve the application's infrastructure. which combination of actions should the solutions architect take to accomplish this? (select two.)
A. migrate the postgresql database to amazon aurora
B. migrate the web application to be hosted on amazon ec2 instances.
C. set up an amazon cloudfront distribution for the web application content.
D. set up amazon elasticache between the web application and the postgresql database
E. migrate the web application to be hosted on aws fargate with amazon elastic container service (amazon ecs)




214, a company wants to migrate its web application to aws. the legacy web application consists of a web tier, an application tier, and a mysql database. the re-architected

application must consist of technologies that do not require the administration team to manage instances or clusters.which combination of services should a solutions architect include in the overall architecture? (select two.)
A. amazon aurora serverless
B. amazon ec2 spot instances
C. amazon elasticsearch service (amazon es)
D. amazon rds for mysql
E. aws fargate




215, a company has developed a microservice that uses an amazon api gateway api and an aws lambda function. the api is publicly accessible and password-protected. partners use the api to upload files of various sizes into an amazon s3 bucket. an application that is hosted on an amazon ec2instance then accesses the uploaded files. the lambda role and ec2 role have read-only permissions for the s3 bucket. the company wants to enforce data protection a solutions architect must build additional controls to make sure that only the ec2 instance and the lambda function can access the content in the s3 bucket. which combination of steps should the solutions architect take to meet these requirements? (select two.)
A. enable amazon macie on amazon s3. enable amazon inspector on the ec2 instance. enable aws x- ray to monitor lambda
B. enable aws cloudtrail to log every api call to the s3 bucket. create an amazon cloudwatch alarm to provide notification if the bucket is accessed by any role other than the lambda role or the ec2 role
C. create a new lambda function that will encrypt each file by using an aws key management service (aws kms) cmk when the file is uploaded into the s3 bucket. grant permission to use the cmk to the lambda role and the ec2 role
D. create a bucket policy to limit access to the lambda role and the ec2 role only
E. create a cmk with a key policy that allows only the lambda role and the ec2 role to use the cmk configure server-side encryption with aws kms managed encryption keys (sse-kms) with this cmk for the s3 bucket


216, a digital marketing company has multiple aws accounts that belong to various teams. the creative team uses an amazon s3 bucket in its aws account to securely store images and media files that are used as content for the company's marketing campaigns. the creative team wants to share the s3 bucket with the strategy team so that the strategy team can view the objects a solutions architect has created an iam role that is namedstrategy_reviewer in the strategy account. the solutions architect also has set up a custom aws key management service (aws kms) key in the creative account and has associated the key with the s3 bucket. however, when users from the strategy account assume the iam role and try to access objects in the s3 bucket, they receive an access denied error.the solutions architect must ensure that users in the strategy account can access the s3 bucket. the solution must provide these users with only the minimum permissions that they need. which combination of steps should the solutions architect take to meet these requirements? (select three.)
A. create a bucket policy that includes read permissions for the s3 bucket. set the principal of the bucket policy to the account id of the strategy account.
B. update the strategy_reviewer iam role to grant full permissions for the s3 bucket and to grant decrypt permissions for the custom kms key
C. update the custom kms key policy in the creative account to grant decrypt permissions to the strategy_reviewer iam role
D. create a bucket policy that includes read permissions for the s3 bucket. set the principal of the bucket policy to an anonymous user.
E. update the custom kms key policy in the creative account to grant encrypt permissions to the strategy_reviewer iam role.
F. update the strategy_reviewer iam role to grant read permissions for the s3 bucket and to grant decrypt permissions for the custom kms key.




217, a company uses an amazon s3 bucket to store static images for its website. the company configured permissions to allow access to amazon s3 objects by privileged users only. what should a solutions architect do to protect against data loss? (select two.)
A. enable versioning on the 53 bucket.
B. enable access logging on the s3 bucket.
C. enable server-side encryption on the 53 bucket.
D. configure an 53 lifecycle rule to transition objects to amazon 53 glacier E.use mea delete to require multi-factor authentication to delete an object.
A. 




218, a company has migrated a javascnpt web application from on premises to aws. the company also has migrated a mysql database to amazon rds for mysql. the application stores the database credentials in an environment fie on an amazon ec2 instance. the ec2instance has an attached instance role that provides all necessary permissions to the running application. the company wants to improve the security of the database credentials now that the application and database are migrated to the aws. the company also wants to implement an automatic credential rotation policy.which combination of steps should a solutions architect take to meet these requirements? (select three.)
A. create a new secret by using aws secrets manager. set the secret type to amazon rds. enter the database credentials and database connection information
B. create a new secret by using aws secrets manager set the secret type to amazon rds. enter the database credentials select the rds for mysql database
C. select automatic rotation. select the desired rotation interval. configure the initial value for the secret
D. select automatic rotation select the desired rotation interval. update the application to use the new secret value
E. use the sample code provided by secrets manager to update the application code update the iam role that is attached to the ec2 instance to allow the following permissions:secretsmanager:listsecrets, secretsmanager:describesecret, secretsmanager:getsecretvalue, and kms:decrypt
F. use the sample code provided by secrets manager to update the application code. update the iam role that is attached to the ec2instance to allow the secretsmanager:* permission




219, a company is storing sensitive user information in an amazon s3 bucket. the company wants to provide secure access to this bucketfrom the application tier running on amazon ec2 instances inside a vpc. which combination of steps should a solutions architect take to accomplish this? (select two.)
A. configure a vpc gateway endpoint for amazon s3 within the vpc.
B. create a bucket policy to make the objects in the s3 bucket public.
C. create a bucket policy that limits access to only the application tier running in the vpc.
A. 
D. create an iam user with an s3 access policy and copy the iam credentials to the ec2 instance.
E. create a nat instance and have the ec2 instances use the nat instance to access the s3 bucket.




220, a company is planning on deploying a newly built application on aws in a default vpc.the application will consist of a web layerand database layer. the web server was created in public subnets, and the mysql database was created in private subnets. all subnets are created with the default network acl settings, and the default security group in the vpc will be replaced with newcustom security groups.the following are the key requirements:--the web servers must be accessible only to users on an ssl connection. --the database should be accessible to the web layer, which is created in a public subnet only. --all traffic to and from the ip range 182 20.00/16 subnet should be blocked. which combination of steps meets these requirements? (select two.)
A. create a database server security group with inbound and outbound rules for mysql port 3306 traffic to and from anywhere (0.0.0.0/0).
B. create a database server security group with an inbound rule for mysql port 3306 and specify the source as a web server security group.
C. create a web server security group with an inbound allow rule for https port 443 traffic from anywhere (0.0.0.0/0) and an inbound deny rule for ip range 182 .20.0.0/16.
D. create a web server security group with an inbound rule for https port 443 traffic from anywhere (0.0.0 .0/0). create network acl inbound and outbound deny rules for ip range 182 20.0.0/16
E. create a web server security group with inbound and outbound rules for https port 443 traffic to and from anywhere (0.0.0.0/0). create a network acl inbound deny rule for ip range 182 20.0.0/16.




22L a company has an ecommerce website and platform that run in the aws cloud. the majority of the company's sales are from digital channels. the company recently set a strategic goal to make future decisions that are based on data.the company created a new data team to develop capabilities to explore the data. the data team runs all the analytics and business intelligence (bl) tools against the ecommerce database. these processes are causing occasional business disruptions during working hours. the ecommerce database

runs on an amazon rds for mysql db instance and is mission-critical.a solutions architect must recommend improvements to the current setup. the recommendations must enhance data exploration without causing business disruptions. which solutions will meet these requirements? (select two.)
A. turn on the muti-az feature for the db instance. point all the analytics and bl tools to the standby endpoint address.
B. create a read replica of the db instance. point all the analytics and bl tools to the replica endpoint address.
C. use the rds export functionality to automatically dump the database content to an amazon s3 bucket each day. use amazon athena to explore the dataset. point the analytics and bl tools to athena.
D. use amazon eventbridge (amazon cloudwatch events) to schedule an aws lambda function to run the rds export functionality to dump the database content into an amazon s3 bucket each day.use aws glue and amazon athena to explore the dataset point the analytics and bl tools to athena.
E. use amazon eventbridge (amazon cloudwatch events) to schedule a mysqldump command against the database to dump the database content in to an amazon s3 bucket each day. use aws glue and amazon athena to explore the dataset. point the analytics and bl tools to athena.




222, a company is using an organization in aws organizations to manage hundreds of aws accounts.a solutions architect is working on a solution to provide baseline protection for the open web application security project (owasp) top 10 web application vulnerabilities. the solutions architect is using aws waf for all existing and new amazon cloudfront distributions that are deployed within the organization. which combination of steps should the solutions architect take to provide the baseline protection? (select three)
A. enable aws config in all accounts
B. enable amazon guardduty in all accounts
C. enable all features for the organization
D. use aws firewall manager to deploy aws waf rules in all accounts for all cloudfront distributions
E. use aws shield advanced to deploy aws waf rules in all accounts for all cloudfront distributions
A. 
F. use aws security hub to deploy aws waf rules in all accounts for all cloudfront distributions




223, a company is running an application in the aws cloud. the company has several third­ party services that integrate with the application through a restful apl.the api is a serverless implementation with an amazon api gateway regional api endpoint that integrates with several different aws lambda functions.the application's data is nonrelational and is stored in an amazon dynamodb table. the application and the api are running in the eu-west-1 region. the company needs the api to also be available in the us- east-1 region. all data must be available in both regions. a solutions architect already has deployed all the lambda functions in us-east-1,which additional steps should the solutions architect take to meet these requirements? (select two.)
A. deploy a second api gateway regional api endpoint in us-east-1. create lambda integration with the functions in us-east-1.
B. enable dynamodb streams on the table in eu-west-1. replicate all changes to a dynamodb table in us-east-1
C. modify the dynamodb table to be a global table in eu-west-1 and in us-east-1
D. change the api gateway api endpoint in eu-west-1 to an edge-optimized endpoint. create lambda integration with the functions in both regions
E. create a dynamodb read replica in us-east-1




224, a company is planning to migrate an amazon rds for oracle database to an rds for postgresql db instance in another aws account. a solutions architect needs to design a migration strategy that will require nodowntime and that will minimize the amount of time necessary to complete the migration. the migration strategy must replicate all existing data and any new data that is created during the migration. the target database must be identical to the source database at completion of the migration process. all applications currently use an amazon route 53 cname record as their endpoint for communication with the rds for oracle db instance.the rds for oracle db instance is in a private subnet. which combination of steps should the solutions architect take to meet these requirements? (select three)
A. create a new rds for postgresql db instance in the target account. use the aws schema conversion tool (aws set) to migrate the database schema from the source database to the target database
A. 
B. use the aws schema conversion tool (aws set) to create a new rds for postgresql db instance in the target account with the schema and initial data from the source database
C. configure vpc peering between the vpcs in the two aws accounts to provide connectivity to both db instances from the target account. configure the security groups that are attached to each db instance to allow traffic on the database port from the vpc in the target account
D. temporarily allow the source db instance to be publicly accessible to provide connectivity from the vpc in the target account. configure the security groups that are attached to each db instance to allow traffic on the database port from the vpc in the target account
E. use aws database migration service (aws dms) in the target account to perform a full load plus change data capture (cdc) migration from the source database to the target database. when the migration is complete, change the cname record to point to the target db instance endpoint.
F. use aws database migration service (aws dms) in the target account to perform a change data capture (cdc) migration from the source database to the target database. when the migration is complete, change the cname record to point to the target db instance endpoint




225, a company wants to improve the availability and performance of its hybrid application.the application consists of a stateful tcp-based workload hosted on amazon ec2 instances in different aws regions and a stateless udp-based workload hosted on premises.which combination of actions should a solutions architect take to improve availability and performance? (select two.)
A. create an accelerator using aws global accelerator. add the load balancers as endpoints.
B. create an amazon cloudfront distribution with an origin that uses amazon route 53 latency-based routing to route requests to the load balancers.
C. configure two application load balancers in each region. the first will route to the ec2 endpoints, and the second will route to the on-premises endpoints.
D. configure a network. load balancer in each region to address the ec2 endpoints. configure a network load balancer in each region that routes to the on-premises endpoints.
E. configure a network load balancer in each region to address the ec2 endpoints. configure an application load balancer in each region that routes to the on-premises endpoints.


226, a solutions architect has created a single vpc on aws. the vpc has one internet gateway and one nat gateway. the vpc extends across three availability zones. each availability zone includes one public subnet and one private subnet. the three private subnets contain amazon ec2 instances that must be able to connect to the internet.which solution will increase the network resiliency of this architecture?
A. add two nat gateways so that each availability zone has a nat gateway. configure a route table for each private subnet to send traffic to the nat gateway in the subnet's availability zone
B. add two nat gateways so that each availability zone has a nat gateway. configure a route table for each public subnet to send traffic to the nat gateway in the subnet's availability zone
C. add two internet gateways so that each availability zone has an internet gateway. configure a route table for each private subnet to send traffic to the internet gateway in the subnet's availability zone
D. add two internet gateways so that each availability zone has an internet gateway. configure a route table for each public subnet to send traffic to the internet gateway in the subnet's availability zone




227, a company needs to find all blocked attempts to access multiple amazon rds db instances. all the db instances are located in a single vpc.the company wants to implement search functionality to identify ip addresses of devices that repeatedly try to connect to the db instances unsuccessfully. the company needs to store the collected data in amazon s3. the company needs a solution that minimizes the operational overhead of data collection. the solution also must provide a user-friendly interface that the company can use to query the entire collected dataset.which solution will meet these requirements?
A. create a vpc flow log for each network interface that is attached to the db instances. publish the flow log records to amazon s3. use amazon athena to search the logs
B. create a vpc flow log for each network interface that is attached to the db instances. publish the flow log records to amazon s3. use s3 select to search the logs
C. create a vpc flow log for the vpc that contains the db instances. publish the flow log records to amazon s3. use s3 select to search the logs
D. create a vpc flow log for the vpc that contains the db instances. publish the flow log records to amazon s3. use amazon athena to search the logs


228, an external audit of a company's serverless application reveals iam policies that grant too many permissions. these policies are attached to the company's aws lambda execution roles hundreds of the company's lambda functions have broad access permissions, such as full access to amazon s3buckets and amazon dynamodb tables. the company wants each function to have only the minimum permissions that the function needs to complete its task.a solutions architect must determine which permissions each lambda function needs. what should the solutions architect do to meet this requirement with the least amount of effort?
A. set up amazon codeguru to profile the lambda functions and search for aws api calls create an inventory of the required api calls and resources for each lambda function create new iam access policies for each lambda function review the new policies to ensure that they meet the company's business requirements
B. turn on aws cloudtrail logging for the aws account. use aws identity and access management access analyzer togenerate iam access policies based on the activity recorded in the cloudtrail log review the generated policies to ensure that they meet the company's business requirements
C. turn on aws cloudrail logging for the aws account. create a script to parse the cloudtrail log, search for aws apicalls by lambda execution role,and create a summary report review the report create iam access polices that provide more restrictive permissions for each lambda function
D. turn on aws cloudtrail logging for the aws account export the cloudtrail logs to amazon s3 use amazon emr toprocess the cloudtrail logs in amazon s3and produce a report of api calls and resources used by each execution role create a new iam access policy for each role export the generated roles to an s3bucket review the generated policies to ensure that they meet the company's business requirements



229, a company is using aws organizations and has created a dedicated aws account for a development environment. software engineers can assume an iam role that allows them to launch amazon ec2 instances. the company needs to implement a solution only for the development account to ensure that no ec2 instances outside the t instance family can be launched.which solution will meet these requirements?
A. use the service quotas console to set the ec2 service quota to zero in all aws regions for all instance types except fort instance types
A. 
B. file an aws support request to eliminate the availability of all instance types except fort instance types in all regions
C. create an scp that denies the ec2:runlnstances action and the ec2:startlnstances action for all instance types except for tinstance types. apply the scp to the development account
D. create an scp that denies the ec2:runinstances action and the ec2:startlnstances action for all instance types except fort instance types. apply the scp to the management account



230, a company had a third-party audit of its aws environment. the auditor identified secrets in developer documentation and found secrets that were hardcoded into aws cloudformation templates throughout the environment. the auditor also identified security groups that allowed inbound traffic from the internet and outbound traffic to all destinations on the internet.a solutions architect must design a solution that will encrypt all secrets and rotate the secrets every 90 days. additionally. the solutions architect must configure the security groups to prevent resources from being accessible from the internet.which solution will meet these requirements?
A. use aws secrets manager to create, store, and access secrets. create new secrets in aws cloudformation by using the aws: secretsmanager::secret resource type. reference the secrets in other templates by using secrets manager dynamic references. configure automatic rotation in secrets manager to rotate the secrets every 90 days use awsfirewall manager to create a policy that identifies all security groups that allow inbound or outbound communications for any protocols to0.0.0/0 whenever the policy flags a security group in violation, remove the noncom pliant rule from security groups
B. use aws systems manager parameter store to create, store, and access secrets. create new parameter store items in aws cioudformation by using the aws::ssm:parameter resource type.access these items by using the aws di oraws apis configure automatic rotation in parameter store to rotate the secrets every 9o days. use aws firewall manager to create a policy that identifies all security groups that allow inbound or outbound communications for any protocols to 0.0.0.0/0. whenever the policy flags a security group in violation, remove the noncompliant rule fromsecurity groups
C. use aws secrets manager to create, store, and access secrets create new secrets in aws cloudformation by using the aws::secretsmanager::secret resource type. reference the secrets in other templates by using secrets manager dynamic references. configure automatic rotation in secrets manager to rotate the secrets every 9o days. use aws firewall manager to create a policy that enforces a requirement for all security groups to explicitly deny inbound and outbound communications for all protocols to 0.0.0.0/0
A. 
D. use aws systems manager parameter store to create, store, and access secrets. create new parameter store items in aws cloudformation by using the aws::ssm::parameter resource type.reference the items in other templates byusing systems manager dynamic references. configure automatic rotation in parameter store to rotate the secrets every lays. use aws firewall manager to create a policy that enforces a requirement for all security groups to explicitly



23L	a company is processing videos in the aws cloud by using amazon ec2 instances in an auto scaling group.it takes 30 minutes to process a video. several ec2 instances scale in and out depending on the number of videos in an amazon simple queue service (amazon sqs) queue. the company has configured the sqs queue with a redrive policy that specifies a target dead-letter queue and a maxreceivecount of 1, the company has set the visibility timeout for the sqs queue to 1 hour. the company has set up an amazon cloudwatch alarm to notify the development team when there are messages in the dead-letter queue.several times during the day, the development team receives notification that messages are in the dead- letter queue and that videos have not been processed properly. an investigation finds no errors in the application logs.how can the company solve this problem?
A. turn on termination protection for the ec2 instances.
B. update the visibility timeout for the sqs queue to 3 hours. C.configure scale-in protection for the instances during processing D.update the redrive policy and set maxreceivecount to o


232, an ecommerce company uses an amazon dynamodb table with auto scaling to store and process orders from customers. every 3 hours, the company runs an application to generate a report on an amazon ec2instance the report contains all the new orders from the past 3 hours that need to be processed in the company's logistics systems. the application uses a scan operation to retrieve the data from the table. during peak hours in the minutes after the reports are generated, users often encounter errors and timeouts when they attempt to view their orders on the website. which solution will resolve this issue with the least cost?
A. replicate all inserts for the orders table to a separate dynamodb table by using dynamodb streams and an aws lambda function that sets a ttl attribute. redirect the scans of the report-generating application to this separate dynamodb table
A. 
B. replicate all inserts for the orders table to a separate table on an amazon redshift cluster by using dynamodb streams and an aws lambda function. redirect the scans of the report­ generating application to this separate table. drop this separate table after the report­ generating application finishes running every 3 hours
C. use amazon cloudwatch metrics to determine the maximum read capacity that is necessary when reports are generated. set the number of rcus by switching to on-demand capacity for reading the orders table
D. deploy an amazon opensearch service (amazon elasticsearch service) cluster use dynamodb streams with an aws lambda function to replicate all inserts for the orders table to the cluster redirect the report-generating application to amazon opensearch service (amazon elasticsearch service) drop the amazon opensearch service (amazon elasticsearch service)index after the report-generating application finishes running every 3 hours




233, a company has set up a multi-account aws environment by using aws control tower. each aws account that aws control tower creates has its own vpc. the company is developing an application that will integrate with many microservices. the company has designated a specific account to host the application. the company will deploy the microservices on amazon ec2instances and will implement the microservices across multiple aws accounts.the microservices require a high degree of interconnectivity. the company needs a solution that will give theapplication the ability to communicate privately with the microservices. the solution also must minimize cost and operational overhead.which solution will meet these requirements?
A. use aws vpn cioudhub to connect the application vpc to all the other vpcs. use a virtual private gateway to provide traffic flow between all the vpcs
B. create vpc peering connections between the application vpc and all the other vpcs. update the security groups androute tables to allow traffic flow between all the vpcs
C. create a transit gateway in the application account attach the application vpc and all the other vpcs to the transit gateway. create a transit gateway route table to direct traffic between the vpcs
D. share the application vpc with the other aws accounts by using aws resource access manager (aws ram) deploy the microservices in the shared vpc


234, a company uses aws organizations to manage multiple aws accounts that are assigned to different departments. each department has its own ou one of the departments uses an application that is deployed on amazon ec2 instances in a private subnet to process
sensitive data. the application will store the processed data in a dedicated amazon s3 bucket. the company needs to secure the s3 bucket so that only the application's ec2 instances can access the s3 bucket. the ec2 instances do not have access to the internet. the company's development team is ready to modify the application to use a dns endpoint.which solution will meet these requirements?
A. create an scp to only allow the ec2 instances to access the s3 bucket and explicitly deny all other access. attach the scp to the department's ou. create an s3 interface endpoint
B. create an s3 gateway endpoint.attach a bucket policy that includes the aws:sourcelp condition to block access to the s3 bucket unless the request is from the specific vpc cidr range. attach the vpc endpoint to the route table of the subnet that contains the ec2 instances
C. create an s3 interface endpoint. attach a bucket policy that includes the aws:sourcevpce condition to block access to the s3 bucket unless the request is from the endpoint. attach a security group to the endpoint to only allow accessfrom a security group thatis associated with the ec2 instances
D. create an s3 gateway endpoint. attach a bucket policy that includes the aws:sourcevpc condition to block access to the s3 bucket unless the request is from a specific vpc. attach the vpc endpoint to the route table of the subnet that contains the ec2 instances




235, a company has deployed multiple copies of a reporting application in separate aws accounts to serve users from different departments. the applications run on amazon ec2instances that run in auto scaling groups. when the ec2 instances in the auto scaling groups scalein. they leave behind objects in a central amazon s3 bucket these objects must be cleaned up within a few minutes the s3objects are prefixed with the instance id the team that manages these applications wants to minimize cost and must use amazon eventbridge (amazon cloudwatch events) as part of the solution. which design will meet these requirements with the least operational overhead?
A. create eventbridge (cloudwatch events)rules in each of the aws accounts to push events to an amazon simple queue service (amazon sqs) queue in a central account. configure spot instances to process the sqs messages and clean up the s3 folder and objects
B. create an eventbridge (cloudwatch events)bus in a central account create an eventbridge (cloudwatch events) rule in each of the aws accounts. configure the rule to push the events
A. 
to the central account event bus from source "aws. autoscaling" specify an amazon elastic container service (amazon ecs) task as a target in the central account.
C. create an eventbridge (cloudwatch events) bus in a central account create an eventbridge (cloudwatch events) rule in each of the aws accounts configure the rule to push the events to the central account event bus from source "aws.autoscaling". invoke an aws lambda function in the central account to clean up the s3folder and objects
D. create an eventbridge (cloudwatch events)rule in each of the aws accounts to handle events from source "aws.autoscaling" configure aws lambda functions on each of the accounts to clean up the s3folder and objects



236, a company has implemented anew security requirement according to the new requirement, the company must scan all traffic from corporate aws instances in the company's vpc for violations of the company's security policies as a result of these scans, the company can block access to and from specific ip addresses.to meet the new requirement the company deploys a set of amazon ec2instancesinprivate subnets to serve as transparent proxies the company installs approved proxy server software on theseec2instances. the company modifies the route tables on all subnets to use the corresponding ec2instances with proxy software as the default route. the company also creates security groups that are compliant with the security policies and assigns these security groups to the ec2instances. despite these configurations the traffic of the ec2instances in their private subnets is not being properly forwarded to the internet.what should a solutions architect do to resolve this issue?
A. disable source/destination checks on the ec2 instances that run the proxy software
B. add a rule to the security group that is assigned to the proxy ec2instances to allow all traffic between instances that have this security group assign this security group to allec2instances in the vpc
C. change the vpcs dhcp options set set the dns server options to point to the addresses of the proxy ec2instancesd assign one additional elastic network interface to each proxyec2 instance. ensure that one of these network interfaces has a route to the private subnets ensure that the other network interface has a route to the internet



237, a large education company recently introduced amazon workspaces to provide access to internal applications across multiple universities. the company is storing user profiles on

an amazon fsx for windows file server file system. the file system is configured with a dns alias and is connected to a self- managed active directory. as more users begin to use the workspaces, login time increases to unacceptable levels an investigation reveals a degradation in performance of the file system. the company created the file system on hdd storage with a throughput of 16 mbps a solutions architect must improve the performance of the file system during a defined maintenance window. what should the solutions architect do to meet these requirements with the least administrative effort?
A. use aws backup to create a point-in-time backup of the file system. restore the backup to a new fsx for windows file server file system. select ssd as the storage type select 32 mbps as the throughput capacity. when the backup andrestore process is completed adjust the dns alias accordingly. delete the original file system
B. disconnect users from the file system. in the amazon fsx console, update the throughput capacity to 32 mbps. update the storage type to ssd. reconnect users to the file system
C. deploy an aws datasync agent onto a new amazon ec2instance. create a task configure the existing file system asthe source location configure a new fsx for windows file server file system with ssd storage and 32 mbps of throughput as the target location. schedule the task when the task is completed, adjust the dns alias accordingly delete the original file system
D. enable shadow copies on the existing file system by using a windows powershell command. schedule the shadow copy job to create a point-in-time backup of the file system. choose to restore previous versions. create a new fsx forwindows file server file system with ssd storage and 32 mbps of throughput. when the copy job is completed, adjust the dns alias delete the original file system




238, a company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a vpn. the company is hosting internal applications with vpcs in multiple aws accounts currently, the applications are accessible from the company's on-premises office network through an aws site- to-site vpn connection the vpc in the company's main aws account has peering connections established with vpcs in other aws accounts.a solutions architect must design a scalable aws client vpn solution for employees to use while they work from homewhat is the most cost-effective solution that meets these requirements?
A. create a client vpn endpoint in each aws account. configure required routing that allows access to internal applications
B. create a client vpn endpoint in the main aws account configure required routing that allows access to internal applications
A. 
C. create a client vpn endpoint in the main aws account provision a transit gateway that is connected to each aws account configure required routing that allows access to internal applications
D. create a client vpn endpoint in the main aws account. establish connectivity between the client vpn endpoint and the aws site-to-site vpn




239, a company is running an image-processing service in the aws cloud. users upload images to an amazon s3bucket for processing when an image is uploaded to the s3 bucket, several microservices that are based on aws lambda functions need to perform different processing tasks on the image each task's processing must start immediately after an image is uploaded.which solution will meet these requirements?
A. configure each microservice to create an s3 event notification with its lambda function as the destination
B. configure aws cloud trail event logging for amazon s3 create an amazon eventbridge (amazon cloudwatch events)rule with an event pattern that matches putobject api calls through cloudtrail register each microservice's lambda function as a target for the rule.
C. create an amazon eventbridge (amazon cloudwatch events)rule with an event pattern for putobject events in s3event notifications. configure each microservice to register an amazon simple queue service (amazon sqs) queue as a target for the rule. invoke the microservice's lambda function from the sqs queue
D. create an s3event notification with an amazon simple notification service (amazon sns) topic as the destination create an sns subscription for each microservice's lambda function




240, a company is using aws single sign-on (aws sso) to centrally manage permissions and access to multiple aws accounts in aws organizations a solutions architect needs to provide users with granular access to aws accounts based on different job functions.what should the solutions architect do to meet these requirements?
A. create an iam group for each job function in aws sso for the management account, create a permission set for each job function add users to the appropriate groups. assign roles to the corresponding groups in all aws accounts
A. 
B. create a group in aws sso for each job function in aws sso for the management account, create a permission set for each job function add users to the appropriate groups. assign groups to aws accounts with corresponding permission sets
C. create an iam role for each job function in all aws accounts create a group in the management account for each jobfunction in aws sso for the management account, create a permission set for each job function.
D. create an iam role for each job function in the management account in aws sso for the management account, create a permission set for each iam role



24L	a company migrated its stateless, compute-intensive web application to aws. the application was deployed on a single compute-optimized amazon ec2 instance. detailed monitoring in amazon cloudwatch shows an average of 70% cpu utilization during business hours unfortunately, random large increases in user traffic quickly drive the cpu to a sustained level of 100% utilization. and the application becomes unresponsive. which combination of actions is the most cost-effective solution to improve application availability and performance? (select three.)
A. purchase a reserved instance of the matching type
B. create an ami of the application instance. use the ami to launch instances that host the application
C. create and configure an application load balancer with an auto scaling group
D. reserve extra spot instances of the matching type
E. purchase bare metal instances of the matching type to host the application
F. launch instances that have the unlimited burst cpu feature turned on



242, a retail company runs a business-critical web service on an amazon elastic container service (amazon ecs) cluster that runs on amazon ec2 instances. the web service receives post requests from end users and writes data to a mysql database that runs on a separate ec2 instance. the company needs to ensure that data loss does not occur.the current code deployment process includes manual updates of the ecs service. during a recent deployment, end users encountered intermittent 502 bad gateway errors in response to valid web requests. the company wants to implement a reliable solution to prevent this issue from

recurring. the company also wants to automate code deployments. the solution must be highly available and must optimize cost- effectiveness.which combination of steps will meet these requirements? (select three)
A. run the web service on an ecs cluster that has a fargate launch type use aws codepipeline and aws codedeploy to perform a blue/green deployment with validation testing to update the ecs service
B. migrate the mysql database to run on an amazon rds for mysql multi-az db instance that uses provisioned iops ssd (io2) storage.
C. configure an amazon simple queue service (amazon sqs) queue as an event source to receive the post requests from the web service. configure an aws lambda function to poll the queue. write the data to the database.
D. run the web service on an ecs cluster that has a fargate launch type. use aws codepipeline and aws codedeploy to perform a canary deployment to update the ecs service
E. configure an amazon simple queue service (amazon sqs) queue. install the sqs agent on the containers that run in the ecs cluster to pol the queue. write the data to the database
F. migrate the mysql database to run on an amazon rds for mysql multi-az db instance that uses general purpose ssd (gp3) storage




243, a company recently started hosting new application workloads in the aws cloud. the company is using amazon ec2 instances,amazon elastic file system (amazon efs) file systems, and amazon rds db instances.to meet regulatory and business requirements, the company must make the following changes for data backups:-backups must be retained based on custom daily, weekly, and monthly requirements. -backups must bereplicated to at least one other aws region immediately after capture. -the backup solution must provide a single source of backup status across the aws environment. -the backup solution must send immediate notifications upon failure of any resource backup. which combination of steps will meet these requirements with the least amount of operational overhead? (select three)
A. create an aws backup plan with a backup rule for each of the retention requirements. B.configure an aws backup plan to copy backups to another region
C.create an aws lambda function to replicate backups to another region and send notification if a failure occurs.
D.add an amazon simple notification service (amazon sns) topic to the backup plan to send a notification for finished jobs that have any status except backup_job completed

E.create an amazon data lifecycle manager (amazon dim) snapshot lifecycle policy for each of the retention requirements.
F.set up rds snapshots on each database.




244, a solutions architect is designing a data processing system that will use amazon ec2 instances. data that needs to be processed will wait in an amazon simple queue service (amazon sqs) queue. at least two data processing instances must run at all times.which combination of actions will meet these requirements most cost-effectively? (select two.)
A. create a spot fleet with a target scaling policy that targets the acceptable backlog per instance.request two on-demand instances for minimum capacity. use spot instances for additional capacity.
B. purchase two reserved instances for the target platform and instance type in the target aws region
C. create on-demand capacity reservations for two instances for the target platform and instance type in the target aws region.
D. create an auto scaling group that uses spot instance requests. configure the scaling policy to scale with the size of the sqs queue. set the minimum value to 2.
E. provision two dedicated hosts. configure aws batch to use spot instances to supply additional capacity.




245, a software company has deployed a web application on aws in a vpc. the application uses an application load balancer and amazon ec2 instances in an auto scaling group for the application tier. the ec2 instances access an ibm db2 database that is hosted on separate ec2 instances. db2 credentials are stored in the configuration file on the application tier and are deployed with aws appconfig. the company has a new requirement to prove that the team in charge of the operations of the platform cannot access the cleartext datathat is stored in db2. a solutions architect must implement a solution to meet this requirement with the least possible redevelopment needed which combination of steps should the solutions architect take? (select two.)
A. use an aws managed cmk to ensure that ebs disks for the ec2 instances are encrypted edit the key policy to ensure that only the roles provided to the ec2 instances in the application tier are allowed to use the key.
A. 
B. use a customer managed cmk to ensure that ebs disks for the ec2instances are encrypted. edit the key policy to ensure that only the roles provided to the ec2 instances in the application tier are allowed to use the key.
C. use aws certificate manager (acm) to implement mutual authentication between the application and the database.
D. use aws secrets manager to ensure that a password is not stored in the application configuration.
E. use client-side encryption in the application


tfr: modify on be->bd



246, a company is running an application on amazon ec2 instances in three environments development, testing, and production. the company uses amls to deploy the ec2 instances. the company builds the amis by using custom deployment scripts and infrastructure orchestration tools for each release in each environment.the company is receiving errors in its deployment process. errors appear during operating system package downloads and during application code installation from a third-party git hosting service. the company needs deployments to become more reliable across all environments. which combination of steps will meet these requirements? (select three)
A. mirror the application code to an aws codecommit git repository. use the repository to build ec2 amis
B. produce multiple ec2 amls. one for each environment, for each release
C. produce one ec2 ami for each release for use across all environments
D. mirror the application code to a third-party git repository that uses amazon s3 storage. use the repository for deployment.
E. replace the custom scripts and tools with aws codebuild update the infrastructure deployment process to use ec2 image builder
F. replace the custom scripts and tools with ec2 image builder update the deployment process to use aws cloudformation.


247,  a company is using multiple aws accounts. the company has a shared services account and several other accounts for different projects.a team has a vpc in a project account the team wants to connect this vpc to a corporate network through an aws direct connect gateway that exists in the shared services account. the team wants to automatically perform a virtual private gateway association with the direct connect gateway by using an already­ tested aws lambda function while deploying its vpc networking stack. the lambda function code can assume a role by using aws security token service (aws sts) the team is using aws cloudformation to deploy its infrastructure.which combination of steps will meet these requirements? (select three)
A. deploy the lambda function to the project account. update the lambda function's iam role with the directconnect- permission.
B. create a cross-account iam role in the shared services account that grants the lambda function the directconnect:*permission. add the sts:assumerole permission to the iam role that is associated with the lambda function in the shared services account.
C. add a custom resource to the cloudformation networking stack that references the lambda function in the project account
D. deploy the lambda function that is performing the association to the shared services account update the lambda function's iam role with the directconnect:* permission
E. create a cross-account iam role in the shared services account that grants the sts:assumerole permission to the lambda function with the directconnect:* permission acting as a resource. add the stsassumerole permission with this cross-account iam role as a resource to the iam role that belongs to the lambda function in the project account




248, a company runs a serverless application in a single aws region. the application accesses external urls and extracts metadata from those sites. the company uses an amazon simple notification service (amazon sns) topic to publish urls to an amazon simple.queue service (amazon sqs) queue. an aws lambda function uses the queue as an event source and processes the urls from the queue. results are saved to an amazon s3 bucket. the company wants to process each url in other regions to compare possible differences in site localization urls must be published from the existing region. results must be written to the existing s3 bucket in the current region.which combination of changes will produce multi­ region deployment that meets these requirements? (select two)
A. deploy the sqs queue with the lambda function to other regions B.subscribe the sns topic in each region to the sqs queue C.subscribe the sqs queue in each region to the sns topic
A. 
D.configure the sqs queue to publish urls to sns topics in each region E.deploy the sns topic and the lambda function to other regions



249, a media company is hosting a high-traffic news website on aws the website's front end is based solely on html and javascript. the company loads all dynamic content by using dynamic asynchronous javascript requests to a dedicated backend infrastructure.the front end runs on four amazon ec2 instances as web servers. the dynamic backend runs in containers on an amazon elastic container service (amazon ecs) cluster that uses an auto scaling group of ec2 instances. the ecs tasks are behind an application load balancer (alb). which solutions should a solutions architect recommend to optimize costs? (select two)
A. migrate the front end of the website to an amazon s3 bucket. deploy an amazon cloudfront distribution set the s3 bucket as the distribution's origin.
B. deploy an amazon cloudfront distribution. configure the distribution to use the alb endpoint as the origin.
C. migrate the front-end services to the ecs cluster. increase the minimum number of nodes in the auto scaling group
D. turn on auto scaling for the front-end ec2 instances. configure a new listener rule on the alb to serve the front end
E. migrate the backend of the website to an amazon s3 bucket. deploy an amazon cloudfront distribution set the s3 bucket as the distribution's origin.




250, a company has a multi-tier web application deployed on aws the web tier consists of an auto scaling group of amazon ec2 spot instances and on-demand instances behind an application load balancer. the application tier connects to an amazon aurora mysql db cluster. during times of peak order volume, users report slow application performance. in addition, amazon rds performance insights indicates that database wait metrics are high.the company has limited operational resources and asks a solutions architect to resolve the current scalability issues while ensuring the least possible ongoing operational maintenance. which actions should the solutions architect recommend to meet these requirements? (select two)
A. deploy an amazon elasticache for redis cluster alongside the database, and modify the application to use the redis cluster.
A. 
B. modify the auto scaling group configuration to use on-demand instances for the web and application tiers.
C. enable aurora auto scaling, and modify the application to segregate database read traffic to the aurora reader endpoint
D. modify the auto scaling group configuration to use larger sized instances for the web and application tiers.
E. migrate the aurora cluster to the same subnet as the application tier to improve network throughput.



25L  a company's factory and automation applications are running in a single vpc. more than 20 applications run on a combination of amazon ec2, amazon elastic container service (amazon ecs), and amazon rds.the company has software engineers spread across three teams. one of the three teams owns each application, and each team is responsible for the cost and performance of all of its applications team resources have tags that represent their application and team.the teams use iam access for daily activitiesthe company needs to determine which costs on the monthly aws bill are attributable to each application or team. the company also must be able to create reports to compare costs from the last 12 months and to help forecast costs for the next 12months. a solutions architect must recommend an aws billing and cost management solution that provides these cost reports.which combination of actions will meet these requirements? (select three)
A. activate the user-defined cost allocation tags that represent the application and the team.
B. activate the aws generated cost allocation tags that represent the application and the team
C. create a cost category for each application in billing and cost management d
D. activate iam access to billing and cost management
E. create a cost budget.
F. enable cost explorer



252, a solutions architect is designing an aws account structure for a company that consists of multiple teams. all the teams will work in the same aws region. the company needs a vpc that is connected to the on-premises network. the company expects less than 50

mbps of total traffic to and from the on-premises network. which combination of steps will meet these requirements most cost-effectively? (select two.)
A. create an aws cloudformation template that provisions a vpc and the required subnets. deploy the template to each aws account.
B. create an aws cloudformation template that provisions a vpc and the required subnets. deploy the template to a shared services account. share the subnets by using aws resource access manager.
C. use aws transit gateway along with an aws site-to-site vpn for connectivity to the on­ premises network. share the transit gateway by using aws resource access manager.
D. use aws site-to-site vpn for connectivity to the on-premises network
E. use aws direct connect for connectivity to the on-premises network




253, a company is deploying a third-party firewall appliance solution from aws marketplace to monitor andprotect traffic that leaves the company's aws environments. the company wants to deploy this appliance into a shared services vpc and route all outbound internet-bound traffic through the appliances. a solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single aws region. the company has set up routing from the shared services vpc to other vpcs.which steps should the solutions architect recommend to meet these requirements? (select three)
A. deploy two firewall appliances into the shared services vpc. each in a separate availability zone.
B. create a new network load balancer in the shared services vpc. create a new target group. and attach it to the new network load balancer. add each of the firewall appliance instances to the target group.
C. create a new gateway load balancer in the shared services vpc create a new target group. and attach it to the new gateway load balancer. add each of the firewall appliance instances to the target group.
D. create a vpc interface endpoint. add a route to the route table in the shared services vpc. designate the new endpoint as the next hop for traffic that enters the shared services vpc from other vpcs.
E. deploy two firewall appliances into the shared services vpc, each in the same availability zone.
A. 
F. create a vpc gateway load balancer endpoint add a route to the route table in the shared services vpc. designate the new endpoint as the next hop for traffic that enters the shared services vpc from other vpcs


It-tfr: adf>acf


254, a company is offering one of its applications as a multi-tenant software-as-a-service (saas) solution. the application has a rest api that runs on a set of amazon ec2 instances behind an application load balancer (alb). the instances run in an auto scaling group last week, one of the tenants ran a campaign that significantly increased traffic to the rest api. the resource constraints affected the performance of other tenants that were running on the same set ofec2 instances. the company wants the ability to throttle api calls for each tenant.which combination of steps should a solutions architect take to meet these requirements? (select three)
A. create an aws waf web ad. add a rate-based rule statement to the web ad set the action to block
B. create an amazon api gateway api. assign an api key usage plan for each tenant
C. create an amazon api gateway api. assign the aws wafweb ad to the api gateway api
D. create an amazon doudfront distribution assign the aws waf web ad to the doudfront distribution.
E. create a vpc link for http apis. set up the alb as the target. configure an http proxy private integration that uses the vpc link
F. modify the application's api requests to target the newly created endpoint.




255, a solutions architect is importing a vm from an on-premises environment by using the amazon ec2 vm import feature of aws import/ export. the solutions architect has created an ami and has provisioned an amazon ec2 instance that is based on that ami. the ec2instance runs inside a public subnet in a vpc and has a public ip address assigned.the ec2instance does not appear as a managed instance in the aws systems manager console which combination of steps should the solutions architect take to troubleshoot this issue? (select two)
A. verify that systems manager agent is installed on the instance and is running
A. 
B. verify that the instance is assigned an appropriate iam role for systems manager.
C. verify the existence of a vpc endpoint on the vpc
D. verify that the aws application discovery agent is configured
E. verify the correct configuration of service-linked roles for systems manager.




256, a company wants to track ts daily aws resource usage to avoid reaching service quotas unexpectedly. the company needs to receive notifications when any service quota is exceeded. which combination of actions should a solutions architect take to meet this requirement? (select two.)
A. configure amazon simple notification service (amazon sns) as the target to send notifications
B. use the describe tnustedadvisorchecks api operation to get aws trusted advisor service limits checks every 24 hours
C. create an aws lambda function that runs every 24 hours and refreshes the aws trusted advisor service limits checks.
D. use aws config to monitor the aws resources service quotas and create a periodic invocation for an aws lambda function
E. use amazon eventbridge (amazon cloudwatch events) to capture the events. configure amazon simple notification service (amazon sns) as the target




257, a company has a microsoft net application that runs on an on-premises windows server. the application stores data by using an oracle database standard edition server. the company is planning a migration to aws and wants to minimize development changes while moving the application. the aws application environment should be highly available. which combination of actions should the company take to meet these requirements? (select two )
A. refactor the application as serverless with aws lambda functions running net core
B. rehost the application in aws elastic beanstalk with the net platform in a multi-az deployment
A. 
C. replatform the application to run on amazon ec2 with the amazon linux amazon machine image (ami).
D. use aws database migration service (aws dms) to migrate from the oracle database to amazon dynamodb in a multi-az deployment
E. use aws database migration service (aws dms) to migrate from the oracle database to oracle on amazon rds in a multi-az deployment



258, a company finds that, as its use of amazon ec2 instances grows us amazon elasti block store (amazon eds) storage costs are increasing faster man expected. which ebs management practices would help reduce costs? (select two.)
A. convert the ebs volumes to an ec2 instance store.
B. monitor and enforce that the detetionon termination attribute is set to true for all ebs volumes, unless persistence requirements dictate otherwise.
C. purchase an ec2 instance savings plan for an ebs volumes that are serving persistent business requirements.
D. for ebs volumes needed for retention purposes that are not being actively used, take a snapshot and terminate the instance and volume.
E. convert the existing ebs volumes to ebs provisio ed iops ssd (iol).



259, a company fails an aws security reviews conducted by the third party. the review finds out that some of the company method to access the amazon emr through the public internet. which combination of steps should the company take to most improve its security? (select two.)
A. set up a vpc peering connect to the amazon emr api.
B. set up vpc endpoints to connect to the amazon emr api.
C. set up a nat gateway to connect to the amazon emr api.
D. set up iam roles to be used to connect to the amazon fmr api.
E. set up each developer with aws secrets manager to store access keys.
A. 




260, a company is running a database on amazon aurora.the database is idle every evening. an application that performs extensive reads on the database experiences performance issues during morning thus when user traffic spikes. during these peak periods, the application receives timeout errors when reading from the database. the company does not have a dedicated operations team and needs an automated solution to address the performance issues. which actions should a solutions architect take to automatically adjust to the increased read load on the database? (select two)
A. migrate the database to aurora serverless.
B. increase the instance size of the aurora database C.configure aurora auto scaling with aurora replicas
D. migrate the data ase to an aurora multi-master cluster
E. migrate the database to an amazon rds for mysql multi-az deployment




26L a company wants to identify underutilized instances for amazon ex2 and amazon rds. the company needs to report on the cost of all underutilized instances and the utilization metrics for each resource. which combination of tools and services will provide this data? (select two.)
A. cost explorer
B. aws cost and usage report
C. aws budgets
D. amazon cloudwarch
E. aws cloudtrail




262, after reviewing the cost optimization checks in aws trusted advisor, a team finds that it has 10,000 amazon elastic block store (amazon ebs) snapshots in its account that are more than 30 days old. when the team determines that it needs to implement better

governance for the lifecycle of its resources. which actions should the team take to automate the lifecycle management of the ebs snapshots with the least effort? (select two)
A. create and schedule a backup plan with aws backup
B. copy the ebs snapshots to amazon s3 and then create lifecycle configurations in the s3 bucket
C. use amazon data lifecycle manager (amazon dim)
D. use a scheduled event in amazon eventbridge (amazon cloudwatch events) and invoke aws step functions to manage the snapshots
E. schedule and run backups in aws systems manager.




263, a company is running an application on amazon ec2 instances hosted in a private subnet of a vpc. the ec2 instances are configured in an auto scaling group behind an elastic load balancer (elb). the ec2 instances use a nat gateway for outbound internet access. however the ec2 instances are not able to connect to the public internet to download software updates. what are the possible root causes of this issue? (select two)
A. the elb is not configured with a proper health check
B. the route tables in the vpc are configured incorrectly
C. the ec2 instances are not associated with an elastic ip address
D. the security group attached to the nat gateway is configured incorrectly
E. the outbound rules on the security group attached to the ec2 instances are configured incorrectly.




264, a solutions architect needs to allow developers to have ssh connectivity to web servers the requirements are as follows:-limit access to users originating from the corporate network-web servers cannot have ssh access directly from the internet-web servers reside in a private subnet.which combination of steps must the architect complete to meet these requirements? (select two.)
A. create a bastion host that authenticates users against the corporate directory
A. 
B. create a bastion host with security group rules that only allow traffic from the corporate network.
C. attach an iam role to the bastion host with relevant permissions
D. configure the web servers' security group to allow ssh traffic from a bastion host. E.deny all ssh traffic from the corporate network in the inbound network ad.



265, a company is using an amazon s3 bucket to store data uploaded by different departments from multiple locations.during an aws well-architected review the financial manager notices that 10 tb of s3 standard sto age data has been charged each month.however, in the aws management console for amazon s3, using the command to select all files and folders shows a total size of 5 th.what are the possible causes for this difference? (select two)
A. some files are stored with deduplication
B. the s3 bucket has versioning enabled
C. there are incomplete s3 multi part uploads
D. the s3 bucket has aws key management service (aws kms) enabled
E. the s3 bucket has intelligent-tiering enabled




266, a company wants to migrate la accounting system from an on-premises data center to the aws cloud in a single aws region data security and an immutable audit log are the top priorities. the company must monitor all aws activities for compliance auditing. the company has enabled aws cloudtrail but wants to make sure it meets these requirements. which actions should a solutions architect take to protect and secure cloudtrail? (select two.)
A. enable cloud trail log tile validation
B. install the cloudtrail processing library
C. enable logging of insights events in cloudtrail
D. enable custom logging from the on-premises resources
A. 
E. create an aws config rule to monitor whether cloud trail is configured to use server-side encryption with aws kms managed encryption keys (sse-kms)




267, a company is deploying a public-facing global application on aws using amazon cloudfront. the application communicates with an external system. a solutions architect needs to ensure the data is secured during end-to-end transit and at rest.which combination of steps will satisfy these requirements? (select two)
A. create a public certificate for the required domain in aws certificate manager and deploy it to cloudfront, an application load balancer, and amazon ec2 instances.
B. acquire a public certificate from a third-party vendor and deploy it to cloudfront, an application load balancer, and amazon ec2 instances.
C. provision amazon ebs encrypted volumes using aws kms and ensure explicit encryption of data when writing to amazon ebs.
D. use ssl or encrypt data while communicating with the external system using a vpn. E.communicate with the external system using plaintext and use the vpn to encrypt the data
in transit.




268, a company is deploying an application that processes large quantities of data in parallel. the company plans to use amazon ec2 instances for the workload. the network architecture must be configurable to provide the lowest possible latency between nodes.which combination of network solutions will meet these requirements? (select two)
A. distribute the ec2 instances across multiple availability zones
B. attach an elastic fabric adapter (efa) to each ec2 instance
C. place the ec2 instances in a single availability zone
D. use amazon elastic block store (amazon ebs) optimized instance types
E. run the ec2 instances in a cluster placement group


269, a solutions architect is investigating aws file storage solutions that can be used with a company's on- premises linux servers and applications. the company has an existing vpn connection set up between the company's vpc and its on-premises network. which aws services should the solutions architect use? (select two)
A. aws backup
B. aws datasync
C. aws snowball edge
D. aws storage gateway
E. amazon elastic file system (amazon efs)




270, an image hosting company uploads its large assets to amazon s3 standard buckets. the company uses multi part upload in parallel by using s3 apis and overwrites if the same object is uploaded again. for the first 30 days after upload the objects will be accessed frequently. the objects will be used less frequently after 30 days but the access patterns for each object will be inconsistent. the company must optimize its s3 storage costs while maintaining high availability and resiliency of stored assets. which combination of actions should a solutions architect recommend lo meet these requirements? (select two.)
A. move assets to s3 intelligent-tiering after 30 days
B. configure an s3 lifecycle policy to clean up incomplete multi part uploads C.configure an s3 1 fecycle policy to clean up expired object delete markers
D. move ass ts to s3 standard-infrequent access (s3 standard-ia) after 30 days
E. move ass ts to s3 one zone infrequent access (s3 one zone-ia) after 30 days




271, a solutions architect must design a web application that will be hosted on aws, allowing users to purchase access to premium, shared content that is stored in an s3 bucket. upon payment, content will be available for download for 14 days before the user is denied access. which of the following would be the least complicated implementation?

A. use an amazon cloudfront distribution with an origin access identity (oai) configure the distribution with an amazon s3 origin to provide access to the file through signed urls design a lambda function to remove data that is older than 14 days
B. use an s3 bucket and provide direct access to the tile design the application to track purchases in a dynamodh table configure a lambda function to remove data that is older than 14 days based on a query to amazon dynamodb
C. use an amazon cloudfront distribution with an oaiconfigure the distribution with an amazon s3 origin to provide access to the file through signed urls design the application to sot an expiration of 14 days for the url
D. use an amazon cloudfront distribution with an oaiconfigure the distribution with an amazon s3 origin to provide access to the file through signed urls design the application to set an expiration of 60 minutes for the url and recreate the url as necessary




272, a company has a three-tier image-sharing application it uses an amazon ec2 instance for the front-end layer, another for the backend tier, and a third for the mysql database. a solutions architect has been tasked with designing a solution that is highly available, and requires the least amount of changes to the application.which solution meets these requirements'?
A. use amazon s3 to host the front-end layer and aws lambda functions for the backend layer.move the database to an amazon dynamodb table and use amazon s3 to store and serve users' images.
B. use load-balanced multi-az aws elastic beanstalk environments for the front-end and backend layers.move the database to an amazon rds instance with multiple read replicas to store and serve users' images.
C. use amazon s3 to host the front-end layer and a fleet of amazon ec2 instances in an auto scaling group for the backend layer.move the database to a memory optimized instance type to store and serve users' images.
D. use load-balanced multi-az aws elastic beanstalk environments for the front-end and backend layers.move the database to an amazon rds instance with a multi-az deployment use amazon s3 to store and serve users' images.


273, a company has an application that is deployed on amazon ec2 instances behind an application load balancer (alb). the instances are part of an auto scaling group. the application has unpredictable workloads and frequently scales out and in. the company's development team wants to It-tfr: application logs to find ways to improve the application's performance. however, the logs are no longer available after instances scale inwhich solution will give the development team the ability to view the application logs after a scale­ in event?
A. enable access logs for the alb. store the logs in an amazon s3 bucket
B. configure the ec2 instances to publish logs to amazon cloudwatch logs by using the unified cloudwatch agent
C. modify the auto scaling group to use a step scaling policy D.instrument the application with aws x-ray tracing



274, a company is planning a migration from an on-premises data center to the aws cloud. the company plans to use multiple aws accounts that are managed in an organization in aws organizations. the company will create a small number of accounts initially and will add accounts as needed.a solutions architect must design a solution that turns on aws cloud trail in all aws accounts. what is the most operationally efficient solution that meets these requirements?
A. create an aws lambda function that creates a new cloud trail trail in all aws accounts in the organization. invoke the lambda function daily by using a scheduled action in amazon eventbridge (amazon cloudwatch events)
B. create a new cloudtrail trail in the organization's management account. configure the trail to log all events for all aws accounts in the organization
C. create a new cloudtrail trail in all aws accounts in the organization. create new trails whenever a new account is created. define an scp that prevents deletion or modification of trails. apply the scp to the root ou
D. create an aws systems manager automation runbook that creates a cloudtrail trail in all aws accounts in the organization. invoke the automation by using systems manager state manager


275, a company is designing an application to run in a vpc on aws. the application consists of amazon ec2 instances that run in private subnets as part of an auto scaling group the application also includes a network load balancer that extends across public subnets.the application stores data in an amazon rds db instance.the company has attached a security group that is named "web-servers" to the ec2 instances. the company has attached a security group that is named "database" to the db instance. how should a solutions architect configure the communication between the ec2instances and the db instance?
A. configure the "web-servers" security group to allow access to the db instance's current ip addresses.configure the "database" security group to allow access from the current set of ip addresses in use by the ec2instances.
B. configure the "web-servers" security group to allow access to the"database" security group. configure the "database" security group to allow access from the "web-servers security group.
C. configure the "web-servers" security group to allow access to the db instance's current ip addresses.configure the "database" security group to allow access from the auto scaling group
D. configure the "web-servers" security group to allow access to the"database"security group configure the "database" security group to allow access from the auto scaling group.



276, a company is deploying a new application to amazon elastic kubermetes service (amazon eks) with an aws fargate cluster. the application needs a storage solution for data persistence. the solution must be highly available and fault tolerant the solution also must be shared between multiple application containers. which solution will meet these requirements with the least operational overhead?
A. create amazon elastic block store (amazon ebs) volumes in the same availability zones where eks worker nodes are placed. register the volumes in a storageclass object on an eks cluster use ebs multi-attach to share the data between containers.
B. create an amazon elastic file system (amazon efs) file system. register the file system in a storageclass object on an eks cluster use the same file system for all containers.
C. create an amazon elastic block store (amazon ebs) volume. register the volume in a storageclass object on an eks cluster. use the same volume for all containers.
D. create amazon elastic file system (amazon efs) file systems in the same availability zones where eks worker nodes are placed. register the file systems in a storageclass object on an eks cluster create an aws lambda function to synchronize the data between file systems.
A. 



277, a company built a food ordering application that captures user data and stores it for future analysis. the application's static front end is deployed on an amazon ec2 instance. the front-end application sends the requests to the backend application running on separate ec2 instance. the backend application then stores the data in amazon rds what should a solutions architect do to decouple the architecture and make it scalable"
A. use amazon s3 to serve the front-end application which sends requests to amazon ec2 to execute the backend application.the backend application will process and store the data in amazon rds
B. use amazon s3 to serve the front-end application and write requests to an amazon simple notification service (amazon sns) topic.subscribe amazon ec2 instances to the http/https endpoint of the topic and process and store the data in amazon rds
C. use an ec2 instance to serve the front end and write requests to an amazon sqs queue.place the backend instance in an auto scaling group and scale based on the queue depth to process and store the data in amazon rds
D. use amazon s3 to serve the static front-end application and send requests to amazon api gateway which writes the requests to an amazon sqs queue.place the backend instances in an auto scaling group and scale based on the queue depth to process and store the data in amazon rds



278, a company wants to implement a disaster recovery plan for its primary on-premises file storage volume. the file storage volume is mounted from an internet small computer systems interface (iscsi) device on a local storage server. the file storage volume holds hundreds of terabytes (tb) of data. the company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency.which solution will meet these requirements with the least amount of change to the company's existing infrastructure?
A. provision an amazon s3 file gateway as a virtual machine (vm) that is hosted on premises set the local cache to 10 tb. modify existing applications to access the files through the nfs protocol. to recover from a disaster. provision an amazon ec2 instance and mount the s3 bucket that contains the files
B. provision an aws storage gateway tape gateway use a data backup solution to back up all existing data to a virtual tape library configure the data backup solution to run nightly after
A. 
the initial backup is complete. to recover from a disaster. provision an amazon ec2 instance and restore the data to an amazon elastic block store (amazon ebs) volume from the volumes in the virtual tape library
C. provision an aws storage gateway volume gateway cached volume. set the local cache to 10 tb mount the volume gateway cached volume to the existing file server by using iscsi, and copy all files to the storage volume. configure scheduled snapshots of the storage volume to recover from a disaster, restore a snapshot to an amazon elastic block store (amazon ebs) volume and attach the ebs volume to an amazon ec2 instance
D. provision an aws storage gateway volume gateway stored volume with the same amount of disk space as the existing file storage volume. mount the volume gateway stored volume to the existing file server by using iscsi, and copy all files to the storage volume. configure scheduled snapshots of the storage volume to recover from a disaster,restore a snapshot to an amazon elastic block store (amazon ebs) volume and attach the ebs volume to an amazon ec2 instance



279, a company is migrating a legacy application from an on-premises data center to aws. the application uses mongodb as a key-value database.according to the company's technical guidelines, all amazon ec2instances must be hosted in a private subnet without an internet connection. in addition, all connectivity between applications and databases must be encrypted. the database must be able to scale based on demand.which solution will meet these requirements?
A. create new amazon documentdb (with mongodb compatibility) tables for the application with provisioned lops volumes use the instance endpoint to connect to amazon documentdb
B. create new amazon dynamodb tables for the application with on-demand capacity. use a gateway vpc endpoint for dynamodb to connect to the dynamodb tables
C. create new amazon dynamodb tables for the application with on-demand capacity. use an interface vpc endpoint for dynamodb to connect to the dynamodb tables
D. create new amazon documentdb (with mongodb compatibility) tables for the application with provisioned iops volumes. use the cluster endpoint to connect to amazon documentdb



280, a company runs an ecommerce website that is using amazon ec2 instances for the web application and amazon s3 buckets for static content. in a recent incident, a malicious user attacked the web application through a known security vulnerability. the company

wants to be prepared fora potential security threat in the future. a solutions architect must improve the architecture so that the company can perform threat detection, root cause analysis, and centralized reporting if another incident occurs.which solution will meet these requirements?
A. enable amazon detective from the aws management console. export log data to amazon cloudwatch. set up amazon inspector for the ec2 instances send all cloudwatch logs and findings from amazon inspector to aws security hub
B. enable amazon detective from the aws management console. export log data to aws cloudtrail.turn on cloudtrail for the aws account. set up amazon inspector for the ec2 instances. enable amazon macie on the s3 bucket. send findings from amazon inspector and macie to cloudtrail
C. enable amazon guardduty from the aws management console. turn on aws cloudtrail for the aws account. set up amazon inspector for the ec2 instances. send findings from guardduty and amazon inspector to amazon cloudwatch
D. enable amazon guardduty and amazon detective from the aws management console. enable amazon macie on the s3 bucket. send findings from guardduty. detective, and macie to aws security hub



28L	a company has a web application that runs on a single on-premises apache server. the apache server maintains browsing sessions for users. the web application is used only at the start of the workday and after lunchtime. the company is migrating the web application to the aws cloud and can implement changes to the application code. the company wants to minimize maintenance efforts and maximize resiliency.what should a solutions architect recommend to meet these requirements?
A. create an aws elastic beanstalk application. migrate the on-premises web application to the elastic beanstalk application adapt the web application to store user session data in amazon dynamodb
B. create an amazon s3 bucket to store static html content. use amazon api gateway with an aws lambda function to process computational requests use amazon dynamodb to store the user session data
C. launch amazon ec2instances with instance store behind a network load balancer (nib). migrate the web application to the ec2 instances. handle the session data through nib session affinity (sticky sessions).
A. 
D. launch an amazon ec2instance. migrate the on-premises apache server to the ec2instance.configure the web application to use an amazon elastic block store (amazon ebs) volume to maintain session data for the users.




282, a solutions architect needs to design a solution that retrieves data every 2 minutes from a third-party web service that is accessible through the internet. a python script runs the data retrieval in less than 100 milliseconds for each retrieval. the response is a json object that contains sensor data that is less than 1 kb in size. the solutions architect needs to store the json object along with the timestamp. which solution meets these requirements most cost-effectively?
A. deploy an amazon ec2instance with a linux operating system. configure a cron job to run the script every 2 minutes. extend the script to store the json object along with the timestamp in a mysql database that is hosted on an amazon rds db instance.
B. deploy an amazon ec2 instance with a linux operating system to extend the script to run in an infinite loop every 2 minutes. store the json object along with the timestamp in an amazon dynamodb table that uses the timestamp as the primary key. run the script on the ec2 instance.
C. deploy an aws lambda function to extend the script to store the json object along with the timestamp in an amazon dynamodb table that uses the timestamp as the primary key. use an amazon eventbridge (amazon cloudwatch events) scheduled event that is initiated every 2 minutes to invoke the lambda function.
D. deploy an aws lambda function to extend the script to run in an infinite loop every 2 minutes. store the json object along with the timestamp in an amazon dynamodb table that uses the timestamp as the primary key. ensure that the script is called by the handler function that is configured for the lambda function.




283, a company has deployed a business-critical application in the aws cloud. the application uses amazon ec2 instances that run in the us-east-1 region. the application uses amazon s3 for storage of all critical data.to meet compliance requirements, the company must create a disaster recovery (dr) plan that provides the capability of a full failover to another aws region.what should a solutions architect recommend for this dr plan?

A. deploy the application to multiple availability zones in us-east-1. create a resource group in aws resource groups. turn on automatic failover for the application to use a predefined recovery region.
B. perform a virtual machine (vm) export by using aws import/export on the existing ec2 instances.copy the exported instances to the destination region. in the event of a disaster, provision new ec2 instances from the exported ec2 instances
C. create snapshots of all amazon elastic block store (amazon ebs) volumes that are attached to the ec2 instances in us-east-1. copy the snapshots to the destination region. in the event of a disaster, provision new ec2 instances from the ebs snapshots
D. use s3 cross-region replication for the data that is stored in amazon s3. create an aws cloudformation template for the application with an s3 bucket parameter. in the event of a disaster, deploy the template to the destination region and specify the local s3 bucket as the parameter



284, a company is refactoring its on-premises order-processing platform in the aws cloud. the platform includes a web front end that is hosted on a fleet of vms, rabbitmq to connect the front end to the backend, and a kubernetes cluster to run a containerized backend system to process the orders. the company does not want to make any major changes to the application. which solution will meet these requirements with the least operational overhead?
A. create an ami of the web server vm. create an amazon ec2auto scaling group that uses the ami and an application load balancer. set up amazon mq to replace the on-premises messaging queue.configure amazon elastic kubernetes service (amazon eks) to host the order-processing backend.
B. create a custom aws lambda runtime to mimic the web server environment. create an amazon api gateway api to replace the front-end web servers, set up amazon mq to replace the on-premises messaging queue. configure amazon elastic kubernetes service (amazon eks) to host the order- processing backend
C. create an aml of the web server vm. create an amazon ec2 auto scaling group that uses the ami and an application load balancer. set up amazon mq to replace the on-premises messaging queue.install kubernetes on a fleet of different ec2 instances to host the order­ processing backend
D. create an ami of the web server vm. create an amazon ec2 auto scaling group that uses the ami and an application load balancer. set up an amazon simple queue service (amazon sqs)
A. 
queue to replace the on-premises messaging queue. configure amazon elastic kubernetes service (amazon eks) to host the order-processing backend



285, a company provides specialized analytics services to customers. the analytics run on amazon ec2instances that need to be launched and terminated in response to requests from customers. a solutions architect is creating automation to manage the ec2 instances that handle customer requests. however, when the automation scripts attempt to launch many ec2 instances at the same time, a requestlimitexceeded error frequently occurs.what should the solutions architect do to handle this error?
A. implement an exponential backoff strategy so that the api token bucket can refill
B. modify the ec2 instance launch configuration to install diagnostic tools on each instance to troubleshoot the issue
C. request an increase for api throttling quotas from the aws support center
D. request an ec2 api quota increase through the service quotas console



286, a company is building its web application by using containers on aws. the company requires three instances of the web application run at all times. the application must be highly available and must be able to scale to meet increases in demand.which solution meets these requirements?
A. use the aws fargate launch type to create an amazon elastic container service (amazon ecs) cluster. create a task definition for the web application. create an ecs service that has a desired count of three tasks
B. use the amazon ec2launch type to create an amazon elastic container service (amazon ecs) cluster that has three container instances in one availability zone create a task definition for the web application. place one task for each container instance.
C. use the aws fargate launch type to create an amazon elastic container service (amazon ecs) cluster that has three container instances in three different availability zones create a task definition for the web application.create an ecs service that has a desired count of three tasks
D. use the amazon ec2launch type to create an amazon elastic container service (amazon ecs) cluster that has one container instance in two different availability zones. create a task
A. 
definition for the web application.place two tasks on on container instance. place one task on the remaining container instance



287, a company is automating an order management application. the company's development team has decided to use sftp to transfer and store the business-critical information files. the files must be encrypted and must be highly available. the files also must be automatically deleted a month after they are created.which solution meets these requirements with the least operational overhead?
A. configure an amazon s3 bucket with encryption enabled. use aws transfer for sftp to securely transfer the files to the s3 bucket. apply an aws transfer for sftp file retention policy to delete the files after a month.
B. install an sftp service on an amazon ec2 instance mount an amazon elastic file system amazon efs) file share on the ec2instance. enable cron to delete the files after a month.
C. configure an amazon elastic file system (amazon efs) file system with encryption enabled use aws transfer for sftp to securely transfer the files to the efs file system. apply an efs lifecycle policy to automatically delete the files after a month.
D. configure an amazon s3 bucket with encryption enabled use aws transfer for sftp to securely transfer the files to the s3 bucket. apply s3 lifecycle rules to automatically delete the files after a month.



288, a company is building a web application that servers a content management system. the content management system runs on amazon ec2 instances behind an application load balancer (alb). the ec2 instances run in an auto scaling group across availability zones. users are constantly adding and updating files, biogs, and other website assets in the content management system.which solution meets these requirements?
A. update the ec2 user data in the auto scaling group lifecycle policy to copy the website assets from the ec2 instance that was launched most recently.configure the alb to make changes to the websites assets only in the newest ec2 instance.
8.copy the website assets to an amazon elastic file system (amazon efs) me system.configure each ec2 instance to mount the efs m system locally.configure the website hosting application to reference the website assets that are stored in the efs file system.

C.copy the website assets to an amazon s3 bucket.ensure that each ec2 instance downloads the website assets from the s3 bucket to the attached amazon basic block store (amazon ebs) volume.run the s3 sync command once each hour to keep files up to date.
D.restore an amazon elastic block store (amazon ebs) snapshot w.th the website assets.attach the ebs snapshot as a secondary ebs volume when a new ebs ec2 instance is launched.configure the website hosting application to reference the website assets that are stored in the secondary ebs volume.



289, a company has an application mat provides marketing services to stores. the services are based on previous purchases by store customers. the stores upload transaction data to the company through sftp, and the data is processed and fii'fr:d to generate new marketing offers.some of the files can exceed 200 gb in size.recently, the company discovered that some of the stores have uploaded tiles that contain personally identifiable information (pii) mat should not have been included. the company wants administrators to be alerted if pll is shared again. the company also wants to automate remediation. what should a solutions architect do to meet these requirements with the leas f development effort?
A.use an amazon s3 bucket as a secure transfer point.use amazon inspector to scan the objects in the bucket if objects contain pll, trigger an s3 lifecycle policy to remove the objects that contain pll.
B.use an amazon s3 bucket as a secure transfer point.use amazon macie to scan the objects in the bucketif objects contain pll, use amazon simple notification service (amazon sns) to trigger a notification to the administrators to remove the objects that contain pll.
C.implement custom scanning algorithms in an aws lambda function.trigger the function when objects are loaded into the bucket.if objects contain pll, use amazon simple notification service (amazon sns) to trigger a notification to the administrators to remove the objects that contain pii.
D.implement custom scanning algorithms in an aws lambda function.trigger the function when objects are loaded into the bucket.if objects contain pll, use amazon simple email service (amazon ses) to trigger a notification to the administrators and trigger an s3 lifecycle policy to remove the objects that contain pll.



290, a company has an application that calls aws lambda functions. a code review shows that database credentials are stored in a lambda function's source code, which violates the

company's security policy. the credentials must be securely stored and must be automatically rotated on an ongoing basis to meet security policy requirements.what should a solutions architect recommend to meet these requirements in the most secure manner?
A.store the password in aws cloudhsm associate the lambda function with a role that can use the key id to retrieve the password from cloudhsm. use cloudhsm to automatically rotate the password.
B.store the password in aws secrets manager. associate the lambda function with a role that can use the secret id to retrieve the password from secrets manager. use secrets manager to automatically rotate the password.
C.store the password in aws key management service (aws kms) associate the lambda function with a role that can use the key id to retrieve the password from aws kms. use aws kms to automatically rotate the uploaded password.
D.move the database password to an environment variable that is associated with the lambda function.retrieve the password from the environment variable by invoking the function. create a deployment script to automatically rotate the password.




29L  a company's web application consists of multiple amazon ec2instances that run behind an application load balancer in a vpc an amazon rds for mysql db instance contains the data. the company needs the ability to automatically detect and respond to suspicious or unexpected behavior in its aws environment. the company already has added as waf to its architecture. what should a solutions architect do next to protect against threats?
A.use amazon guardduty to perform threat detection. configure amazon eventbridge (amazon cloudwatch events) to filter for guardduty findings and to invoke an aws lambda function to adjust the aws waf rules
B.use aws firewall manager to perform threat detection. configure amazon eventbridge (amazon cloudwatch events) to filter for firewall manager findings and to invoke an aws lambda function to adjust the aws waf webacl
C.use amazon inspector to perform threat detection and to update the aws waf rules. create a vpc network acl to limit access to the web application.
D.use amazon made to perform threat detection and to update the aws waf rules. create a vpc network acl to limitaccess to the web application.


292, a company is running a global application. the application's users submit multiple videos that are then merged into a single video file.the application uses a single amazon s3 bucket in the us-east-1 region to receive uploads from users. the same s3 bucket provides the download location of the single video file that is produced. the final video file output has an average size of 250 gb.the company needs to develop a solution that delivers faster uploads and downloads of the video files that are stored in amazon s3.the company will offer the solution as a subscription to users who want to pay for the increased speed. what should a solutions architect do to meet these requirements?
A.enable aws global accelerator for the s3 endpoint. adjust the application's upload and download links to use the global.accelerator s3 endpoint for users who have a subscription
B.enable s3cross-region replication to s3 buckets in all other aws regions. use an amazon route 53 geolocation routing policy to route s3 requests based on the location of users who have a subscription.
C.create an amazon cloudfront distribution, and use the s3 bucket in us-east-1 as an origin. adjust the application to use the cloudfront url as the upload and download links for users who have a subscription.
D.enable s3 transfer acceleration for the s3 bucket in us-east-1. configure the application to use the bucket's s3- accelerate endpoint domain name for the upload and download links for users who have a subscription.




293, a company sells datasets to customers who do research in artificial intelligence and machine learning (aimu.the datasets are large formatted files met are stored in an amazon s3 bucket in the us-easl-1 region. the company hosts a web application that the customers use o purchase access to a given dataset. the web application is deployed on mutate amazon ec2 instances behind an application load balancer. after a purchase is made customers receive an s3 signed url that allows access to the files. the customers are distributed across north america and europe. the company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. what should a solutions architect do to meet these requirements?
A.configure s3 transfer accelerator on the ex sting s3 bucket direct customer requests to the s3 transfer acceleration endpoint continue to use s3 signed urls tor access control
B.deploy an amazon cloudfront distribution with the existing s3 bucket as the origin direct customer requests to the cloudfront urlswitch to cloudfront signed urls for access control

C.set up a second s3 ducket in the eu-centtal-1 region with s3 cross-region replication between lite duckets direct customer requests to the closest region.continue to use s3 signed urls for access control
D.modify the web application to enable streaming of the datasets to and users configure the web application to read the data from the existing s3 bucket implement access control directly in the application


 tfr: https:/ / docs.aws.amazon.com/amazons3/latest/userguide/transfer­ acceleration.html


294, a company has designed an application where users provide small sets of textual data by calling a public api the application runs on aws and includes a public amazon api gateway api that forwards requests to an aws lambda function for processing. the lambda function then writes the data to an amazon aurora serverless database for consumption.the company is concerned that it could lose some user data if a lambda function fails to process the request property or reaches a concurrency limit.what should a solutions architect recommend to resolve this concern?
A.split the existing lambda function into two lambda functions. configure one function to receive api gateway requests and put relevant items into amazon simple queue service (amazon sqs). configure the other function to read items from amazon sqs and save the data into aurora
B.configure the lambda function to receive api gateway requests and write relevant items to amazon elasticache.configure elasticache to save the data into aurora.
(.increase the memory for the lambda function. configure aurora to use the muiti-az feature
D.split the existing lambda function into two lambda functions. configure one function to receive api gateway requests and put relevant items into amazon simple notification service (amazon sns) configure the other function to read items from amazon sns and save the data into aurora



295, a company wants to deploy a new public web application on aws. the application includes a web server tier that uses amazon ec2 instances. the application also includes a database tier that uses an amazon rds for mysql db instance.the application must be secure and accessible for global customers that have dynamic ip addresses. how should a solutions architect configure the security groups to meet these requirements?

A.configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0.configure the security group for the db instance to allow inbound traffic on port 3306 from the security group of the web servers.
B.configure the security group for the web servers to allow inbound traffic on port 443 from the ip addresses of the customers. configure the security group for the db instance to allow inbound traffic on port 3306 from the security group of the web servers.
C.configure the security group for the web servers to allow inbound traffic on port 443 from the ip addresses of thecustomers. configure the security group for the db instance to allow inbound traffic on port 3306 from the ip addresses of the customers.
D.configure the security group for the web servers to allow inbound traffic on port 443 from 00.0.0/0.configure the security group for the db instance to allow inbound traffic on port 3306 from 0.0.0.0/0.




296, a company is planning to run a group of amazon ec2 instances that connect to an amazon aurora database. the company has built an aws cloudformation template to deploy the ec2 instances and the aurora db cluster. the company wants to allow the instances to authenticate to the database in a secure way. the company does not want to maintain static database credentials. which solution meets these requirements with the least operational effort?
A.create a database user with a user name and password add parameters for the database user name and password to the cloudformation template. pass the parameters to the ec2 instances when the instances are launched.
B.create a database user with a user name and password. store the user name and password in aws systems manager parameter store. configure the ec2 instances to retrieve the database credentials from parameter store.
C.configure the db cluster to use iam database authentication create a database user to use with iam authentication.associate a role with the ec2 instances to allow applications on the instances to access the database
D.configure the db cluster to use iam database authentication with an iam user. create a database user that has a name that matches the iam user. associate the iam user with the ec2 instances to allow applications on the instances to access the database.


297, a company is implementing new data retention policies for all databases that run on amazon rds db instances. the company must retain daily backups for a minimum period of 2 years. the backups must be consistent and restorable.which solution should a solutions architect recommend to meet these requirements?
A.create a backup vault in aws backup to retain rds backups. create a new backup plan with a daily schedule and an expiration period of 2 years after creation. assign the rds db instances to the backup plan.
B.configure a backup window for the rds db instances for daily snapshots. assign a snapshot retention policy of 2 years to each rds db instance. use amazon data lifecycle manager (amazon dim) to schedule snapshot deletions.
C.configure database transaction logs to be automatically backed up to amazon cloudwatch logs with an expiration period of 2 years.
D.configure an aws database migration service (aws dms) replication task deploy a replication instance, and configure a change data capture (cdc) task to stream database changes to amazon s3 as the target configure s3 lifecycle policies to delete the snapshots after 2 years.



298, a company hosts a multi-tier web application that uses an amazon aurora mysql db cluster for storage. the application tier is hosted on amazon ec2 instances. the company's it security guidelines mandate that the database credentials be encrypted and rotated every 14 days. what should a solutions architect do to meet this requirement with the least operational effort?
A.create a new aws key management service (aws kms) encryption key. use aws secrets manager to create a new secret that uses the kms key with the appropriate credentials. associate the secret with the aurora db cluster configure a custom rotation period of 14 days.
B.create two parameters in aws systems manager parameter store.one for the user name as a string parameter and one that uses the secure string type for the password. select aws key management service (aws kms) encryption for the password parameter, and load these parameters in the application tier. implement an aws lambda function that rotates the password every 14 days
C.store a file that contains the credentials in an aws key management service (aws kms) encrypted amazon elastic file system (amazon efs) file system. mount the efs file system in all ec2 instances of the application tier. restrict the access to the file on the file system so that the application can read the file and that only super users can modify the

file.implement an aws lambda function that rotates the key in aurora every 14 days and writes new credentials into the file.
D.store a file that contains the credentials in an aws key management service (aws kms) encrypted amazon s3 bucket that the application uses to load the credentials download the file to the application regularly to ensure that the correct credentials are used. implement an aws lambda function that rotates the aurora credentials every 14 days and uploads these credentials to the file in the s3 bucket.



299, a company is using a content management system that runs on a single amazon ec2 instance. the ec2 instance contains both the web server and the database software. the company must make its website platform highly available and must enable the website to scale to meet user demand. what should a solutions architect recommend to meet these requirements?
A.move the database to amazon rds, and enable automatic backups. manually launch another ec2 instance in the same availability zone. configure an application load balancer in the availability zone, and set the two instances as targets.
B.migrate the database to an amazon aurora instance with a read replica in the same availability zone as the existing ec2 instance. manually launch another ec2 instance in the same availability zone configure an application load balancer, and set the two ec2 instances as targets.
C.move the database to amazon aurora with a read replica in another availability zone create an amazon machine image (ami) from the ec2 instance. configure an application load balancer in two availability zones. attach an auto scaling group that uses the ami across two availability zones
D.move the database to a separate ec2 instance, and schedule backups to amazon s3. create an amazon machine image (ami) from the original ec2 instance. configure an application load balancer in two availability zones. attach an auto scaling group that uses the ami across two availability zones.



300, a company is using amazon redshift for analytics and to generate customer reports. the company recently acquired 50 tb of additional customer demographic data. the data is stored in csv files in amazon s3. the company needs a solution that joins the data and

visualizes the results with the least possible cost and effort.what should a solutions architect recommend to meet these requirements?
A. use amazon redshift spectrum to query the data in amazon s3 directly and join that data with the existing data in amazon redshift. use amazon quicksight to build the visualizations.
B. use amazon athena to query the data in amazon s3. use amazon quicksightto join the data from athena with the existing data in amazon redshift and to build the visualizations
C. increase the size of the amazon redshift cluster, and load the data from amazon s3 use amazon emr notebooks to query the data and build the visualizations in amazon redshift
D. export the data from the amazon redshift cluster into apache parquet files in amazon s3. use amazon elasticsearch service (amazon es) to query the data use kibanato visualize the results.




30L  a solutions architect is implementing federated access to aws for users of the company's mobile application. due to regulatory and security requirements, the application must use a custom-built solution for authenticating users and must use iam roles for authorization. which of the following actions would enable authentication and authorization and satisfy the requirements? (select two.)
A. use a custom-built saml -compatible solution for authentication and aws sso for authorization.
B. create a custom-built ldap connector using amazon api gateway and aws lambda for authentication. store authorization tokens in amazon dynamodb, and validate authorization requests using another lambda function that reads the credentials from dynamodb.
C. use a custom-built openid connect compatible solution with aws sso for authentication and authorization.
D. use a custom-built saml-compatible solution that uses ldap for authentication and uses a saml assertion to perform authorization to the iam identity provider.
E. use a custom-built openid connect-compatible solution for authentication and use amazon cognito for authorization.




302, a company is using multiple aws accounts. the dns records are stored in a private hosted zone for amazon route 53 in account a the company's applications and databases are

running in account ba solutions architect will deploy a two-tier application in a new vpc. to simplify the configuration, the db.example.com cname record set for the amazon rds endpoint was created in a private hosted zone for amazon route 53.during deployment, the application failed to start. troubleshooting revealed that db.example.com is not resolvable on the amazon ec2 instance. the solutions architect confirmed that the record set was created correctly in route 53.which combination of steps should the solutions architect take to resolve this issue? (select two.)
A. deploy the database on a separate ec2 instance in the new vpc create a record set for the instance's private ip in the private hosted zone.
B. use ssh to connect to the application tier ec2 instance. add an rds endpoint ip address to the / etc/ resolv.conf file.
C. create an authorization to associate the private hosted zone in account a with the new vpc
in account b
D. create a private hosted zone for the example.com domain in account b configure route 53 replication between aws accounts.
E. associate a new vpc in account b with a hosted zone in account a delete the association authorization in account a


,ff ;j;fr: if you want to associate a vpc that you created with one aws account with a private hosted zone that you created with a different account, perform the following procedure:to associate an amazon vpc and a private hosted zone that you created with different aws accountsusing the account that created the hosted zone, authorize the association of the vpc with the private hosted zone by using one of the following methods:aws di - see create-vpc­ association-authorization in the aws di command reference awssdk or aws tools for windows powershell - see the applicable documentation on theawsdocumentation pageamazon route 53 api - see createvpcassociationauthorization in the amazon route 53 api reference note the following:if you want to associate multiple vpcs that you created with one account with a hosted zone that you created with a different account, you must submit one authorization request for each vpcwhen you authorize the association, you must specify the hosted zone id, so the private hosted zone must already exist.you can't use the route 53 console either to authorize the association of a vpc with a private hosted zone or to make the association.using the account that created the vpc, associate the vpc with the hosted zone. as with authorizing the association, you can use the aws sdk, tools for windows powershell, the aws di, or the route 53 api. if you're using the api, use the associatevpcwithhostedzone action.recommended - delete the authorization to associate the vpc with the hosted zone. deleting the authorization does not affect the association, it just prevents you from reassociating the vpc with the hosted zone in the future. if you want to reassociate the vpc with the hosted zone, you'll need to repeat steps 1 and 2 of this

procedure.if you don't delete the authorization, the other account can re-associate the vpc at any time because the authorization is still in place. for more information, see delete-vpc­ association-authorization.


303, a company has a photo sharing social networking application. to provide a consistent experience for users.the company performs some image processing on the photos uploaded by users before publishing on the application.the image processing is implemented using a set of python libraries.the current architecture is as follows.-the image processing python code runs in a single amazon ec2 instance and stores the processed images in an amazon s3 bucket named imagebucket.-the front-end application, hosted in another bucket, loads the images from imagebucket to display to users.with plans for global expansion, the company wants to implement changes in its existing architecture to be able to scale for increased demand on the application and reduce management complexity as the application scales.which combination of changes should a solutions architect make? (select two.)
A. place the image processing ec2 instance into an auto scaling group.
B. use aws lambda to run the image processing tasks.
C. use amazon rekognition for image processing.
D. use amazon cloudfront in front of imagebucket
E. deploy the applications in an amazon ecs cluster and apply service auto scaling.



304, a large company with hundreds of aws accounts has a newly established centralized internal process for purchasing new or modify existing reserved instances. this process requires all business units that want to purchase or modify reserved instances to submit requests to a dedicated team for procurement or execution.previously, business units would directly purchase or modify reserved instances in their own respective aws accounts autonomously.which combination of steps should be taken to proactively enforce the new process in the most secure way possible? (select two)
A. ensure all aws accounts are part of an aws organizations structure operating in all features mode.
B. use aws config to report on the attachment of an iam policy that denies access to the.ec2: purchasereservedinstancesoffering and ec2: modify reservedinstances actions.
C. in each aws account,create an iam policy with a deny rule to the ec2 purchase reservedinstances offering and ec2: modify reservedinstances actions.
A. 
D. create an scp that contains a deny rule to the ec2: purchase reservedinstances offering and ec2:modify reservedinstances actions. attach the scp to each organizational unit(ou)of the aws organizations structure.
E. ensure that all aws accounts are part of an aws organizations structure operating in consolidated billing features mode.


,ff i'fr: forst put all accounts into ou and the apply scp to deny access to the ec2 api that procure new reserved instances or modify existing reserved instances.scps are available only in an organization that has all features enabled.an scp restricts permissions for iam users and roles in member accounts.


305, a financial services company is moving to aws and wants to enable developers to experiment and innovate while preventing access to production applications. the company has the following requirements production workloads cannot be directly connected to the internet all workloads must be restricted to the us- west-2 and eu-central-1 regions notification should be sent when developer sandboxes exceed $500 in aws spending monthlywhich combination of actions needs to be taken to create a multi-account structure that meets the company's requirements? (select three)
A. create accounts for each production workload within an organization in aws organizations place the production accounts within an organizational unit (ou). for each account, delete the default vpc.create an scp with a deny rule for the attach an internet gateway and create a default vpc actions. attach the scp to the ou for the production accounts.
B. create accounts for each production workload within an organization in aws organizations. place the production accounts within an organizational unit (ou). create an scp with a deny rule on the attach an internet gateway action create an scp with a deny rule to prevent use of the default vpc. attach the scps to the ou for the production accounts.
C. create a scp containing a deny effect for cloudfront: iam: route53 support with a stringnotequals condition on an aws requestedregion condition key with us-west-2 and eu­ central-1 values. attach the scp to the organizations root.
D. create an iam permission boundary containing a deny effect for cloudfront:iam: route53:, and support:with a stringnotequals condition on an aws requested region condition key with us-west-2 and eu- central-1 values attach the ermission boundary to an iam group containing the development and production users.
E. create accounts for each development workload within an organization in aws organizations. place the development accounts within an organizational unit(ou). create a
A. 
custom aws config rule to deactivate all iam users when an account's monthly bill exceeds
$500.
F. create accounts for each development workload within an organization in aws organizations place the development accounts within an organizational unit(ou). create a budget within aws budgets for each development account to monitor and report on monthly spending exceeding $500.


 tfr: you can create a baseline scp to deny usage of default vpc and tag it to production ou instead of manually deleting the default vpc (containing igw) every time a new production account is created.


306, a company's main intranet page has experienced degraded response times as its user base has increased although there are no reports of users seeing error pages. the application uses amazon dynamodb in read-only mode. amazon dynamodb latency metrics for successful requests have been in a steady state even during times when users have reported degradation. the development team has correlated the issue to provisioned throughputexceeded exceptions in the application logs when doing scan and read operations. the team also identified an access pattern of steady spikes ofread activity on a distributed set of individual data items the chief technology officer wants to improve the user experience. which solutions will meet these requirements with the least amount of changes to the application? (select two.)
A. change the data model of the dynamodb tables to ensure that all scan and read operations meet dynamodb best practices of uniform data access, reaching the full request throughput provisioned for the dynamodb tables.
B. enable dynamodb auto scaling to manage the throughput capacity as table traffic increases. set the upper and lower limits to control costs, and set a target utilization given the peak usage and how quickly the traffic changes.
C. provision amazon elasticache for redis with cluster mode enabled. the cluster should be provisioned with enough shards to spread the application load and provision at least one read replica node for each shard.
D. implement the dynamodb accelerator(dax) client and provision a dax cluster with the appropriate node types to sustain the application load. tune the item and query cache configuration for an optimal user experience.
E. remove error retries and exponential backoffs in the application code to handle throttling errors.
A. 


Mtfr: will require least amount of changes in the application while increasing over all performance ->fm41


307, a company is migrating its applications to aws. the applications will be deployed to aws accounts owned by business units. the company has several teams of developers who are responsible for the development and maintenance of all applications. the company is expecting rapid growth in the number of users.the company's chief technology officer has the following requirements. developers must launch the aws infrastructure using aws cloudformation. developers must not be able to create resources outside of cloud formation. the solution must be able to scale to hundreds of aws accounts.which of the following would meet these requirements?(select two)
A. using cloudformation, create an iam role that can be assumed by cloud formation that has permissions to create all the resources the company needs. use cloudformation stack sets to deploy this template to each aws account.
B. in a central account, create an iam role that can be assumed by developers and attach a policy that allows interaction with cloudformation. modify the assume policy document action to allow the iam role to be passed to cloudformation.
C. using cloud formation, create an iam role that can be assumed by developers, and attach policies that allow interaction with and passing a role to cloudformation. attach an inline policy to deny access to all other aws services. use cloudformation stack sets to deploy this template to each aws account.
D. using cloud formation, create an iam role for each developer, and attach policies that allow interaction with cloudformation. use cloud formation stack sets to deploy this template to each aws account.
E. in a central as account, create an iam role that can be assumed by cloud formation that has permissions to create the resources the company requires. create a cloudformation stack policy that allows the iam role to manage resources use cloudformation stack sets to deploy the cloudformation stack policy to each aws account.


Mtfr: bis wrong."modify the assumerolepolicydocument action to allow the iam role to be passed to cloudformation." => this sentence is wrong."assumerolepolicydocumentthe trust policy that is associated with this role. trust policies define which entities can assume the role." we need to use iam:passrole to pass the role from developer to cloudformation.assumerolepolicydocument is used for assume the role only.e is a misleading option. you need to deploy the cloudformation template and not just the stack

policy. moreover, the purpose of the stack policy is to prevent accidental changes to the resources being created by the cloudformation template which is not the requirement.modiyf on ae>ac


308, a manufacturing company is growing exponentially and has secured funding to improve its it infrastructure and ecommerce presencethe company's ecommerce platform consists ofstatic assets primanly comprised of product images stored in amazon s3 amazon dynamodb tables that store product information, user information, and order information web servers containing the applications front-end behind elastic load balancers the company wants to set up a disaster recovery site in a separate region which combination of actions should the solutions architect take to implement the new design while meeting all therequirements?(select three)
A. enable amazon route 53 health checks to determine if the primary site is down, and route traffic to the disaster recoverysite if there is an issue.
B. enable amazon s3 cross-region replication on the buckets that contain static assets.
C. enable multi-region targets on the elastic load balancer and target amazon ec2 instances in both regions.
D. enable dynamodb global tables to achieve a multi-region table replication.
E. enable amazon cloudwatch and create cloud watch alarms that route traffic to the disaster recovery site when applicationlatency exceeds the desired threshold.
F. enable amazon s3 versioning on the source and destination buckets containing static assets to ensure there is a rollbackversion available in the event of data corruption.


 tfr: need to have route 53 to sense issue and switch to the dr site. need to have s3 replication to have static content synced up. global table in dynamo db to synchronize nosql data content.


309, a company has an amazon vpc that is divided into a public subnet and a private subnet. a web application runs in amazon vpc, and each subnet has its own nacl. the public subnet has a cidr of0.0.0/24. an application load balancer is deployed to the public subnet. the private subnet has a cidr of 10.0.1.0/24.amazon ec2 instances that run a web server on port 80 are launched into the private subnet.only network traffic that is required for the application load balancer to access the web application can be allowed to travel between the public and private subnets.what collection of rules should be written to ensure that the private subnet's nacl meets the requirement?(choose two.)

A. an inbound rule for port 80 from source 0.0.0.0/0.
B. an inbound rule for port 80 from source 10.0.0.0/24.
C. an outbound rule for port 80 to destination 0.0.0.0/0.
D. an outbound rule for port 80 to destination 10.0.0.0/24.
E. an outbound rule for ports 1024 through 65535 to destination0.0.0/24.

Mi-fr: never forget way out for ephemeral ports on nacl.in practice, to cover the different types of clients that might initiate traffic to public-facing instances in your vpc, you can open ephemeral ports 1024-65535. however, you can also add rules to the acl to deny traffic on any malicious ports within that range. ensure that you place the deny rules earlier in the table than the allow rules that open the wide range of ephemeral ports.
https:/ / docs.aws.amazon.com/vpc/latest/userguide/vpc_scenario3.html


310, a solutions architect is designing a publicly accessible web application that is on an amazon cloudfront distribution with an amazon website endpoint as the origin. when the solution is deployed, the website returns an error 403: access denied message.which steps should the solutions architect take to correct the issue? (select two.)
A. remove the s3 block public access option from the s3 bucket
B. remove the requester pays option from the s3 bucket
C. remove the origin access identity (oai) from the cloudfront distribution
D. change the storage class from s3 standard to s3 one zone-infrequent access (s3 one zone­ ia).
E. disable s3 object versioning.




31L	a retail company has a custom net web application running on aws that uses microsoft sql server for the database the application servers maintain a user's session locally. which combination of architecture changes are needed ensure all tiers of the solution are highly available? (select three.)
A. refactor the application to store the user's session in amazon elasticache use application load balancers to distribute the load between application instances
A. 
B. set up the database to generate hourly snapshots using amazon ebs configure an amazon cloudwatch events rule to launch a new database instance if the primary one fails
C. migrate the database to amazon rds tor sql server configure the rds instance to use a multi- az deployment
D. move the net content to an amazon s3 bucket configure the bucket for static website hosting
E. put the application instances in an auto scaling group configure the auto scaling group to create new instances if an instance becomes unhealthy
F. deploy amazon cloudfront in front of the application tier configure cloudfront to serve content from healthy application instances only

Mi-fr: dis incorrect because s3 doesn't support server-side scripting like "asp.net, php, or jsp". in addition, high availability is one of the features of elbelb features -s3 static web site hosting - https:/ / docs.amazonaws.cn/en_us/ amazons3/latest/user-guide/static-website­ hosting.html "you can host a static website on amazon s3. on a static website, individual webpages include static content. a static website might also contain client-side scripts. by contrast, a dynamic website relies on server-side processing, including server-side scripts such as php, jsp, or asp.net. amazon s3 does not support server-side scripting."


312, a company wants to fftnfr: log data using date ranges with a custom application running on aws. the application generates about 10 gb of data every day, which is expected
to grow. a solutions architect is tasked with storing the data in amazon s3 and using amazon athena to fftnfr: the data.which combination of steps will ensure optimal performance as the data grows? (choose two.)
A. store each object in amazon s3 with a random string at the front of each key.
B. store the data in multiple s3 buckets.
C. store the data in amazon s3 in a columnar format, such as apache parquet or apache ore.
D. store the data in amazon s3 in objects that are smaller than 10 mb.
E. store the data using apache hive partitioning in amazon s3 using a key that includes a date, such as dt=2019-02.

Mi-fr: optimize columnar data store generation+ partitionoptimal performance with athena is achieved with columnar storage and partitioning the data.

https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon­ athena/


313, a utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of- use metering. when a meter sends data to aws, the data is sent to amazon api gateway, processed by an aws lambda function, and stored in an amazon dynamodb table. during the pilot phase, the lambda functions took from 3 to 5 seconds to complete. as more smart meters are deployed, the engineers notice the lambda functions are taking from 1 to 2 minutes to complete. the functions are also increasing in duration as new types of metrics are collected from the devices. there are many provisionedthroughputexceededexception errors while performing put operations on dynamodb, and there are also manytoomanyrequestsexception errors from lambda.which combination of changes will resolve these issues? (select two.)
A. increase the write capacity units to the dynamodb table.
B. increase the memory available to the lambda functions.
C. increase the payload size from the smart meters to send more data.
D. stream the data into an amazon kinesis data stream from api gateway and process the data in batches.
E. collect data in an amazon sqs fifo queue, which triggers a lambda function to process each message.

Mi-fr: a-> dynamo increase write limit fix provisionedthroughputexceededexception d -> kinesis will group requests in batch so decrease requests in lambda so fixlamda is having 100 concurrent executions per region. so need to use data streams and batch processing


314, during a security audit of a service team's application, a solutions architect discovers that a usernameand password for an amazon rds database and a set of aws iam user credentials can be viewed in the aws lambda function code. the lambda function uses the username and password to run queries on the database, and it uses the iam credentials to call aws services in a separate management account. the solutions architect is concerned that the credentials could grant inappropriate access to anyone who can view the lambda code.the management account and the service team's account are in separate aws organizations organizational units (ous). which combination of changes should the solutions architect make to improve the solution's security? (select two.)

A. configure lambda to assume a role in the management account with appropriate access to aws.
B. configure lambda to use the stored database credentials in aws secrets manager and enable automatic rotation.
C. create a lambda function to rotate the credentials every hour by deploying a new lambda version with the updated credentials :
D. use an scp on the management account's ou to prevent iam users from accessing resources in the service team's account.
E. enable aws shield advanced on the management account to shield sensitive resources from unauthorized iam access.


Mi-fr: scp will prevent an iam user credentials to access the services which will cause the lambda function to fail. we don't want the lambda function to fail wile calling aws services. option a provides an elegant and standard solution to allow lambda in one account to access aws services in another account by assuming the iam role that provides it access to call those aws services.d is wrong as users from one account cannot access resources from another account if not allowed through cross- account access using assumed roles. there's no need to use scp for denye is wrong as shield is used for ddos protectionc does not make sense with hourly redeploying of lambda


315, an advisory firm is creating a secure data analytics solution for its regulated financial services users. users will upload their raw data to an amazon s3 bucket, where they have putobject permissions only. data will be Mi-Jr:d by applications running on an amazon emr cluster launched in a vpc. the firm requires that the environment be isolated from the internet. all data at rest must be encrypted using keys controlled by the firm.which combination of actions should the solutions architect take to meet the user's security requirements? (choose two.)
A. launch the amazon emr cluster in a private subnet configured to use an aws kms cmk for at- rest encryption.configure a gateway vpc endpoint for amazon s3 and an interface vpc endpoint for aws kms.
B. launch the amazon emr cluster in a private subnet configured to use an aws kms cmk for at- rest encryption.configure a gateway vpc endpoint for amazon s3 and a nat gateway to access aws kms.
A. 
C. launch the amazon emr cluster in a private subnet configured to use an aws cloudhsm appliance for at-rest encryption.configure a gateway vpc endpoint for amazon s3 and an interface vpc endpoint for cloudhsm.
D. configure the s3 endpoint policies to permit access to the necessary data buckets only.
E. configure the s3 bucket policies to permit access using an aws:sourcevpce condition to match the s3 endpoint id.

Mi-fr: as the firm requires to be isolated from the internet, its better to go with endpoints so option a in b it uses nat which is in public subnet. next option is to use bucket policies with association with vpc endpoint. vpc endpoints and bucket policies...without removing the existing putobject permissions for the users who are
uploading.https: / / aws.amazon.com/premiumsu pport/knowledge-center /block-s3-traffic­ vpc-ip/


316, a company has registered its domain name with amazon route 53. the company uses amazon api gateway in the ca-central-1 region as a public interface for its backend microservice apis. third-party services consume the apis securely. the company wants to design its api gateway url with the company's domain name and corresponding certificate so that the third-party services can use https.which solution will meet these requirements?
A. create stage variables in api gateway with name="endpoint-url" and value="company domain name" to overwrite the default url. import the public certificate associated with the company's domain name into aws certificate manager (acm).
B. create route 53 dns records with the company's domain name. point the alias record to the regional api gateway stage endpoint. import the public certificate associated with the company's domain name into aws certificate manager (acm) in the us-east-1 region.
C. create a regional api gateway endpoint associate the api gateway endpoint with the company's domain name. import the public certificate associated with the company's domain name into aws certificate manager (acm) in the same region. attach the certificate to the api gateway endpoint.configure route 53to route traffic to the api gateway endpoint.
D. create a regional api gateway endpoint associate the api gateway endpoint with the company's domain name. import the public certificate associated with the company's domain name into aws certificate manager (acm) in the us-east-1 region. attach the certificate to the api gateway apis.create route 53 dns records with the company's domain name. point an a record to the company's domain name.


317, a company stores confidential data in an amazon aurora postgresql database in the
ap-southeast-3 region. the database is encrypted with an aws key management service (aws kms) customer managed key. the company was recently acquired and must securely share a backup of the database with the acquiring company's aws account in ap-southeast-3.what should a solutions architect do to meet these requirements?
A. create a database snapshot. copy the snapshot to a new unencrypted snapshot. share the new snapshot with the acquiring company's aws account
B. create a database snapshot. add the acquiring company's aws account to the kms key policy. share the snapshot with the acquiring company's aws account.
C. create a database snapshot that uses a different aws managed kms key. add the acquiring company's aws account to the kms key alias. share the snapshot with the acquiring company's aws account.
D. create a database snapshot. download the database snapshot. upload the database snapshot to an amazon s3 bucket. update the s3 bucket policy to allow access from the acquiring company's aws account.




318, a payment processing company records all voice communication with its customers and stores the audio files in an amazon s3 bucket. the company needs to capture the text from the audio files. the company must remove from the text any personally identifiable information (pii) that belongs to customers. what should a solutions architect do to meet these requirements?
A. process the audio files by using amazon kinesis video streams. use an aws lambda function to scan for known pii patterns
B. when an audio file is uploaded to the s3 bucket, invoke an aws lambda function to start an amazon textract task to	tfr: the call recordings
C. configure an amazon transcribe transcription job with pii redaction turned on. when an audio file is uploaded to the s3 bucket, invoke an aws lambda function to start the transcription job. store the output in a separate s3 bucket
D. create an amazon connect contact flow that ingests the audio files with transcription turned on. embed an aws lambda function to scan for known pii patterns. use amazon eventbridge (amazon cloudwatch events) to start the contact flow when an audio file is uploaded to the s3 bucket.
A. 



319, a company has more than 5 tb of file data on windows file servers that run on premises. users and applications interact with the data each day.the company is moving its windows workloads to aws. as the company continues this process, the company requires access to aws and on-premises file storage with minimum latency. the company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. the company uses an aws site-to-site vpn connection for connectivity to aws what should a solutions architect do to meet these requirements?
A. deploy and configure amazon fsx for windows file server on aws. move the on-premises file data to fsx for windows file server. reconfigure the workloads to use fsx for windows file server on aws.
B. deploy and configure an amazon s3 file gateway on premises. move the on-premises file data to the s3 file gateway. reconfigure the on-premises workloads and the cloud workloads to use the s3 file gateway
C. deploy and configure an amazon s3 file gateway on premises. move the on-premises file data to amazon s3. reconfigure the workloads to use either amazon s3 directly or the s3 file gateway, depending on each workload's location.
D. deploy and configure amazon fsx for windows file server on aws. deploy and configure an amazon fsx file gateway on premises. move the on-premises file data to the fsx file gateway configure the cloud workloads to use fsx for windows file server on aws. configure the on­ premises workloads to use the fsx file gateway.



320,  a company has thousands of edge devices that collectively generate 1 tb of status alerts each day each alert is approximately 2 kb in size. a solutions architect needs to implement a solution to ingest and store the alerts for future analysis.the company wants a highly available solution. however, the company needs to minimize costs and does not want to manage additional infrastructure. additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days. what is the most operationally efficient solution that meets these requirements?
A. create an amazon kinesis data firehose delivery stream to ingest the alerts. configure the kinesis data firehosestream to deliver the alerts to an amazon s3 bucket set up an s3 lifecycle configuration to transition data to amazon s3 glacier after 14 days
A. 
B. launch amazon ec2 instances across two availability zones and place them behind an elastic load balancer to ingest the alerts. create a script on the ec2instances that will store the alerts in an amazon s3 bucket. set up an s3 lifecycle configuration to transition data to amazon s3 glacier after 14 days
C. create an amazon kinesis data firehose delivery stream to ingest the alerts. configure the kinesis data firehose stream to deliver the alerts to an amazon elasticsearch service (amazon es) cluster.set up the amazon es cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.
D. create an amazon simple queue service (amazon sqs) standard queue to ingest the alerts, and set the message retention period to 14 days. configure consumers to poll the sqs queue, check the age of the message, and nfr: the message data as needed if the message is 14 days old, the consumer should copy the message to an amazon s3 bucket and delete the message from the sqs queue



32L	a company has an application that processes customer orders. the company hosts the application onanamazonec2instance that saves the orders to an amazon aurora database occasionally when traffic is high, the workload does not process orders fast enough.what should a solutions architect do to write the orders reliably to the database as quickly as possible?
A. increase the instance size of the ec2instance when traffic is high write orders to amazon simple notification service (amazon sns). subscribe the database endpoint to the sns topic.
B. write orders to an amazon simple queue service (amazon sqs) queue use ec2instances in an auto scaling group behind an application load balancer to read from the sqs queue and process orders into the database
C. write orders to amazon simple notification service (amazon sns). subscribe the database endpoint to the sns topic use ec2 instances in an auto scaling group behind an application load balancer to read from the sns topic
D. write orders to an amazon simple queue service (amazon sqs) queue when the ec2instance reaches cpu threshold limits use scheduled scaling of ec2 instances in an auto scaling group behind an application load balancer to read from the sqs queue and process orders into the database.


322, a solutions architect needs to design a highly available application consisting of web, application, and database tiers https content delivery should be as close to the edge as possible, with the least delivery time.which solution meets these requirements and is most secure?
A. configure a public application load balancer (alb) with multiple redundant amazon ec2 instances in public subnets.configure amazon cloudfront to deliver https content using the public alb as the origin
B. configure a public application load balancer with multiple redundant amazon ec2 instances in private subnets. configure amazon cloudfront to deliver https content using the ec2 instances as the origin
C. configure a public application load balancer (alb) with multiple redundant amazon ec2 instances in private subnets.configure amazon cloudfront to deliver https content using the public alb as the origin.
D. configure a public application load balancer with multiple redundant amazon ec2 instances in public subnets configure amazon cloudfront to deliver https content using the ec2 instances as the origin.




323, a company's containerized application runs on an amazon ec2instance. the application needs to download security certificates before it can communicate with other business applications. the company wants a highly secure solution to encrypt and decrypt the certificates in near real time. the solution also needs to store data in highly available storage after the data is encrypted. which solution will meet these requirements with theleast operational overhead?
A. create aws secrets manager secrets for encrypted certificates. manually update the certificates as needed. control access to the data by using fine-grained iam access
B. create an aws lambda function that uses the python cryptography library to receive and perform encryption operations. store the function in an amazon s3 bucket
C. create an aws key management service (aws kms) customer managed key. allow the ec2 role to use the kms key for encryption operations. store the encrypted data on amazon s3
D. create an aws key management service (aws kms) customer managed key. allow the ec2 role to use the kms key for encryption operations. store the encrypted data on amazon elastic block store (amazon ebs) volumes


324, a company runs a global web application on amazon ec2 instances behind an application load balancer. the application stores data in amazon aurora the company needs to create a disaster recoverysolution and can tolerate up to 30 minutes of downtime and potential data loss. the solution does not need to handle the load when the primary infrastructure is healthy. what should a solutions architect do to meet these requirements?
A. deploy the application with the required infrastructure elements in place. use amazon route 53 to configure active- passive failover. create an aurora replica in a second aws region
B. host a scaled-down deployment of the application in a second aws region. use amazon route 53 to configure active-active failover. create an aurora replica in the second region.
C. replicate the primary infrastructure in a second aws region. use amazon route 53to configure active- active failover. create an aurora database that is restored from the latest snapshot
D. back up data with aws backup. use the backup to create the required infrastructure in a second aws region. use amazon route 53to configure active-passive failover. create an aurora second primary instance in the second region.



325, a company is launching a new application and will display application metrics on an amazon cloudwatch dashboard. the company's product manager needs to access this dashboard periodically. the product manager does not have an aws account. a solutions architect must provide access to the product manager by following the principle of least privilege.which solution will meet these requirements?
A. share the dashboard from the cloudwatch console. enter the product manager's email address, and complete the sharing steps. provide a shareable link for the dashboard to the product manager.
B. create an iam user specifically for the product manager. attach the cloudwatchreadonlyaccess aws managed policy to the user. share the new login credentials with the product manager. share the browser url of the correct dashboard with the product manager.
C. create an iam user for the company's employees attach the viewonlyaccess aws managed policy to the iam user. share the new login credentials with the product manager. ask the product manager to navigate to the cloudwatch console and locate the dashboard by name in the dashboards section
A. 
D. deploy a bastion server in a public subnet. when the product manager requires access to the dashboard, start the server and share the rdp credentials. on the bastion server, ensure that the browser is configured to open the dashboard url with cached aws credentials that have appropriate permissions to view the dashboard.



326, a company performs monthly maintenance on its aws infrastructure. during these maintenance activities, the company needs to rotate the credentials for its amazon rds for mysql databases across multiple aws regions.which solution will meet these requirements with the least operational overhead?
A. store the credentials as secrets in aws secrets manager. use multi-region secret replication for the required regions. configure secrets manager to rotate the secrets on a schedule
B. store the credentials as secrets in aws systems manager by creating a secure string parameter. use multi-region secret replication for the required regions. configure systems manager to rotate the secrets on a schedule.
C. store the credentials in an amazon s3 bucket that has server-side encryption (sse) enabled. use amazon eventbridge (amazon cloudwatch events) to invoke an aws lambda function to rotate the credentials.
D. encrypt the credentials as secrets by using aws key management service (aws kms) multi­ region customer managed keys. store the secrets in an amazon dynamodb global table. use an aws lambda function to retrieve the secrets from dynamodb. use the rds api to rotate the secrets.



327, a company wants to improve its ability to clone large amounts of production data into a test environment in the same aws region. the data is stored in amazon ec2instances on amazon elastic block store (amazon ebs) volumes. modifications to the cloned data must not affect the production environment. the software that accesses this data requires consistently high io performance.a solutions architect needs to minimize the time that is required to clone the production data into the test environment.which solution will meet these requirements?
A. take ebs snapshots of the production ebs volumes. restore the snapshots onto ec2instance store volumes in the test environment
A. 
B. configure the production ebs volumes to use the ebs multi-attach feature. take ebs snapshots of the production ebs volumes. attach the production ebs volumes to the ec2 instances in the test environment.
C. take ebs snapshots of the production ebs volumes. create and initialize new ebs volumes. attach the new ebs volumes to ec2 instances in the test environment before restoring the volumes from the production ebs snapshots.
D. take ebs snapshots of the production ebs volumes. turn on the ebs fast snapshot restore feature on the ebs snapshots. restore the snapshots into new ebs volumes. attach the new ebs volumes to ec2instances in the test environment



328, a company needs to configure a real-time data ingestion architecture for its application. the company needs an api, a process that transforms data as the data is streamed, and a storage solution for the data. which solution will meet these requirements with the least operational overhead?
A. deploy an amazon ec2 instance to host an api that sends data to an amazon kinesis data stream.create an amazon kinesis data firehose delivery stream that uses the kinesis data stream as a data source. use awslambda functions to transform the data. use the kinesis data firehose delivery stream to send the data to amazon s3
B. deploy an amazon ec2 instance to host an api that sends data to aws glue. stop source/destination checking on the ec2 instance. use aws glue to transform the data and to send the data to amazon s3
C. configure an amazon api gateway api to send data to an amazon kinesis data stream. create an amazon kinesis data firehose delivery stream that uses the kinesis data stream as a data source. use aws lambda functions to transform the data. use the kinesis data firehose delivery stream to send the data to amazon s3.
D. configure an amazon api gateway api to send data to aws glue. use aws lambda functions to transform the data. use aws glue to send the data to amazon s3.



329, a company is building a new dynamic ordering website. the company wants to minimize server maintenance and patching. the website must be highly available and must scale read and write capacity as quickly as possible to meet changes in user demand.which solution will meet these requirements?

A. host static content in amazon s3. host dynamic content by using amazon api gateway and aws lambda. use amazon dynamodb with on-demand capacity for the database. configure amazon cloudfront to deliver the websitecontent.
B. host static content in amazon s3. host dynamic content by using amazon api gateway and aws lambda. use amazon aurora with aurora auto scaling for the database. configure amazon cloudfront to deliver the website content.
C. host all the website content on amazon ec2 instances. create an auto scaling group to scale the ec2instances. use an application load balancer to distribute traffic. use amazon dynamodb with provisioned write capacity for the database.
D. host all the website content on amazon ec2 instances. create an auto scaling group to scale the ec2instances use an application load balancer to distribute traffic. use amazon aurora with aurora auto scaling for the database.



330, an online retail company has more than 50 million active customers and receives more than 25,000 orders each day. the company collects purchase data for customers and stores this data in amazon s3.additional customer data is stored in amazon rds.the company wants to make all the data available to various teams so that the teams can perform analytics. the solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overheadwhich solution will meet these requirements?
A. migrate the purchase data to write directly to amazon rds. use rds access controls to limit access
B. schedule an aws lambda function to periodically copy data from amazon rds to amazon s3. create an aws glue crawler. use amazon athena to query the data use s3 policies to limit access
C. create a data lake by using aws lake formation. create an aws glue jdbc connection to amazon rds. register the s3 bucket in lake formation. use lake formation access controls to limit access
D. create an amazon redshift cluster. schedule an aws lambda function to periodically copy data from amazon s3 and amazon rds to amazon redshift. use amazon redshift access controls to limit access


33L	a company has introduced a new policy that allows employees to work remotely from their homes if they connect by using a vpn. the company is hosting internal applications with vpcs in multiple aws accounts currently, the applications are accessible from the company's on-premises office network through an aws site- to-site vpn connection the vpc in the company's main aws account has peering connections established with vpcs in other aws accounts.a solutions architect must design a scalable aws client vpn solution for employees to use while they work from homewhat is the most cost-effective solution that meets these requirements?
A. create a client vpn endpoint in each aws account. configure required routing that allows access to internal applications
B. create a client vpn endpoint in the main aws account configure required routing that allows access to internal applications
C. create a client vpn endpoint in the main aws account provision a transit gateway that is connected to each aws account configure required routing that allows access to internal applications
D. create a client vpn endpoint in the main aws account. establish connectivity between the client vpn endpoint and the aws site-to-site vpn




332, a company has developed a web application. the company is hosting the application on a group of amazon ec2 instances behind an application load balancer. the company wants to improve the security posture of the application and plans to use aws wafweb ads. the solution must not adversely affect legitimate traffic to the application.how should a solutions architect configure the web ads to meet these requirements?
A. set the action of the web ad rules to count enable aws waf logging. analyze the requests for false positives. modify the rules to avoid any false positive. over time, change the action of the web ad rules from count to block
B. use only rate-based rules in the web ads, and set the throttle limit as high as possible. temporarily block all requests that exceed the limit. define nested rules to narrow the scope of the rate tracking
C. set the action of the web ad rules to block use only aws managed rule groups in the web ads.evaluate the rule groups by using amazon cloudwatch metrics with aws waf sampled requests or aws waf logs.
D. use only custom rule groups in the web ads, and set the action to allow. enable aws waf logging.analyze the requests for false positives. modify the rules to avoid any false positive. over time, change the action of the web ad rules from allow to block
A. 


Mtfr: this question is about how to test web acl, count first, then block
https:/ / docs.aws.amazon.com/waf/latest/developerguide /web-ad-testing.html furthermore, a also includes both aws rules and customer rules


333, an application is using an amazon rds for mysql multi-az db instance in the us-east-1 region after a failover test, the application lost the connections to the database and could not re-establish the connections. after a restart of the application, the application re­ established the connections. a solutions architect must implement a solution so that the application can re-establish connections to the database without requiring a restart.which solution will meet these requirements?
A. create an amazon aurora mysql serverless vl db instance migrate the rds db instance to the aurora serverless vl db instance update the connection settings in the application to point to the aurora reader endpoint
B. create an rds proxy. configure the existing rds endpoint as a target update the connection settings in the application to point to the rds proxy endpoint
C. create a two-node amazon aurora mysql db cluster. migrate the rds db instance to the aurora db cluster. create an rds proxy. configure the existing rds endpoint as a target. update the connection settings in the application to point to the rds proxy endpoint
D. create an amazon s3 bucket. export the database to amazon s3 by using aws database migration service (aws dms). configure amazon athena to use the s3 bucket as a data store. install the latest open database connectivity (odbc) driver for the application update the connection settings in the application to point to the athena endpoint




334, a video processing company wants to build a machine learning (ml) model by using 600 tb of compressed data that is stored as thousands of files in the company's on-premises network attached storage system. the company does not have the necessary compute resources on premises for ml experiments and wants to use aws.the company needs to complete the data transfer to aws within 3 weeks. the data transfer will be a one- time transfer. the data must be encrypted in transit.the measured upload speed of the company's internet connection is 100 mbps, and multiple departments sharethe connection.which solution will meet these requirements most cost-effectively? 01-05-06

A. order several aws snowball edge storage optimized devices by using the aws management console. configure the devices with a destination s3 bucket. copy the data to the devices. ship the devices back to aws
B. set up a 10 gbps aws direct connect connection between the company location and the nearest aws region. transfer the data over a vpn connection into the region to store the data in amazon s3
C. create a vpn connection between the on-premises network attached storage and the nearest aws region. transfer the data over the vpn connection
D. deploy an aws storage gateway file gateway on premises. configure the file gateway with a destination s3bucket copy the data to the file gateway



335, a company wants to deploy an aws waf solution to manage aws waf rules across multiple aws accounts. the accounts are managed under different ous in aws organizations administrators must be able to add or remove accounts or ous from managed aws waf rule sets as needed. administrators also must have the ability to automatically update and remediate noncompliant aws waf rules in all accounts. which solution meets these requirements with the least amount of operational overhead?
A. use aws firewall manager to manage aws waf rules across accounts in the organization. use an aws systems manager parameter store parameter to store account numbers and ous to manage update the parameter as needed to add or remove accounts or ous. use an amazon eventbridge (amazon cloudwatch events) rule to identify any changes to the parameter and to invoke an aws lambda function to update the security policy in the firewall manager administrative account.
B. deploy an organization-wide aws config rule that requires all resources in the selected ous to associate the aws waf rules. deploy automated remediation actions by using aws lambda to fix noncompliant resources deploy aws waf rules by using an aws cloudformation stack set to target the same ous where the aws config rule is applied.
C. create aws waf rules in the management account of the organization. use aws lambda environment variables to store account numbers and ous to manage. update environment variables as needed to add or remove accounts or ous. create cross-account iam roles in member accounts.assume the roles by using aws security token service (aws sts) in the lambda function to create and update aws waf rules in the member accounts.
D. use aws control tower to manage aws waf rules across accounts in the organization use aws key management service (aws kms) to store account numbers and ous to manage. update aws kms as needed to add or remove accounts or ous. create iam users in member
A. 
accounts allow aws control tower in the management account to use the access key and secret access key to create and update aws waf rules in the member accounts



336, a solutions architect must analyze a company's amazon ec2 instances and amazon elastic block store (amazon ebs) volumes to determine whether the company is using resources efficiently. the company is running several large, high-memory ec2 instances to host database clusters that are deployed in active/ passive configurations. the utilization of these ec2instances varies by the applications that use the databases, and the company has not identified a pattern.the solutions architect must analyze the environment and take action based on the findings which solution meets these requirements most cost-effectively?
A. create a dashboard by using aws systems manager ops center. configure visualizations for amazon cloudwatch metrics that are associated with the ec2 instances and their ebs volumes. review the dashboard periodically, and identify usage patterns. rightsize the ec2 instances based on the peaks in the metrics.
B. turn on amazon cloudwatch detailed monitoring for the ec2instances and their ebs volumes. create and review adashboard that is based on the metrics. identify usage patterns. rightsize the ec2 instances based on the peaks in the metrics.
C. install the amazon cloudwatch agent on each of the ec2 instances. turn on aws compute optimizer, and let it run for at least 12 hours. review the recommendations from compute optimizer, and rightsize the ec2 instances as directed.
D. sign up for the aws enterprise support plan. tum on aws trusted advisor. wait 12 hours review the recommendations from trusted advisor, and rightsize the ec2 instances as directed.

Mi-fr: c - for sure since its memory instance need to install cw agent and to configure memory metrics - optimizer will do the work and analyze and suggest rightsizing for both ec2 ebs and ec2 instances - need to pay just for one extra metric per ec2.a is incorrect opscenter is identifying issue with resources like instance failures etc.. not for cost optimizing d is not cost effectiveb - will not fulfil the requirement - no memory data also enable detailed monitoring for all ec2 instances is expensive


337, an external audit of a company's serverless application reveals iam policies that grant too many permissions. these policies are attached to the company's aws lambda execution roles hundreds of the company's lambda functions have broad access permissions, such as

full access to amazon s3buckets and amazon dynamodb tables. the company wants each function to have only the minimum permissions that the function needs to complete its task.a solutions architect must determine which permissions each lambda function needs. what should the solutions architect do to meet this requirement with the least amount of effort?
A. set up amazon codeguru to profile the lambda functions and search for aws api calls create an inventory of the required api calls and resources for each lambda function create new iam access policies for each lambda function review the new policies to ensure that they meet the company's business requirements
B. turn on aws cloudtrail logging for the aws account. use aws identity and access management access analyzer togenerate iam access policies based on the activity recorded in the cloudtrail log review the generated policies to ensure that they meet the company's business requirements
C. turn on aws cloudrail logging for the aws account. create a script to parse the cloud trail log, search for aws apicalls by lambda execution role,and create a summary report review the report create iam access polices that provide more restrictive permissions for each lambda function
D. turn on aws cloudtrail logging for the aws account export the cloudtrail logs to amazon s3 use amazon emr toprocess the cloudtrail logs in amazon s3and produce a report of api calls and resources used by each execution role create a new iam access policy for each role export the generated roles to an s3bucket review the generated policies to ensure that they meet the company's business requirements



338, a company has an organization in aws organizations that has a large number of aws accounts. one of the aws accounts is designated as a transit account and has a transit gateway that is shared with all of the other aws accounts aws site-to-site vpn connections are configured between all of the company's global offices and the transit account. the company has aws config enabled on all of its accounts. the company's networking team needs to centrally manage a list of internal ip address ranges that belong to the global offices developers will reference this list to gain access to their applications securely. which solution meets these requirements with the least amount of operational overhead?
A. create a json file that is hosted in amazon s3and that lists all of the internal ip address ranges configure an amazon simple notification service (amazon sns) topic in each of the accounts that can be invoked when the json file is updated subscribe an aws lambda function to the sns topic to update all relevant security group rules with the updated ip address ranges
A. 
B. create anew aws config managed rule that contains all of the internal ip address ranges. use the rule to check the security groups in each of the accounts to ensure compliance with the list of ip address ranges configure the rule to automatically remediate any noncompliant security group that is detected.
C. in the transit account, create a vpc prefix list with all of the internal ip address ranges use aws resource access manager to share the prefix list with all of the other accounts. use the shared prefix list to configure security group rules in the other accounts.
D. in the transit account, create a security group with all of the internal ip address ranges configure the security groups in the other accounts to reference the transit account's security group by using a nested security group reference of "/sg- la2b3c4d"




339, a company wants to use a third-party software-as-a-service (saas) application. the third-party saas application is consumed through several api calls. the third-party saas application also runs on aws inside a vpc.the company will consume the third-party saas application from inside a vpc. the company has internal security policies that mandate the use of private connectivity that does not traverse the internet no resources that run in the company vpc are allowed to be accessed from outside the company's vpc. all permissions must conform to the principles ofleast privilege.which solution meets these requirements?
A. create an aws privatelink interface vpc endpoint. connect this endpoint to the endpoint service that the third- party saas application provides. create a security group to limit the access to the endpoint.associate the security group with the endpoint.
B. create an aws site-to-site vpn connection between the third-party saas application and the company vpc. configure network ads to limit access across the vpn tunnels.
C. create a vpc peering connection between the third-party saas application and the company vpc. update route tables by adding the needed routes for the peering connection.
D. create an aws privatelink endpoint service. ask the third-party saas provider to create an interface vpc endpoint for this endpoint service. grant permissions for the endpoint service to the specific account of the third-party saas provider.


f@Hfr:   https:/ / docs.aws.amazon.com/vpc/latest/privatelink/interface-endpoints.htmlyou can change the security groups that are associated with the network interfaces for your interface endpoint. the security group rules control the traffic that is allowed to the endpoint network interface from the resources in your vpc.

340, a company is running a web application in the aws cloud. the application consists of dynamic content that is created on a set of amazon ec2 instances. the ec2 instances run in an auto scaling group that is configured as a target group for an application load balancer (alb). the company is using an amazoncloudfront distribution to distribute the application globally. the cloudfront distribution uses the alb as an origin. the company uses amazon route 53 for dns and has created an a record of www.example.com for the cloudfront distribution. a solutions architect must configure the application so that it is highly available and fault tolerant.which solution meets these requirements?
A. provision a full, secondary application deployment in a different aws region. update the route 53arecord to be a failover record. add both of the cloudfront distributions as values. create route 53 health checks
B. provision an alb, an auto scaling group, and ec2instances in a different aws region. update the cloudfront distribution, and create a second origin for the new alb. create an origin group for the two origins. configure one origin as primary and one origin as secondary.
C. provision an auto scaling group and ec2 instances in a different aws region. create a second target for the new auto scaling group in the alb. set up the failover routing algorithm on the alb
D. provision a full, secondary application deployment in a different aws region. create a second cloudfront distribution, and add the new application setup as an origin. create an aws global accelerator accelerator.add both of the cloudfront distributions as endpoints




34L a team collects and routes behavioral data for an entire company. the company runs a multi-azvpc environment with public subnets, private subnets, and in internet gateway. each public subnet also contains a nat gateway. most of the company's applications read from and write to amazon kinesis data streams.most of the workloads run in private subnets.a solutions architect must review the infrastructure. the solutions architect needs to reduce costs and maintainthe function of the applications. the solutions architect uses cost explorer and notices that the cost in the ec2- other category is consistently high. a further review shows that natgateway-bytes charges are increasing the cost in the ec2-other category.what should the solutions architect do to meet these requirements?
A. enable vpc flow logs. use amazon athena to analyze the logs for traffic that can be removed ensure that security groups are blocking traffic that is responsible for high costs.
B. add an interface vpc endpoint for kinesis data streams to the vpc ensure that applications have the correct iam permissions to use the interface vpc endpoint.
A. 
C. enable vpc flow logs and amazon detective. review detective findings for traffic that is not related to kinesis data streams. configure security groups to block that traffic.
D. add an interface vpc endpoint for kinesis data streams to the vpc ensure that the vpc endpoint policy allows traffic from the applications.


ffllHfr: https:// docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints­ access.htmlwhen you create an interface endpoint or a gateway endpoint, you can attach an endpoint policy. the endpoint policy controls which aws principals (aws accounts, iam users, and iam roles) can use the vpc endpoint to access the endpoint service.you cannot attach more than one policy to an endpoint. however, you can modify an endpoint policy at any time.an endpoint policy does not override or replace iam user policies or service-specific policies (such as s3 bucket policies). if you're using an interface endpoint to connect to amazon s3, you can also use amazon s3 bucket policies to control access to buckets from specific endpoints or specific vpcs.


342, a company is building a software-as-a-service (saas) solution on aws. the company has deployed an amazon api gateway restapi with aws lambda integration in multiple aws regions and in the same production account.the company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of api calls per second. the premium tier offers up to 3,000 calls per second. and customers are identified by a unique api key. several premium tier customers in various regions report that they receive error responses of 429 too many requests from multiple api methods during peak usage hours. logs indicate that the lambda function is never invoked.what could be the cause of the error messages for these customers?
A. the lambda function reached its concurrency limit.
B. the lambda function reached its region limit for concurrency.
C. the company reached its api gateway account limit for calls per second.
D. the company reached its api gateway default per-method limit for calls per second




343, a retail company has anon-premises data center in europe the company also has a multi-region awspresence that includes the eu-west-1 and us-east-1 regions. the company wants to be able to route network traffic from its on-premises infrastructure into vpcs in either of those regions. the company also needs to support traffic that is routed directly between vpcs in those regions.no single points of failure can exist on the network.the

company already has created two 1 gbps aws direct connect connections from its on­ premises data center. each connection goes into a separate direct connect location in europe for high availability. these two locations are named dx-a and dx-b, respectively each region has a single aws transit gateway that is configured to route all inter-vpc traffic within that region.which solution will meet these requirements?
A. create a private vif from the dx-a connection into a direct connect gateway create a private vif from the dx-bconnection into the same direct connect gateway for high availability. associate both the eu-west-1 and us-east-1 transit gateways with the direct connect gateway peer the transit gateways with each other to support cross-region routing.
B. create a transit vif from the dx-a connection into a direct connect gateway. associate the eu-west-1 transit gateway with this direct connect gateway. create a transit mf from the dx­ b connection into a separate direct connect gateway.associate the us-east-n transit gateway with this separate direct connect gateway. peer the direct connect gateways with each other to support high availability and cross-region routing.
C. create a transit vif from the dx-a connection into a direct connect gateway. create a transit vif from the dx-bconnection into the same direct connect gateway for high availability. associate both the eu-west-1 and us-east-1 transit gateways with this direct connect gateway. configure the direct connect gateway to route traffic between the transit gateways.
D. create a transit vif from the dx-a connection into a direct connect gateway create a transit vif from the dx-bconnection into the same direct connect gateway for high availability. associate both the eu-west-1 and us-east-1 transit gateways with this direct connect gateway. peer the transit gateways with each other to support cross-region routing.

Mi-fr: a - no, there is no connection between vpcs.b - no, bcz dx gateway doesn't support routing from one vpn to another
( https:/ / docs.aws.amazon.com/directconnect/latest/userguide/ direct-connect-gateways­ intro.html )c - right answer. https://docs.aws.amazon.com/whitepapers/latest/aws-vpc­ connectivity-options/aws-direct-connect- aws-transit-gateway.htmld - no, you can not connect direct connect to the transit gateway without direct connect gateway in the middle.


344, a company has created an ou in aws organizations for each of its engineering teams. each ou owns multiple aws accounts. the organization has hundreds of aws accounts. a solutions architect must design a solution so that each ou can view a breakdown of usage costs across its aws accounts.which solution meets these requirements?
A. create an aws cost and usage report (cur) for each ou by using aws resource access manager allow each team to visualize the cur through an amazon quicksight dashboard.
A. 
B. create an aws cost and usage report (cur) from the aws organizations management account.allow each team to visualize the cur through an amazon quicksight dashboard
C. create an aws cost and usage report (cur) in each aws organizations member account allow each team to visualize the cur through an amazon quicksight dashboard
D. create an aws cost and usage report (cur) by using aws systems manager. allow each team to visualize the cur through systems manager opscenter dashboards.


fAIHfr: bis the only valid option to conclude all bills from all ous. d - is not possible how can you visualize cur with opscenter? a - ram cant create cur c - not efficient and not possible with organization since it creates one cur for all accounts


345, a company recently deployed an application on aws. the application uses amazon dynamodb. the company measured the application load and configured the rcus and wcus on the dynamodb table to match the expected peak load. the peak load occurs once a week fora 4-hour period and is double the average load. the application load is close to the average load for the rest of the week the access pattern includes many more writes to the table than reads of the table. a solutions architect needs to implement a solution to minimize the cost of the table.which solution will meet these requirements?
A. use aws application auto scaling to increase capacity during the peak period. purchase reserved rcus and wcus to match the average load.
B. configure on-demand capacity mode for the table.
C. configure dynamodb accelerator (dax) in front of the table. reduce the provisioned read capacity to match the new peak load on the table.
D. configure dynamodb accelerator (dax) in front of the table. configure on-demand capacity mode for the table.




346, a company is planning on hosting its ecommerce platform on aws using a multi-tier web application designed for a nosql database the company plans to use the us-west-2 region as its primary region the company want to ensure that copies of the application and data are available in a second region, us- west-i for disaster recovery the company wants to keep the time to fail over as low as possible. failing back to the primary region should be possible without administrative interaction after the primary service is restored.which design should the solutions architect use?

A. use aws cloudformation stacksets to create the stacks in both regions with auto scaling groups for the web and application tiers. asynchronously replicate static content between regions using amazon s3 cross-region replication use an amazon route 53 dns failover routing policy to direct users to the secondary site in us-west-i in the event of an outage use amazon dynamodb global tables for the database tier
B. use aws cloudeormation stacksets to create the stacks in both regions with auto scaling groups for the web and application tiers. asynchronously replicate static content between regions using amazon s3 cross-region replication use an amazon route 53 dns failover routing policy to direct users to the secondary site in us-west-i in the event of an outage deploy an amazon aurora global database for the database tier
C. use aws service catalog to deploy the web and application servers in both regions. asynchronously replicate static content between the two regions using amazon s3 cross­ region replication use amazon route 53 health checks to identify a primary region failure and update the public dns entry listing to the secondary region in the event of an outage use amazon rds for mysql with cross- region replication for the database tier
D. use aws cloudformation stacksets to create the stacks in both regions using auto scaling groups for the web and application tiers. asynchronously replicate static content between regions using amazon s3 cross-region replication use amazon cloudfront with static files in amazon s3, and multi- region origins for the front-end web tier use amazon dynamodb tables in each region with scheduled backups to amazon s3.



347, a company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. the device will push 8 kb of genomic data every second to a data platform that will need to process and	tfr: the data and provide information back to researchers. the data platform must meet the following requirements. provide near-real-time analytics of the inbound genomic data ensure the data is flexible, parallel, and durable deliver results of processing to a data warehousewhich strategy should a solutions architect use to meet these requirements?
A. use amazon kinesis data firehose to collect the inbound sensor data, analyze the data with kinesis clients, and save the results to an amazon rds instance.
B. use amazon kinesis data streams to collect the inbound sensor data, analyze the data with kinesis clients, and save the results to an amazon redshift cluster using amazon emr.
C. use amazon s3 to collect the inbound device data, analyze the data from amazon sqs with kinesis, and save the results to an amazon redshift cluster.
A. 
D. use an amazon api gateway to put requests into an amazon sqs queue, analyze the data with an aws lambda function,and save the results to an amazon redshift cluster using amazon emr.




348, a company hosts a legacy application that runs on an amazon ec2 instance inside a vpc without internet access. users access the application with a desktop program installed on their corporate laptops. communication between the laptops and the vpc flows through aws direct connect (dx). a new requirement states that all data in transit must be encrypted between users and the vpc. which strategy should a solutions architect use to maintain consistent network performance while meeting this new requirement?
A. create a client vpn endpoint and configure the laptops to use an aws client vpn to connect to the vpc over the internet.
B. create a new public virtual interface for the existing dx connection, and create a new vpn that connects to the vpc over the dx public virtual interface.
C. create a new site-to-site vpn that connects to the vpc over the internet.
D. create a new private virtual interface for the existing dx connection, and create a new vpn that connects to the vpc over the dx private virtual interface.


 It-tfr: d>bhttps:/ / aws.amazon.com/premiumsupport/knowledge-center/ create-vpn­ direct-connectj.remember that to connect to services such as ec2 using just direct connect you need to create a private vif.however if you want to encrypt the traffic flowing through directconnect, you will need to use the public vif of dx to create a vpn connection that will allow access to aws services such as s3, ec2 etc. the video describes this.


349, a company has an application that generates reports and stores them in an amazon s3 bucket. when a user accesses their report, the application generates a signed url to allow the user to download the report. the company's security team has discovered that the files are public and that anyone can download them without authentication the company has suspended the generation of new reports until the problem is resolved.which set of actions will immediately remediate the security issue without impacting the application's normal workflow?
A. create an aws lambda function that applies a deny all policy for users who are not authenticated.create a scheduled event to invoke the lambda function.
A. 
B. review the aws trusted advisor bucket permissions check and implement the recommended actions.
C. run a script that puts a private ad on all of the objects in the bucket.
D. use the block public access feature in amazon s3 to set the ignorepublicacls option to true on the bucket.


 tfr: modify on b->dremember that the purpose of creating a pre-signed url is to allows unauthenticated users access to the bucket or the objects in the bucket which are private. so if someone can still access the bucket then the buckets or the objects in the bucket have been granted a public ad which needs to be blocked and the way to do that is by using the ignorepublicacls setting.


350, a european online newspaper service hosts its public-facing wordpress site in a colocated data center in london. the currentwordpress infrastructure consists of a load balancer; two web servers, and one mysql database server. a solutions architect is tasked with designing a solution with the following requirements:improve the website s performance. make the web tier scalable and stateless.improve the database server performance for read-heavy loads. reduce latency for users across europe and the us.design the new architecture with a goal of 99.9% availability. which solution meets these requirements while optimizing operational efficiency?
A. use an application load balancer (alb) in front of an auto scaling group of word press amazon ec2 instances in one aws region and three availability zones.configure an amazon elasticache cluster in front of multi-az amazon aurora mysql db cluster. move the wordpress shared files to amazon efs.configure amazon cloudfront with the alb as the origin, and select a price class that includes the us and europe.
B. use an application load balancer (alb) in front of an auto scaling group of wordpress amazon ec2 instances in two aws regions and two availability zones in each region.configure an amazon elasticache cluster in front of a global amazon aurora mysql database. move the wordpress shared files to amazon efs.configure amazon cloudfront with the alb as the origin, and select a price class that includes the us and europe. configure efs cross region replication.
C. use an application load balancer (alb) in front of an auto scaling group of word press amazon ec2 instances in one aws region and three availability zones.configure an amazon documentdb table in front of a multi-az amazon aurora mysql db cluster. move the wordpress shared files to amazon efs.configure amazon cloudfront with the alb as the origin, and select a price class that includes all global locations.
A. 
D. use an application load balancer (alb) in front of an auto scaling group of wordpress amazon ec2 instances in two aws regions and three availability zones in each region.configure an amazon elasticache cluster in front of a global amazon aurora mysql database. move the wordpress shared files to amazon fsx with cross-region synchronization.configure amazon cloudfront with the alb as the origin and a price class that includes the us and europe.

Mi-fr: band dare eliminated since auto scaling spans across multiple availability zones within the same region but cannot span across regions. c doesn't make sense by documentdb infront of another database.


35L a company is using aws cloudformation as its deployment tool for all applications. it stages all application binaries and templates within amazon s3 buckets with versioning enabled. developers have access to an amazon ec2 instance that hosts the integrated development environment (ide). the developers download the application binaries from amazon s3 to the ec2 instance, make changes, and upload the binaries to an s3 bucket after running the unit tests locally. the developers want to improve the existing deployment mechanism and implement ci/cd using aws codepipeline.the developers have the following requirements. use aws codecommit for source control automate unit testing and security scanning.alert the developers when unit tests fail.turn application features on and off. and customize deployment dynamically as part of ci/cd. have the lead developer provide approval before deploying an application.which solution will meet these requirements?
A. use aws codebuild to run unit tests and security scans. use an amazon eventbridge rule to send amazon sns alerts to the developers when unit tests fail. write aws cloud development kit (aws cdk) constructs for different solution features, and use a manifest file to turn features on and off in the aws cdk application: use a manual approval stage in the pipeline to allow the lead developer to approve applications.
B. use aws lambda to run unit tests and security scans. use lambda in a subsequent stage in the pipeline to send amazon sns alerts to the developers when unit tests fail. write aws amplify plugins for different solution features and utilize user prompts to turn features on and off. use amazon ses in the pipeline to allow the lead developer to approve applications.
C. use jenkins to run unit tests and security scans use an amazon eventbridge rule in the pipeline to send amazon ses alerts to the developers when unit tests fail. use aws cloudformation nested stacks for different solution features and parameters to turn features on and off. use aws lambda in the pipeline to allow the lead developer to approve applications.
D. use aws codedeploy to run unit tests and security scans. use an amazon cloudwatch alarm in the pipeline to sendamazon sns alerts to the developers when unit tests fail. use docker
A. 
images for different solution features and the aws di to turn features on and off use a manual approval stage in the pipeline to allow the lead developer to approve applications.



352, a large company recently experienced an unexpected increase in amazon rds and amazon dynamodb costs. the company needs to increase visibility into details of aws billing and cost management. there are various accounts associated with aws organizations,including many development and production accounts. there is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using aws cloudformation with consistent tagging. management requires cost center numbers and project id numbers for all existing and future dynamodb tables and rds instances. which strategy should the solutions architect provide to meet these requirements?
A. use tag editor to tag existing resources. create cost allocation tags to define the cost center and project id and allow 24 hours for tags to propagate to existing resources.
B. use an aws config rule to alert the finance team of untagged resources. create a centralized aws lambda based solution to tag untagged rds databases and dynamodb resources every hour using a cross-account role.
C. use tag editor to tag existing resources. create cost allocation tags to define the cost center and project id. use scps to restrict resource creation that do not have the cost center and project id on the resource.
D. create cost allocation tags to define the cost center and project id and allow 24 hours for tags to propagate to existing resources. update existing federated roles to restrict privileges to provision resources that do not include the cost center and project id on the resource.



353, an ecommerce website running on aws uses an amazon rds for mysql db instance with general purpose ssd storage. the developers chose an appropriate instance type based on demand, and configured 100 gb of storage with a sufficient amount of free space . the website was running smoothly for a few weeks until a marketing campaign launched. on the second day of the campaign, users reported long wait times and time outs. amazon cloudwatch metrics indicated that both reads and writes to the db instance were experiencing long response times. the cloudwatch metrics show 40% to 50% cpu and memory utilization, and sufficient free storage space is still available. the application server logs show no evidence of database connectivity issues. what could be the root cause of the issue with the marketing campaign?

A. it exhausted the 1/o credit balance due to provisioning low disk storage during the setup phase.
B. it caused the data in the tables to change frequently, requiring indexes to be rebuilt to optimize queries.
C. it exhausted the maximum number of allowed connections to the database instance.
D. it exhausted the network bandwidth available to the rds for mysql db instance.




354, a north american company with headquarters on the east coast is deploying a new web application running on amazon ec2 in the us-east-1 region. the application should dynamically scale to meet user demand and maintain resiliency. additionally, the application must have disaster recovery capabilities in an active- passive configuration with the us-west-1 region.which steps should a solutions architect take after creating a vpc in the us-east-1 region?
A. create a vpc in the us-west-1 region use inter. region vpc peering to connect both vpcs. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us-east-1 region. deploy ec2 instances across multiple azs in each region as part of an auto scaling group spanning both vpcs and served by the alb.
B. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us- east- 1 region.deploy ec2 instances across multiple azs as part of an auto scaling group served by the alb deploy the same solution to the us-west-1 region. create an amazon route 53 record set with a failover routing policy and health checks enabled to provide high availability across both regions.
C. create a vpc in the us-west-1 region. use inter-region vpc peering to connect both vpcs. deploy an application load balancer (alb) that spans both vpcs. deploy ec2 instances across multiple availability zones as part of an auto scaling group in each vpc served by the alb. create an amazon route 53 record that points to the alb.
D. deploy an application load balancer (alb) spanning multiple availability zones (azs) to the vpc in the us east-1 region.deploy ec2 instances across multiple azs as part of an auto scaling group served by the alb. deploy the same solution to the us-west-1 region. create separate amazon route 53 records in each region that point to the alb in the region. use route 53 health checks to provide high availability across both regions.


 tfr: keywords active-passive failover for dr. we dont need albs to span across vpcs using peering.
 
355, a company has developed a new release of a popular video game and wants to make it available for public download the new release package is approximately 5 gb in size.the company provides downloads for existing releases from a linux based, publicly facing ftpsite hosted in an on-premises data center. the company expects the new release will be downloaded by users worldwide. the company wants a solution that provides improved download performance and low transfer costs,regardless of a user's location .which solutions will meet these requirements?
A. store the game files on amazon ebs volumes mounted on amazon ec2 instances within an auto scaling group. configure an ftp service on the ec2 instances. use an application load balancer in front of the auto scaling group. publish the game download url for users to download the package.
B. store the game files on amazon efs volumes that are attached to amazon ec2 instances within an auto scaling group.configure an ftp service on each of the ec2 instances. use an application load balancer in front of the auto scaling group. publish the game download url for users to download the package.
C. configure amazon route 53 and an amazon s3 bucket for website hosting.upload the game files to the s3 bucket. use amazon cloudfront for the website. publish the game download url for users to download the package.
D. configure amazon route 53 and an amazon s3 bucket for website hosting.upload the game files to the s3 bucket set requester pays for the s3 bucket. publish the game download url for users to download the package.




356, a new startup is running a serverless application using aws lambda as the primary source of compute new versions of the application must be made available to a subset of users before deploying changes to all users. developers should also have the ability to abort the deployment and have access to an easy rollback mechanism. a solutions architect decides to use aws codedeploy to deploy changes when a new version is available.which codedeploy configuration should the solutions architect use?
A.a blue/green deployment
B.a linear deployment
C.a canary deployment
D.an all-at once deployment





357, a company built an application based on aws lambda deployed in an aws cloudformation stack. the last production release of the web application introduced an issue that resulted in an outage lasting several minutes. a solutions architect must adjust the deployment process to support a canary release.which solution will meet these requirements?
A.create an alias for every new deployed version of the lambda function. use the aws di update-alias command with the routing-config parameter to distribute the load.
B.deploy the application into a new cloudformation stack. use an amazon route 53 weighted routing policy to distribute the load.
C.create a version for every new deployed lambda function. use the aws cli update-function
- configuration command with the routing-config parameter to distribute the load.
D.configure aws codedeploy and use codedeploydefault oneatatime in the deployment configuration to distribute the load.




358, a large company will be migrating to aws the company has 20 business units and anticipates another 10 coming online in the future.each business unit will need its own ip range and will operate in its own aws account. there will be a lot of communication between business units with very large data transfers.the company wants to make sure that the proposed solution will minimize data transfer costs and reduce complexity.how should a solutions architect design the network to meet these requirements?
A.create a transit vpc in a networking account. within each business unit's aws account, create redundant vpn connections to the transit vpc.
B.create a transit gateway in a networking account. share the transit gateway with each business unit's aws account. attach the vpc in each account to the transit gateway.
C.create two subnets for each business unit in a networking account share the subnets with each business unit's aws account using aws resource access manager.
D.create a vpc for each business unit's aws account. use vpc peering to route traffic between the vpcs in each account.


359, a company has a web application that uses amazon api gateway, aws lambda, and amazon dynamodb. a recent marketing campaign has increased demand. monitoring software reports that many requests have significantly longer response times than before the marketing campaign. a solutions architect enabled amazon cloudwatch logs for api gateway and noticed that errors are occurring on 20% of the requests.in cloudwatch, the lambda function throttles metric represents 1% of the requests and the errors metric represents 10% of the requests. application logs indicate that, when errors occur, there is a call to dynamodb.what change should the solutions architect make to improve the current response times as the web application becomes more popular?
A.increase the concurrency limit of the lambda function. B.implement dynamodb auto scaling on the table. (.increase the api gateway throttle limit.
D.re-create the dynamodb table with a better-partitioned primary index.




360, a company has many services running in its on-premises data center. the data center is connected to aws using aws direct connect (dx) and an ipsec vpn the service data is sensitive and connectivity cannot traverse the internet the company wants to expand into a new market segment and begin offering its services to other companies that are using aws.which solution will meet these requirements?
A.create a vpc endpoint service that accepts tcp traffic, host it behind a network load balancer. and make the service available over dx.
8.create a vpc endpoint service that accepts http or https traffic, host it behind an application load balancer, and make the service available over dx.
C.attach an internet gateway to the vpc, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.
D.attach a nat gateway to the vpc, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic.




36L	a company is running several large workloads on amazon ec2instances each ec2 instance has multiple amazon elastic block store (amazon ebs) volumes attached to it once each day,an aws lambda function invokes the creation of ebs volume snapshots these

snapshots accumulate until an administrator manually purges them.the company must maintain backups for a minimum of 30 days. a solutions architect needs to reduce the costs of this process.which solution meets these requirements most cost-effectively?
A.search aws marketplace. find a third-party solution to deploy to automatically manage the ebs volume backups.
B.create a second lambda function to move the ebs snapshots that are older than 30 days to amazon s3 glacier deep archive.
C.set an amazon s3 lifecycle policy on the s3 bucket that contains the snapshots. create a rule with an expiration action to delete ebs snapshots that are older than 30 days
D.migrate the backup functionality to amazon data lifecycle manager (amazon dim). create a lifecycle policy for the daily backup of the ebs volumes. set the retention period for the ebs snapshots to 30 days.




362, a company is building a software-as-a-service (saas) solution on aws. the company has deployed an amazon api gateway restapi with aws lambda integration in multiple aws regions and in the same production account.the company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of api calls per second. the premium tier offers up to 3,000 calls per second. and customers are identified by a unique api key. several premium tier customers in various regions report that they receive error responses of 429 too many requests from multiple api methods during peak usage hours. logs indicate that the lambda function is never invoked.what could be the cause of the error messages for these customers?
A.the lambda function reached its concurrency limit.
B.the lambda function reached its region limit for concurrency.
C.the company reached its api gateway account limit for calls per second.
D.the company reached its api gateway default per-method limit for calls per second




363, a company has an organization in aws organizations. the organization consists of a large number of aws accounts that belong to separate business units. the company requires all amazon ec2 instances to be provisioned with custom, hardened amls. the company wants

a solution that provides each aws account access to the amis.which solution will meet these requirements with the most operational efficiency?
A.create the amls with ec2lmage builder. create an aws codepipeline pipeline to share the amis across all aws accounts.
B.deploy jenkins on an ec2 instance. create jobs to create and share the amls across all aws accounts
C.create and share the amls with ec2 image builder use aws service catalog to configure a product that provides access to the amis across all aws accounts
D.create the amis with ec2 image builder. create an aws lambda function to share the amls across all aws accounts



364, a company in the united states (us) has acquired a company in europe. both companies use the aws cloud. the us company has built anew application with a microservices architecture. the us company is hosting the application across five vpcs in the us-east-2 region. the application must be able to access resources in one vpc in the eu-west- 1 region. however, the application must not be able to access any other vpcs.the vpcs in both regions have no overlapping cidr ranges all accounts are already consolidated in one organization in aws organizations.which solution will meet these requirements most cost­ effectively?
A.create one transit gateway in eu-west-1. attach the vpcs in us-east-2 and the vpcin eu­ west-lto the transit gateway. create the necessary route entries in each vpc so that the traffic is routed through the transit gateway.
B.create one transit gateway in each region. attach the involved subnets to the regional transit gateway.create the necessary route entries in the associated route tables for each subnet so that the traffic is routed through the regional transit gateway. peer the two transit gateways.
C.create a full mesh vpc peering connection configuration between all the vpcs. create the necessary route entries in each vpc so that the traffic is routed through the vpc peering connection.
D.create one vpc peering connection for each vpc in us-east-2 to the vpc ineu-west-1. create the necessary route entries in each vpc so that the traffic is routed through the vpc peering connection


365, a hybrid network architecture must be used during a company's multi-year data center migration from multiple private data centers to aws. the current data centers are linked together with private fiber. due to unique legacy applications, network address translation (nat) cannot be used. during the migration period,many applications will need access to other applications in both the data centers and aws. which option offers a hybrid network architecture that is secure and highly available. that allows for consistent high bandwidth and a multi-reqion deployment post-migration?
A.use an aws direct connect connection to each data center from different providers, and configure routing to failover to the other data center's direct connect connection if one fails. ensure that no vpc cidr blocks overlap one another or the on-premises network
B.use multiple hardware vpn connections to aws from the on-premises data center. route different subnet traffic through different vpn connections. ensure that no vpc cidr blocks overlap one another or the on-premises network
C.use a software vpn with clustering both in aws and the on-premises data center, and route traffic through the cluster. ensure that no vpc cidr blocks overlap one another or the on­ premises network
D.use a single aws direct connect connection and a vpn as backup, and configure both to use the same virtual private gateway and border gateway protocol (bgp).ensure that no vpc cidr blocks overlap one another or the on- premises network




366, a company is developing a new on-demand video application that is based on microservices. the application will have 5 million users at launch and will have 30 million users after 6 months. the company has deployed the application on amazon elastic container service (amazon ecs) on aws fargate the company developed the application by using ecs services that use the https protocol. a solutions architect needs to implement updates to the application by using blue/green deployments. the solution must distribute traffic to each ecs service through a load balancer. the application must automatically adjust the number of tasks in response to an amazon cloudwatch alarm.which solution will meet these requirements?
A.configure the ecs services to use the blue/green deployment type and a network load balancer.request increases to the service quota for tasks per service to meet the demand.
B.configure the ecs services to use the blue/green deployment type and a network load balancer.implement an auto scaling group for each ecs service by using the cluster autoscaler.

C.configure the ecs services to use the blue/green deployment type and an application load balancer.implement an auto scaling group for each ecs service by using the cluster autoscaler.
D.configure the ecs services to use the blue/green deployment type and an application load balancer.implement service auto scaling for each ecs service.



367, a web application is hosted in a dedicated vpc that is connected to a company's on­ premises data center over a site-to-site vpn connection. the application is accessible from the company network only. this is a temporary non-production application that is used during business hours. the workload is generally low with occasional surges.the application has an amazon aurora mysql provisioned database cluster on the backend. the vpc has an internet gateway and a nat gatewaysattached. the web servers are in private subnets in an auto scaling group behind an elastic load balancer. the web servers also upload data to an amazon s3 bucket through the internet. a solutions architect needs to reduce operational costs and simplify the architecture which strategy should the solutions architect use?
A.review the auto scaling group settings and ensure the scheduled actions are specified to operate the amazon ec2 instances during business hours only. use 3-year scheduled reserved instances for the web server ec2 instances. detach the internet gateway and remove the nat gateways from the vpc.use an aurora serverless database and set up a vpc endpoint for the s3 bucket
B.review the auto scaling group settings and ensure the scheduled actions are specified to operate the amazon ec2 instances during business hours only. detach the internet gateway and remove the nat gateways from the vpc. use an aurora serverless database and set up a vpc endpoint for the s3 bucket., then update the network routing and security rules and policies related to the changes
C.review the auto scaling group settings and ensure the scheduled actions are specified to operate the amazon ec2instances during business hours only. detach the internet gateway from the vpc, and use an aurora serverless database. set up a vpc endpoint for the s3 bucket, then update the network routing and security rules and policies related to the changes
D.use 3-year scheduled reserved instances for the web server amazon ec2 instances. remove the nat gateways from the vpc, and set up a vpc


368, a company owns a chain of travel agencies and is running an application in the aws cloud. company employees use the application to search for information about travel destinations. destination content is updated four times each year.two fixed amazon ec2 instances serve the application. the company uses an amazon route 53public hosted zone with a multivalue record of travel.example.com that returns the elastic ip addresses for the ec2 instances. the application uses amazon dynamodb as its primary data store. the company uses a self-hosted redis instance as a caching solution.during content updates, the load on the ec2 instances and the caching solution increases drastically. thisincreased load has led to downtime on several occasions. a solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.which solution will meet these requirements?
A.set up dynamodb accelerator (dax) as in-memory cache. update the application to use dax. create an auto scaling group for the ec2 instances. create an application load balancer (alb). set the auto scaling group as a target for the alb. update the route 53recordto use a simple routing policy that targets the alb's dns alias. configure scheduled scaling for the ec2 instances before the content updates.
B.set up amazon elasticache for redis. update the application to use elasticache create an auto scaling group for the ec2 instances. create an amazon cloudfront distribution, and set the auto scaling group as an origin for the distribution. update the route 53 record to use a simple routing policy that targets the cloudfront distribution's dns alias. manually scale up ec2instances before the content updates.
C.set up amazon elasticache for memcached. update the application to use elasticache. create an auto scaling group for the ec2 instances. create an application load balancer (alb). set the auto scaling group as a target for the alb. update the route 53 record to use a simple routing policy that targets the alb's dns alias. configure scheduled scaling for the application before the content updates
D.set up dynamodb accelerator (dax) as in-memory cache. update the application to use dax. create an auto scaling group for the ec2instances. create an amazon cloudfront distribution, and set the auto scaling group as an origin for the distribution. update the route 53record to use a simple routing policy that targets the cloudfront distribution's dns alias. manually scale up ec2 instances before the content updates.




369, a health insurance company stores personally identifiable information (pii) in an amazon s3 bucket. the company uses server-side encryption with s3 managed encryption keys (sse-s3) to encrypt the objects.according to a new requirement, all current and futureobjects in the s3 bucket must be encrypted by keys that the company's security team

manages. the s3 bucket does not have versioning enabled.which solution will meet these requirements?
A.in the s3bucket properties, change the default encryption to sse-s3 with a customer managed key.use the aws di to re-upload all objects in the s3 bucket. set an s3 bucket policy to deny unencrypted putobject requests
B.in the s3bucket properties, change the default encryption to server-side encryption with aws kms managed encryption keys (sse-kms). set an s3 bucket policy to deny unencrypted putobject requests. use the aws di to re- upload all objects in the s3 bucket
C.in the s3bucket properties, change the default encryption to server-side encryption with aws kms managed encryption keys (sse-kms).set an s3 bucket policy to automatically encrypt objects on getobject and putobject requests
D.in the s3bucket properties, change the default encryption to aes-256 with a customer managed key.attach a policy to deny unencrypted putobject requests to any entities that access the s3 bucket. use the aws di to re-upload all objects inthe s3 bucket.




370, a company wants to expand its use of the aws cloud. the company has only a few aws accounts but will add many more accounts soon.a solutions architect is reviewing the company's aws governance policies. all aws accounts need to automatically inherit the same baseline configurations, such as the activation of aws single sign-on and the deployment of a common aws cloudtrail configuration.the company's security team is concerned that some of these baseline configurations might be turned off or removed after an account is created. any new solution must enforce the company's governance policies on the aws accounts.which solution meets these requirements with the least operational overhead?
A.use aws organizations to create scps that manage the permissions of aws accounts and prevent unwanted changes. create an aws cloudformation template that implements the required baseline configurations. launch the cloudformation template as a stack set from the management account.
B.use aws organizations to create scps that manage the permissions of aws accounts and prevent unwanted changes. create scp templates that implement the baseline configuration that all aws accounts should follow.
(.implement aws control tower. create a configurable account template, and automate the provisioning of new aws accounts. implement preventive and detective guardrails. import the existing aws accounts.
D.create an aws cloudformation template that implements the required baseline configurations. apply the cloudformation template to all aws accounts. use a cl/cd pipeline

to run the cloudformation template daily to detect and prevent changes to the baseline account configurations.



371, a company has set up its entire infrastructure on aws. the company uses amazonec2instances to host its ecommerce website and uses amazon s3 to store static data. three engineers at the company handle the cloud administration and development through one aws account. occasionally an engineer alters an ec2 security group configuration of another engineer and causes noncompliance issues in the environment.a solutions architect must set up a system that tracks changes that the engineers make. the system must send alerts when the engineers make noncompliant changes to the security settings for the ec2instances. what is the fastest way for the solutions architect to meet these requirements?
A.set up aws organizations for the company. apply scps to govern and track noncompiant security group changes that are made to the aws account
B.enable aws cloudtrail to capture the changes to ec2 security groups enable amazon cloudwatch rules to provide alerts when noncompliant security settings are detected
C.enable scps on the aws account to provide alerts when noncompliant security group changes are made to the environment
D.enable aws config on the ec2 security groups to track any noncompliant changes send the changes as alerts through an amazon simple notification service (amazon sns) topic



372, a gaming company created a game leaderboard by using a muti-az deployment of an amazon rds database. the number of users is growing, and the queries to get individual player rankings are getting slower over time. the company expects a surge in users for an upcoming version and wants to optimize the design for scalability and performance.which solution will meet these requirements?
A.migrate the database to amazon dynamodb. store the leaderboard data in two different tables. use apache hiveql join statements to build the leaderboard.
B.keep the leaderboard data in the rds db instance. provision a multi-az deployment of an amazon elasticache for redis cluster.
C.stream the leaderboard data by using amazon kinesis data firehose with an amazon s3 bucket as the destination. query the s3 bucket by using amazon athena for the leaderboard.

D.add a read-only replica to the rds db instance. add an rds proxy database proxy



373, to abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public aws regions, including in the united states, where the company's headquarters is located. the solutions architect is required to provide access to the data stored in aws to the company's global wan network. the security team mandates that no traffic accessing this data should traverse the public internet.how should the solutions architect design a highly available solution that meets the requirements and is cost- effective?
A.establish aws direct connect connections from the company headquarters to all aws regions in use.use the company wan to send traffic over to the headquarters and then to the respective dx connection to access the data.
B.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use inter-region vpc peering to access the data in other aws regions.
C.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use an aws transit vpc solution to access data in other aws regions.
D.establish two aws direct connect connections from the company headquarters to an aws region.use the company wan to send traffic over a dx connection. use direct connect gateway to access data in other aws regions.



374, a financial services company in north america plans to release anew online web application to its customers on aws. the company will launch the application in the us-east- 1 region on amazon ec2 instances. the application must be highly available and must dynamically scale to meet user traffic. the company also wants to implement a disaster recovery environment for the application in the us-west-1 region by using active- passive failover.which solution will meet these requirements?
A.create a vpc in us-east-1 and a vpc in us-west-1. configure vpc peering. in the us-east-1 vpc. create an application load balancer (alb) that extends across multiple availability zones in both vpcs.create an auto scaling group that deploys the ec2 instances across the multiple availability zones in both vpcs. placethe auto scaling group behind the alb.

B.create a vpc in us-east-1 and a vpc in us-west-1. in the us-east-1vpc, create an application load balancer (alb) that extends across multiple availability zones in that vpc. create an auto scaling group that deploys the ec2 instances across the multiple availability zones in the us­ east-1vpc. place the auto scaling group behind the alb. set up the same configuration in the us-west-1vpc. create an amazon route 53 hosted zone. create separate records for each alb enable health checks to ensure high availability between regions.
C.create a vpc in us-east-1 and a vpc in us-west-1.in the us-east-1vpc, create an application load balancer (alb) that extends across multiple availability zones in that vpc. create an auto scaling group that deploys the ec2 instances across the multiple availability zones in the us­ east-1vpc. place the auto scaling group behind the alb. set up the same configuration in the us-west-1 vpc. create an amazon route 53 hosted zone. create separate records for each alb. enable health checks and configure a failover routing policy for each record.
D.create a vpc in us-east-1 and a vpc in us-west-1. configure vpc peering. in the us-east-lvpc, create an application load balancer (alb) that extends across multiple availability zones in both vpcs.create an auto scaling group that deploys the ec2 instances across the multiple availability zones in both vpcs place the auto scaling group behind the alb. create an amazon route 53 hosted zone.create a record for the alb.




375, a company is running a log processing application on an amazon elastic container service (amazon ecs) cluster on amazon ec2 instances. the instances use a 40 gb general purpose ssd (gp3) amazon elastic block store (amazon ebs) root volume. the application downloads data from an amazon s3 bucket in the form of zip flies, which can vary in size by up to 5 gb. the application then extracts the data from the zip files, runs log analysis routines, and uploads the results to another s3 bucket. the application initially works as expected.however, in 1-2hours, log processing output slows after a scale out to the cluster's max capacity of 60 instances all instances are still performing log processing tasks. initial analysis shows neither a sudden increase in cpu utilization nor a memory-related issue. the application itself is not producing error messages.what should a solutions architect recommend to resolve this issue with the least amount of code change?
A.increase the size of the ec2 instances. use compute optimized instances.
B.increase the iops of the root volume to fulfill the requirements of the log processing application.
C.create a 100 gb provisioned iops ssd (io2) ebs volume with 3,000 provisioned iops. reconfigure the application to use the new volume as a log processing temporary area
D.change the ecs cluster so that it uses aws fargate





376, a public-facing web application queries a database hosted on a amazon ec2 instance in a private subnet. a large number of queries involve multiple table joins, and the application performance has been degrading due to an increase in complex queries. the application team will be performing updates to improve performance.what should a solutions architect recommend to the application team? (select two.)
A. cache query data in amazon sqs
B. create a read replica to offload queries
C. migrate the database to amazon athena
D. implement amazon dynamodb accelerator to cache data. E.migrate the database to amazon rds

Mtfr: rds with read replica should do the the job.



377, a solutions architect is implementing a document review application using an amazon s3 bucket for storage.the solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available.users must be able to download, modify, and upload documents. which combination of actions should be taken to meet these requirements'? (select two)
A. enable a read-only bucket acl
B. enable versioning on the bucket
C. attach an iam policy to the bucket
D. enable mfa delete on the bucket
E. encrypt the bucket using aws kms


 tfr: none of the options present a good solution for specifying permissions required to write and modify objects so that requirement needs to be taken care of separately. the other requirements are to prevent accidental deletion and the ensure that all versions of the document are available. the two solutions for these requirements are versioning and mfa
  
delete. versioning will retain a copy of each version of the document and multi-factor authentication delete (mfa delete) will prevent any accidental deletion as you need to supply a second factor when attempting a delete. correct: "enable versioning on the bucket" is a correct answer. correct: "enable mfa delete on the bucket" is also a correct answer. incorrect: "set read- only permissions on the bucket" is incorrect as this will also prevent any writing to the bucket which is not desired.incorrect: "attach an iam policy to the bucket" is incorrect as users need to modify documents which will also allow delete. therefore, a method must be implemented to just control deletes. incorrect:"encrypt the bucket using aws sse-s3" is incorrect as encryption doesn't stop you from deleting an object. references:https:/ / docs.aws.amazon.com/amazons3/latest/dev/versioning.html https://docs.aws.amazon.com/ amazons3/latest/dev/usingmfadelete.html save time with our exam-specific cheat sheets:https:/ / digitalcloud.training/certification-training/aws­ solutions-architect- associate/storage/amazon-s3/


378, a company wants to improve the availability of an existing firewall. to meet the compliance requirements of the applications hosted in the vpc. the company's security team is using a proprietary firewall running on amazon ec2 instances. all internet traffic flows through the primary firewall. when the primary firewall goes down, the team manually changes the vpc route table so that it uses a secondary firewall running in a different availability zone. which strategies should a solutions architect use to improve the availability of the firewall? (select two.)
A. create an ec2 gateway endpoint in the vpc where the firewall is hosted.
B. create an ec2 interface endpoint in the vpc where the firewall is hosted.
C. enable enhanced networking on the ec2 instance running the proprietary firewall
D. deploy a scheduled aws lambda function in the vpc to monitor the primary firewall and change the route table to use the secondary firewall in case of failure.
E. monitor the firewall instance health in amazon eventbridge (amazon cloudwatch events).trigger an event rule to restart the primary firewall upon a detected failure.




379, a solutions architect is moving the static content from a public website hosted on amazon ec2 instances to an amazon s3 bucket. an amazon cloudfront distribution will be used to deliver the static assets. the security group used by the ec2 instances restricts access to a limited set of ip ranges. access to the static content should be similarly restricted. which combination of steps will meet these requirements? (select two.)

A. create an origin access identity (oai) and associate it with the distribution. change the permissions in the bucket policy so that only the oai can read the objects.
B. create an aws waf web ad that includes the same ip restrictions that exist in the ec2 security group. associate this new web ad with the cloudfront distribution.
C. create a new security group that includes the same ip restrictions that exist in the current ec2 security group. associate this new security group with the cloudfront distribution.
D. create a new security group that includes the same ip restrictions that exist in the current ec2 security group.associate this new security group with the s3 bucket hosting the static content.
E. create a new iam role and associate the role with the distribution. change the permissions either on the s3 bucket or on the files within the s3 bucket so that only the newly created iam role has read and download permissions.




380, a solutions architect needs to design a low-latency solution for a static single-page application accessed by users utilizing a custom domain name. the solution must be serverless, encrypted in transit, and cost- effective.which combination of aws services and features should the solutions architect use? (select two.)
A. amazon s3
B. amazon ec2
C. aws fargate
D. amazon cloudfront
E. elastic load balancer


Mtfr: s3, cloud front - serverlesss3- static website with less costcloud front - low latency with less cost.


381, a solutions architect is designing a two-tier web application. the application consists of a public-facing web tier hosted on amazon ec2 in public subnets. the database tier consists of microsoft sql server running on amazon ec2 in a private subnet security is a high priority for the company. how should security groups be configured in this situation? (select two)

A. configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/70
B. configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0
C. configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier
D. configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier
E. configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier

Mi-fr: in this scenario an inbound rule is required to allow traffic from any internet client to the web front end on ssl/tls port 443. the source should therefore be set to 0.0.0.0/0 to allow any inbound traffic. to secure the connection from the web frontend to the database tier, an outbound rule should be created from the public ec2 security group with a destination of the private ec2 security group. the port should be set to 1433 for mysql. the private ec2 security group will also need to allow inbound traffic on 1433 from the public ec2 security group.this configuration can be seen in the diagram:correct: "configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0" is a correct answer.correct: "configure the security group for the database tier to allow inbound traffic on port 1433 from the security group for the web tier" is also a correct answer. incorrect: "configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0" is incorrect as this is configured backwards. incorrect: "configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to the security group for the web tier" is incorrect as the mysql database instance does not need to send outbound traffic on either of these ports.incorrect: "configure the security group for the database tier to allow inbound traffic on ports 443 and 1433 from the security group for the web tier" is incorrect as the database tier does not need to allow inbound traffic on port 443.references:https://docs.aws.amazon.com/vpc/latest/userguide/vpc_securitygroups.ht   ml save time with our exam- specific cheat
sheets:https:/  /  digitalcloud.training/certification-training/aws-solutions-architect­ associate/networking-and- content- delivery/amazon-vpc/


382, a company has several business systems that require access to data stored in a file share. the business systems will access the file share using the server message block (smb) protocol. the file share solution should be accessible from both of the company's legacy on­ premises environment and with aws. which services mod the business requirements? (select two.)

A. amazon ebs
B. amazon efs
C. amazon fsx for windows
D. amazon s3
E. aws storage gateway file gateway

Mi-fr: keyword: smb + on-premisescondition: file accessible from both on-premises and aws amazon fsx for windows file serveramazon fsx for windows file server provides fully managed, highly reliable, and scalable file storage that is accessible over the industry­ standard server message block (smb) protocol. it is built on windows server, delivering a wide range of administrative features such as user quotas, end-user file restore, and microsoft active directory (ad) integration. it offers single-az and multi- az deployment options, fully managed backups, and encryption of data at rest and in transit. you can optimize cost and performance for your workload needs with ssd and hdd storage options; and you can scale storage and change the throughput performance of your file system at any time. amazon fsx file storage is accessible from windows, linux, and macos compute instances and devices running on aws or on premises.how fsx for windows file server workscorrect: "amazon fsx for windows" is the correct answer. correct: "amazon storage file gateway" is the correct answer.incorrect: "amazon ebs" is incorrect as unsupported nfs/smb.incorrect: "amazon efs" is incorrect as unsupported nfs/smb.incorrect: "amazon s3" is incorrect as unsupported nfs/smb.references:https://aws.amazon.com/fsx/windows/https://aws.amazon.com/stor agegateway/?whats-new-cards.sort- by=item.additionalfields.postdatetime&whats-new­ cards.sort-order=desc https:/ /aws.amazon.com/blogs/ aws/file-interface-to-aws-storage­ gateway/ https:/ /dO.awsstatic.com/whitepapers/aws-storage-gateway-file- gateway-for­ hybrid- architectures.pdfhttps://youtu.be/t5klnnj7-qgsave time with our exam-specific cheat sheets:https:/ / digitalcloud.training/certification-training/ aws-solutions-architect­ associate /storage/amazon-fsx/ https:/ / digitalcloud.training/certification-training/aws­ solutions-architect-associate/storage/aws- storage- gateway/


383, a company has a web server running on an amazon ec2 instance in a public subnet with an elastic ip address.the default security group is assigned to the ec2 instance. the default network acl has been modified to block all traffic. a solutions architect needs to make the web server accessible from everywhere on port 443. which combination of steps will accomplish this task? (select two.)
A. create a security group with a rule to allow tcp port 443 from source0.0.0/0.
A. 
B. create a security group with a rule to allow tcp port 443 to destination 0 0 0 0/0.
C. update the network ad to allow tcp port 443 from source 0.0 0 0/0.
D. update the network ad to allow inbound/outbound tcp port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.
E. update the network ad to allow inbound tcp port 443 from source0.0 0/0 and outbound tcp port 32768-65535 to destination 0 0 0.0/0




384, a company is planning to migrate its virtual server-based workloads to aws the company has internet- facing load balancers backed by application servers. the application servers rely on patches from an internet- hosted repository which services should a solutions architect recommend be hosted on the public subnet? (select two.)
A. nat gateway
B. amazon rds db instances C.application load balancers
D. amazon ec2 application servers
E. amazon elastic file system (amazon efs) volumes


fAIHfr: both nat gateway and alb needs to be in the public subnet.



385, an application is running on amazon ec2 instances sensitive information required for the application is stored in an amazon s3 bucket.the bucket needs to be protected from internet access while only allowing services within the vpc access to the bucket.which combination of actions should a solutions archived take to accomplish this" (select two.)
A. create a vpc endpoint for amazon s3.
B. enable server access logging on the bucket
C. apply a bucket policy to restrict access to the s3 endpoint.
D. add an s3 ad to the bucket that has sensitive information
E. restrict users using the iam policy to use the specific bucket
A. 


Mtfr: acl is a property at object level not at bucket level .also by just adding acl you cant let the services in vpc allow access to the bucket .


386, a company captures clickstream data from multiple websites and tftfr:s it using batch processing. the data is loaded nightly into amazon redshift and is consumed by business analysts. the company wants to move towards near-real-time data processing for timely insights. the solution should process the streaming data with minimal effort and operational overhead. which combination of aws services are most cost- effective for this solution? (choose two.)
A. amazon ec2 B.awslambda
C.amazon kinesis data streams
D.amazon kinesis data firehose
E.amazon kinesis data analytics


Mtfr: a)amazon ec2 - expensiveb)aws lambda - non minimal effortc)kinesis data stream - non near real timed)kinesis data firehose- by default the manner to ingest data - correct e) kinesis data analytics- we need perform analytics - correct https://dO.awsstatic.com/whitepapers/whitepaper-streaming-data-  solutions-on-aws­ with- amazonkinesis.pdf (9)https://aws.amazon.com/kinesis/#evolve_from_batch_to_real­ time_analytics


387, a solutions architect is moving the static content from a public website hosted on amazon ec2 instances to an amazon s3 bucket. an amazon cloudfront distribution will be used to deliver the static assets. the security group used by the ec2 instances restricts access to a limited set of ip ranges. access to the static content should be similarly restricted. which combination of steps will meet these requirements? (select two.)
A. create an origin access identity (oai) and associate it with the distribution. change the permissions in the bucket policy so that only the oai can read the objects.
B. create an aws waf web ad that includes the same ip restrictions that exist in the ec2 security group. associate this new web acl with the cloudfront distribution.
A. 
C. create a new security group that includes the same ip restrictions that exist in the current ec2 security group. associate this new security group with the cloudfront distribution.
D. create a new security group that includes the same ip restrictions that exist in the current ec2 security group. associate this new security group with the s3 bucket hosting the static content.
E. create a new iam role and associate the role with the distribution. change the permissions either on the s3 bucket or on the files within the s3 bucket so that only the newly created iam role has read and download permissions.

It-tfr: -use signed urls or cookies-restrict access to content in amazon s3 buckets=> a-use aws wafweb ads=> b-use geo
restrictionhttps:/ / docs.aws.amazon.com/amazoncloudfront/latest/developerguide/data­ protection- summary.html#data- protection-summary-restrict-
accesshttps:/  /  docs.aws.amazon.com/waf/latest/ developerguide/web-ad.html


388, a solutions architect is designing a customer-facing application. the application is expected to have a variable amount ofreads and writes depending on the time of year and clearly defined access patterns throughout the year. management requires that database auditing and scaling be managed in the aws cloud. the recovery point objective (rpo) must be less than 5 hours.which solutions can accomplish this? (select two.)
A. use amazon dynamodb with auto scaling.use on-demand backups and aws cloudtrail.
B. use amazon dynamodb with auto scaling.use on-demand backups and amazon dynamodb streams.
C. use amazon redshift configure concurrency scaling.enable audit logging.perform database snapshots every 4 hours.
D. use amazon rds with provisioned iops. enable the database auditing parameter. perform database snapshots every 5 hours.
E. use amazon rds with auto scaling.enable the database auditing parameter.configure the backup retention period to at least 1 day.

Mtfr: first, the question has 2 demands we need to understand it clearly.l)auditing and scaling2)rpo should be less than 5 hrs.we are talking about one data source where this is done. there is no point in the question that says we need to audit on a different data source and rpo on different data source.hence we must not mix and match the data sources. there

is no audit for red shift hence c is out.rds had audit and rpo but the is 5 hrs and as per the question it should be less than 5 so the pair d and e is out.


389, a company's application is running on amazon ec2 instances ma single region in the event of a disaster a solutions architect needs to ensure that the resources can also be deployed to a second region. which combination of actions should the solutions architect take to accomplish this-? (selecttwo)
A. detach a volume on an ec2 instance and copy it to amazon s3
B. launch a new ec2 instance from an amazon machine image (ami) in a new region
C. launch a new ec2 instance in a new region and copy a volume from amazon s3 to the new instance
D. copy an amazon machine image (ami) of an ec2 instance and specify a different region for the destination
E. copy an amazon elastic block store (amazon ebs) volume from amazon s3 and launch an ec2 instance in the destination region using that ebs volume

Mi-fr: cross region ec2 ami copy -we know that you want to build applications that span aws regions and we're working to provide you with the services and features needed to do so. we started out by launching the ebs snapshot copy feature late last year. this feature gave you the ability to copy a snapshot from region to region with just a couple of clicks. in addition, last month we made a significant reduction (26% to 83%) in the cost of transferring data between aws regions, making it less expensive to operate in more than one aws region. today we are introducing a new feature: amazon machine image (ami) copy. ami copy enables you to easily copy your amazon machine images between awsregions. ami copy helps enable several key scenarios including:simple and consistent multi-region deployment 1111 you can copy an ami from one region to another, enabling you to easily launch consistent instances based on the same ami into different regions. scalability 1111 you can more easily design and build world-scale applications that meet the needs of your users, regardless of their location.performance 1111 you can increase performance by distributing your application and locating critical components of your application in closer proximity to your users. you can also take advantage of region- specific features such as instance types or other aws services. even higher availability 1111 you can design and deploy applications across aws regions, to increase availability. once the new ami is in an available state the copy is complete.reference:https://aws.amazon.com/blogs/aws/ec2-ami-copy-between-regions/

390, a solutions architect is designing a mission-critical web application. it will consist of amazon ec2 instances behind an application load balancer and a relational database. the database should be highly available and fault tolerant.which database implementations will meet these requirements? (select two.)
A. amazon redshift
B. amazon dynamodb
C. amazon rds for mysql
D. mysql-compatible amazon aurora multi-az E.amazon rds for sql server standard edition mufti-az
Mi-fr: since the multi-az support by amazon rds for ms sql server a - redshift is a cloud data warehouse not a sql databaseb - it does not say that global tables is active and it is a nosql database c - it is not multi az
https:/ / docs.aws.amazon.com/amazonrds /latest/userguide/ user_sqlserverm ultiaz.html


39L	a company recently deployed multiple amazon elastic file system (amazon efs) file systems in an aws account.the company wants to access the efs file systems in amazon linux ec2 instance in a second aws account permissions are already granted from the source account. which combination of actions should the solutions architect recommend to meet these requirements? (select two.)
A. add a line to the/etc/hosts file on the ec2 instance that references the filesystemld of the efs file system.
B. call the describemounttargets operation in the source account for the file system to identify the mount target name for the availability zone that matches the availability zone of the ec2 instance
C. add a line to the /etc/hosts file on the ec2 instance that references the mounttargetld of the efs mount target
D. call the describemounttargets operation in the source account for the file system to identify the mount target ip address for the availability zone that matches the availability zone of the ec2 instance
E. add a line to the /etc/hosts file on the ec2 instance that references the ipaddress of the efs mount target


tfr: modify on ad->de


392, a company is running a serverless application that consists of several aws lambda functions and amazon dynamodb tables. the company has created new functionality that requires the lambda functions to access an amazon neptune db cluster. the neptune db cluster is located in three subnets in a vpc. which of the possible solutions will allow the lambda functions to access the neptune db cluster and dynamodb tables? (select two.)
A. create three public subnets in the neptune vpc, and route traffic through an internet gateway host the lambda functions in the three new public subnets.
B. create three private subnets in the neptune vpc, and route internet traffic through a nat gateway host the lambda functions in the three new private subnets.
C. host the lambda functions outside the vpc. update the neptune security group to allow access from the ip ranges of the lambda functions.
D. host the lambda functions outside the vpc create a vpc endpoint for the neptune database, and have the lambda functions access neptune over the vpc endpoint.
E. create three private subnets in the neptune vpc host the lambda functions in the three new isolated subnets. create a vpc endpoint for dynamodb, and route dynamodb traffic to the vpc endpoint.




393, a solutions architect is building a web application that uses an amazon rds for postgresql db instance. the db instance is expected to receive many more reads than writes. the solutions architect needs to ensure that the large amount of read traffic can be accommodated and that the db instance is highly available.which steps should the solutions architect take to meet these requirements? (select three)
A. create multiple read replicas and put them into an auto scaling group
B. create multiple read replicas in different availability zones
C. create an amazon route 53 hosted zone and a record set for each read replica with a ttl and a weighted routing policy.
D. create an application load balancer (alb) and put the read replicas behind the alb
E. configure an amazon cloudwatch alarm to detect a failed read replica. set the alarm to directly invoke an aws lambda function to delete its route 53record set
A. 
F. configure an amazon route 53 health check for each read replica using its endpoint




394, a company needs to migrate two individual applications from on premises to aws:-the first application is a legacy custom application that is hosted on a physical windows server. the application source code is no longer available. the application has little documentation, has hard coded operating system configuration settings, and is used by an external third party. --the second application is an ibm db2 database that is hosted on a single linux vm that uses network- attached storage (nas) to store the database data. the company uses this database internally for employee records. the applications are hosted in a data center that the company plans to decommission in 90 days. where possible, the company must use managed aws services.which actions for migration should a solutions architect recommend to meet these requirements? (select two)
A. migrate the windows server with the legacy application to amazon ec2 by using cloudendure migration.
B. migrate the linux vm with the ibm db2 database service to an amazon ec2 instance by using cloudendure migration.
C. migrate the windows server with the legacy application to amazon ec2 by using aws server migration service (aws sms).
D. migrate the ibm db2 database data to amazon rds for mysql by using aws database migration service (aws dms) and the aws schema conversion tool replication agent
E. migrate the lbm db2database data to amazon rds for mysql by using aws datasync and the aws schema conversion tool data extraction agent


f@Hfr: modify on bc->ad



395, a company manages an on-premises javascript front-end web application. the application is hosted on two servers secured with a corporate active directory. the application calls a set of java-based microservices on an application server and stores data in a clustered mysql database. the application is heavily used during the day on weekdays. it is lightly used during the evenings and weekends daytime traffic to the application has increased rapidly, and reliability has diminished as a result. the company wants to migrate the application to aws with a solution that eliminates the need for server maintenance. with an api to securely connect to the microservices.which combination of actions will meet these requirements? (select three)

A. host the web application on amazon s3. use amazon cognito identity pools (federated identities) with saml for authentication and authorization.
B. host the web application on amazon ec2 with auto scaling use amazon cognito federation and login with amazon for authentication and authorization.
C. create an api layer with amazon api gateway. rehost the microservices on aws fargate containers.
D. create an api layer with amazon api gateway. rehost the microservices on amazon elastic container service (amazon ecs) containers
E. replatform the database to amazon rds for mysql. F.replatform the database to amazon aurora mysql serverless.


396, a company is using multiple aws accounts and has multiple devops teams running production and non- production workloads in these accounts. the company would like to centrally-restrict access to some of the aws services that the devops teams do not use. the company decided to use aws organizations and successfully invited all aws accounts into the organization. they would like to allow access to services that are currently in- use and deny a few specific services also they would like to administer multiple accounts together as a single unit.what combination of steps should the solutions architect take to satisfy these requirements? (select three)
A. use a deny list strategy.
B. review the access advisor in aws iam to determine services recently used.
C. review the aws trusted advisor report to determine services recently used.
D. remove the default fullawsaccess scp.
E. define organizational units (ous) and place the member accounts in the ous.
F. remove the default denyawsaccess scp.



397, a company has a data late in amazon s3 that needs to be accessed by hundreds of applications acrossmany aws accounts.the company's information security policy states that the s3 bucket must not be accessed over the public internet and that each application

should have the minimum permissions necessary to function. to meet these requirements, a solution architect plans to use an s3 access point that is restricted to specific vpcs for each application.which combination of steps should the solutions architect take to implement this solution? (choose two)
A. create an s3 access point for each application in the aws account that owns the s3 bucket.configure each access point to be accessible only from the application's vpc. update the bucket policy to require access from an access point.
B. create an interface endpoint for amazon s3 in each application's vpc configure the endpoint policy to allow access to an s3 access point.create a vpc gateway attachment for the s3 endpoint.
C. create a gateway endpoint for amazon s3 in each application's vpc. configure the endpoint policy to allow access to an s3 access point. specify the route table that is used to access the access point.
D. create an s3 access point for each application in each aws account and attach the access points in the s3 bucket.configure each access point to accessible only from the application's vpc update the bucket policy to require access from an access point
E. create a gateway endpoint for amazon s3 in the data lake's vpc.attach an endpoint policy to allow access to the s3 bucket. specify the route able that is used to access the bucket.



398, a company has 50 aws accounts that are members of an organization in aws organizations. each account contains multiple vpcs. the company wants to use aws transit gateway to establish connectivity between the vpcs in each member account. each time a new member account is created, the company wants to automate the process of creating a new vpc and a transit gateway attachment. which combination of steps willmeet these requirements? (select two.)
A. from the master account, share the transit gateway with member accounts by using aws resource access manager.
B. from the master account, share the transit gateway with member accounts by using an aws organizations scp.
C. launch an aws cloudformation stack set from the master account that automatically creates a new vpc and a vpc transit gateway attachment in a member account.associate the attachment with the transit gateway in the master account by using the transit gateway id.
D. launch an aws cloudformation stack set from the master account that automatically creates a new vpc and a peering transit gateway attachment in a member account.share the
A. 
attachment with the transit gateway in the master account by using a transit gateway service- linked role.
E. from the master account, share the transit gateway with member accounts by using aws service catalog.




399, a company runs an application in the cloud that consists of a database and a website. users can post data to the website, have the data processed, and have the data sent back to them in an email data is stored is a mysql database running on an amazon ec2 instance. the database is running in a vpc with two private subnets. the website is running on apache tomcat in a single ec2 instance in a different vpc with one public subnet.there is a single vpc peering connection between the database and website vpc. the website has suffered several outages during the last month due to high traffic. which actions should a solutions architect take to increase the reliability of the application? (selectthree)
A. place the tomcat server in an auto scaling group with multiple ec2 instances behind an application load balancer
B. provision an additional vpc peering connection
C. migrate the mysql database to amazon aurora with one aurora replica D.provision two nat gateways in the database vpc
E. move the tomcat server to the database vpc
F. create an additional public subnet in a different availability zone in the website vpc.


tfr: modify on bce>ace



400, a company is testing amazon elastic file service (efs) in its development vpc, and would like to extend this test on-premises. efs is running in us-east-land the corporate network is currently connected to this region through a site-to-site vpn. all on-premises computers and servers are required to have all dns traffic resolved by their on-premises dns servers the on-premises users would like to connect to the efs using a dns name instead of an ip addresswhat collection of steps must be taken to meet this requirement? (select two.)

A. create a new amazon route 53 private hosted zone with a domain name of awscloud.example.com and associate the development vpc to this zone. create a cname record and point this to the efs endpoint.
B. create a new amazon route 53 public hosted zone with a domain name ofawscloud.example.com and associate the development vpc to this zone. create a cname record and point this to the efs endpoint.
C. create a conditional forwarder rule in the on-premises dns servers to forward requests for awscloud.example.com to the amazon route 53 resolver inbound endpoints
D. create a conditional forwarder rule in the on-premises dns servers to forward requests for awscloud example.com to the amazon route 53 resolver outbound endpoints
E. create a conditional forwarder rule in the on-premises dns servers to forward requests for awscloud .example.com to the amazon-provided dns server.


 tfr: https:/ / docs.aws.amazon.com/zh_cn/ efs/latest/ug/efs-onpremises.html https:/ / docs.aws.amazon.com/ zh_cn/route53 /latest/developerguide/resolver­ forwarding-inbound- queries.html


40L	a company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app.usage peaks between 8 am and 5 pm on weekdays, with thousands of uploads per minute. the app is rarely used at any other time a user is notified when image processing is complete when combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (select three)
A. upload files from the mobile software directly to amazon s3.use s3 event notifications to create a message in an amazon mq queue.
B. upload files from the mobile software directly to amazon s3 use s3 event notifications to create a message in an amazon simple queue service (amazon sqs) standard queue.
C. invoke an aws lambda function to perform mage processing when a message is available in the queue
D. invoke an s3 batch operations job to perform image processing when a message is available in the queue
E. send a push notification to the mobile app by using amazon simple notification service (amazon sns) when processing is complete
F. send a push notification to the mobile app by using amazon simple email service (amazon ses) when processing is complete.
A. 


Mtfr: very corelated to each other and makes senses3 batch operations job seems not relevant so bee https://docs.aws.amazon.com/amazons3/latest/userguide/batch-ops­ basics.html


402, a software company hosts an application on aws with resources in multiple aws accounts and regions. the application runs on a group of amazon ec2 instances man application vpc located in the us-east-1 region with an ipv4 cidr block of 10.10.0.0/16. in a different aws account, a shared services vpc is located in theus-east-2 region with an ipv4 cidr block of 10.10.10.0/24. when a cloud engineer uses aws cloudformation to attempt to peer the application vpc with the shared services vpc. an error message indicates a peering failure.which factors could cause this error? (select two)
A. the ipv4 cidr ranges of the two vpcs overlap
B. the vpcs are not in the same region
C. one or both accounts do not have access to an internet gateway
D. one of the vpcs was not shared through aws resource access manager.
E. the iam role in the peer accepter account does not have the correct permissions.


tfr: cloud engineer uses "aws cloudformation" to attempt to peer the application.



403, a solutions architect needs to provide aws cost and usage report data from a company's aws organizations master account.the company already has an amazon s3 bucket to store the reports. the reports must be automatically ingested into a database that can be visualized with other tools. which combination of steps should the solutions architect take to meet these requirements? (select three)
A. create an amazon eventbridge (amazon cloudwatch events) rule that a new object creation in the s3 bucket will trigger
B. create an aws cost and usage report configuration to deliver the data into the s3 bucket C.configure an aws glue crawler that a new object creation in the s3 bucket will trigger D.create an aws lambda function that a new object creation in the s3 bucket will trigger
A. 
E.create an aws glue crawler that the aws lambda function will trigger to crawl objects in the s3 bucket
F.create an aws glue crawler that the amazon eventbridge (amazon cloudwatch events) rule will trigger to crawl in the s3 bucket.


ffllHfr: modify on bde>abf



404, a company stores customer data in an amazon s3 bucket with s3 versioning enabled in the us>west-2 region.the s3 bucket is encrypted with an aws key management service (aws kms) customer managed cmk. a compliance policy states that redundant copies of all s3 objects must be stored in the us-east-2 region. the s3 buckets are allowed to stay in the same aws account. which combination of steps will meet these requirements with the least operational effort? (select three)
A. configure an aws lambda function that copies objects to the us-east-2 bucket and is triggered when objects are created in the us-west-2 bucket
B. create a destination s3 bucket in us-east-2 with s3 versioning enabled
C. set up s3 cross-region replication between the two s3 buckets.
D. create and assign an s3 bucket policy that allows reading from the source s3 bucket.
E. create and assign to amazon s3 an iam role with a policy that allows reading from the source s3 bucket and replication to the destination s3 bucket
F. create a destination s3 bucket in us-east-2


ffllHfr: modify on bcd>bce



405, a greeting card company and a gift basket company both use aws organizations to manage their aws accounts. the greeting card company recently acquired the gift basket company and wants to consolidate all accounts under the greeting card company's management account the change of ownership of the gift basket company's aws accounts must happen without interruption to either business. which actions should a solutions architect advise both companies to complete to facilitate a successful migration? (select three)

A. ensure that valid credit card payment details have been added to each of the member accounts.
B. delete existing credit card payment details that have been added to the member accounts.
C. contact aws and request that aws support move the member accounts to the new management account.
D. check aws artifact for any existing legal, privacy, or compliance organization agreements that are in place.
E. update any resource-based policies by using the aws:principalorgld condition key to reference the new organization id.
F. delete the organizationaccountaccessrole iam role from each of the member accounts.




406, a company collects a steady stream of 10 million data records from 100,000 sources each day. these records are written to an amazon rds mysql db. a query must produce the daily average of a data source over the past 30 days. there are twice as many reads as writes. queries to the collected data are for one source id at a time. how can the solutions architect improve the reliability and cost effectiveness of this solution?
A. use amazon aurora with mysql in a multi-az mode. use four additional read replicas.
B. use amazon dynamodb with the source id as the partition key and the timestamp as the sort key. use a time to live (ttl) to delete data after 30 days.
C. use amazon dynamodb with the source id as the partition key. use a different table each day.
D. ingest data into amazon kinesis using a retention period of 30 days. use aws lambda to write data records to amazon elasticache for read access.

Mi-fr: b cheapest and can do the job.d most expensive one.


407, a solutions architect is designing a network solution for a company that has applications running in a data center in northern virginia. the applications in the company's data center require predictable performance to applications running in a virtual private cloud (vpc) located in us-east-1, and a secondary vpc in us-west-2 within the same account. the company data center is collocated in an aws direct connect facility that serves the us-

est-1 region. the company has already ordered an aws direct connect connection and a cross-connect has been established.which solution will meet the requirements at the lowest cost?
A. provision a direct connect gateway and attach the virtual private (vgw) for the vpc in us­ east-1 and the vgw for the vpc in us-west-2. create a private vif on the direct connect connection and associate it to the direct connect gateway.
B. create private vifs on the direct connect connection for each of the company's vpcs in the us-east-1 and us-west-2 regions. configure the company's data center router to connect directly with the vpcs in those regions via the private vifs.
C. deploy a transit vpc solution using amazon ec2-based router instances in the us-east-1 region.establish ipsec vpn tunnels between the transit routers and virtual private gateways (vgws) located in the us-east-1 and us-west-2 regions, which are attached to the company's vpcs in those regions.create a public vif on the direct connect connection and establish ipsec vpn tunnels over the public vif between the transit routers and the company's data center router.
D. order a second direct connect connection to a direct connect facility with connectivity to the us-west-2 region. work with partner to establish a network extension link over dark fiber from the direct connect facility to the company's data center. establish private vifs on the direct connect connections for each of the company's vpcs in the respective regions.configure the company's data center router to connect directly with the vpcs in those regions via the private vifs.


 tfr: a direct connect gateway is global resource, which makes connect to other region fast as well https:// docs.aws.amazon.com/directconnect/latest/userguide/direct-connect­ gateways-intro.html b a is better as less latencyhttps://aws.amazon.com/premiumsupport/knowledge-center/public-private­ interface-dx/ c i don't we think we need public vif here. also, maintaining a ec2 server is overhead.d this will work, but a is much cheaper


408, a company is building an aws landing zone and has asked a solutions architect to design a multi-account access strategy that will allow hundreds of users to use corporate credentials to access the aws console. the company is running a microsoft active directory and users will use an aws direct connect connection to connect to aws. the company also wants to be able to federate to third-party services and providers, including custom applications.which solution meets the requirements by using the least amount of management overhead?

A. connect the active directory to aws by using single sign-on and an active directory federation services (ad fs) with saml 2.0, and then configure the identity provider (idp) system to use formbased authentication. build the ad fs portal page with corporate branding, and integrate third- party applications that support saml 2.0 as required.
B. create a two-way forest trust relationship between the on-premises active directory and the aws directory service. set up aws single sign-on with aws organizations. use single sign­ on integrations for connections with third-party applications.
C. configure single sign-on by connecting the on-premises active directory using the aws directory service ad connector. enable federation to the aws services and accounts by using the iam applications and services linking function. leverage third-party single sign-on as needed.
D. connect the company's active directory to aws by using ad fs and saml 2.0. configure the ad fs claim rule to leverage regex third-party single sign-on as needed, and add it to the ad fs server.


fAIHfr: a this will work but you will need to build login page in your on-prem environment and ad fs portal page and the adfs serverc i don't think service linking function is used in this way. we should use aws sso for federations so that we could leverage third-party sso. https://docs.aws.amazon.com/iam/latest/userguide/using-service-    linked-roles.html https:/   /aws.amazon.com/blogs/security/how-to-create-and-manage-users-within-aws­ sso/ d will need to maintain the ad fs server


409, a large global financial services company has multiple business units. the company wants to allow developers to try new services, but there are multiple compliance requirements for different workloads. the security team is concerned about the access strategy for on-premises and aws implementations. they would like to enforce governance for aws services used by business team for regulatory workloads, including payment card industry (pci) requirements.which solution will address the security team's concerns and allow the developers to try new services?
A. implement a strong identity and access management model that includes users, groups, and roles in various aws accounts. ensure that centralized aws cloudtrail logging is enabled to detect anomalies.build automation with aws lambda to tear down unapproved aws resources for governance.
B. build a multi-account strategy based on business units, environments, and specific regulatory requirements. implement saml-based federation across all aws accounts with an on-premises identity store. use aws organizations and build organizational units (ous)
A. 
structure based on regulations and service governance. implement service control policies across ous.
(.implement a multi-account strategy based on business units, environments, and specific regulatory requirements. ensure that only pci-compliant services are approved for use in the accounts. build iam policies to give access to only pci-compliant services for governance.
D.build one aws account for the company for the strong security controls. ensure that all the service limits are raised to meet company scalability requirements. implement saml federation with an on- premises identity store, and ensure that only approved services are used in the account.

 tfr: b should try to stop service get created at the first place c scp is better fit here? d not the best practicea: stop developers from trying new services. c: does not show the enforcement tool.d: one account contradict with the requirement.bis correct!!!


410, a company operating a website on aws requires high levels of scalability, availability and performance. the company is running a ruby on rails application on amazon ec2. it has a data tier on mysql 5.6 on amazon ec2 using 16 tb of amazon ebs storage amazon cloudfront is used to cache application content. the operations team is reporting continuous and unexpected growth of ebs volumes assigned to the mysql database. the solutions architect has been asked to design a highly scalable, highly available, and high- performing solution.which solution is the most cost-effective at scale?
A.implement multi-az and auto scaling for all ec2 instances in the current configuration.ensure that all ec2 instances are purchased as reserved instances. implement new elastic amazon ebs volumes for the data tier.
B.design and implement the docker-based containerized solution for the application using amazon ecs.migrate to an amazon aurora mysql multi-az cluster. implement storage checks for aurora mysql storage utilization and an aws lambda function to grow the aurora mysql storage, as necessary.ensure that multi-az architectures are implemented.
C.ensure that ec2 instances are right-sized and behind an elastic load balancing load balancer.implement auto scaling with ec2 instances. ensure that the reserved instances are purchased for fixed capacity and that auto scaling instances run on demand. migrate to an amazon aurora mysql multi-az cluster. ensure that multi-az architectures are implemented.
D.ensure that ec2 instances are right-sized and behind an elastic load balancer. implement auto scaling with ec2 instances. ensure that reserved instances are purchased for fixed capacity and that auto scaling instances run on demand. migrate to an amazon aurora mysql multi-az cluster.implement storage checks for aurora mysql storage utilization and an aws

lambda function to grow aurora mysql storage, as necessary. ensure multi-az architectures are implemented.

 tfr: a database with ec2 is expensive, and ebs has maximum size of 16th b aurora storage can  sacale automaticallyhttps://docs.aws.amazon.com/amazonrds/latest/aurorauserguide/ aurora.managing.performance.html#aurora.managing.performance.storagescaling d aurora storage can sacale automatically


411, an internal security audit of aws resources within a company found that a number of amazon ec2 instances running microsoft windows workloads were missing several important operating system-level patches. a solutions architect has been asked to fix existing patch deficiencies, and to develop a workflow to ensure that future patching requirements are identified and taken care of quickly. the solutions architect has decided to use aws systems manager. it is important that ec2 instance reboots do not occur at the same time on all windows workloads to meet organizational uptime requirements. which workflow will meet these requirements in an automated manner?
A.add a patch group tag with a value of windows servers to all existing ec2 instances.ensure that all windows ec2 instances are assigned this tag. associate the aws- defaultpatchbaseline to the windows servers patch group. define an aws systems manager maintenance window, conduct patching within it, and associate it with the windows servers patch group. register instances with the maintenance window using associated subnet ids. assign the aws­ runpatchbaseline document as a task within each maintenance window.
B.add a patch group tag a value of windows servers to all existing ec2 instances. ensure that all windows ec2 instances are assigned this tag. associate the aws-windowspatchbaseline document as a task associated with thewindows servers patch group. create an amazon cloudwatch events rule configured to use a cron expression to schedule the execution of patching using the aws systems manager run command. create an aws systems manager state manager document to define commands to be executed during patch execution.
C.add a patch group tag with a value of either windows serversl or windows server2 to all existing ec2 instances. ensure that all windows ec2 instances are assigned this tag.associate the awsdefaultpatchbaseline with both windows servers patch groups. define two non­ overlapping aws systems manager maintenance windows, conduct patching within them, and associate each with a different patch group. register targets with specific maintenance windows using the patch group tags. assign the aws-runpatchbaseline document as a task within each maintenance window.
D.add a patch group tag with a value of either windows serversl or windows server2 to all existing ec2 instances. ensure that all windows ec2 instances are assigned this tag.associate

the awswindowspatchbaseline with both windows servers patch groups. define two non­ overlapping aws systems manager maintenance windows, conduct patching within them, and associate each with a different patch group. assign the aws-runwindowspatchbaseline document as a task within each maintenance window.create an aws systems manager state manager document to define commands to be executed during patch execution.

Mi-fr: a all instances will be patched at the same time, down time b aws­ windowspatchbaseline is not a valid one also when using patch group, you don't use run commandc https:/ / docs.aws.amazon.com/systems-manager/latest/userguide/ sysman­ patch-scheduletasks.html https:/ / docs.aws.amazon.com/systems-
manager/latest/userguide/sysman-patch-patchgroups.html d aws-windowspatchbaseline is not a valid onehttps://docs.aws.amazon.com/systems-
manager/latest/userguide/ sysman-patch-baselines.html


412, a company had a tight deadline to migrate its on-premises environment to aws. it moved over microsoft sql servers and microsoft windows servers using the virtual machine import/export service and rebuild other applications native to the cloud. the team created both amazon ec2 databases and used amazon rds.each team in the company was responsible for migrating their applications, and they have created individual accounts for isolation ofresources. the company did not have much time to consider costs, but now it would like suggestions on reducing its aws spend.which steps should a solutions architect take to reduce costs?
A.enable aws business support and review aws trusted advisor's cost checks. create amazon ec2 auto scaling groups for applications that experience fluctuating demand. save aws simple monthly calculator reports in amazon s3 for trend analysis. create a master account under organizations and have teams join for consolidating billing.
B.enable cost explorer and aws business support reserve amazon ec2 and amazon rds db instances. use amazon cloudwatch and aws trusted advisor for monitoring and to receive costsavings suggestions. create a master account under organizations and have teams join for consolidated billing.
C.create an aws lambda function that changes the instance size based on amazon cloudwatch alarms.reserve instances based on aws simple monthly calculator suggestions. have an aws well- architected framework review and apply recommendations.create a master account under organizations and have teams join for consolidated billing.
D.create a budget and monitor for costs exceeding the budget. create amazon ec2 auto scaling groups for applications that experience fluctuating demand. create an aws lambda function that changes instance sizes based on amazon cloudwatch alarms. have each team

upload their bill to an amazon s3 bucket for analysis of team spending. use spot instances on nightly batch processing jobs.


 tfr: a simple monthly aculator report is not really a report for analysis trends. c resize instance may require stop the instance first, which is not ideal for production environments	d consolidate billing is a must, this is out


413, a company has an application behind a load balancer with enough amazon ec2 instances to satisfy peak demand. scripts and third-party deployment solutions are used to configure ec2 instances when demand increases or an instance fails. the team must periodically evaluate the utilization of the instance types to ensure that the correct sizes are deployed. how can this workload be optimized to meet these requirements?
A.use cloudformer' to create aws cloudformation stacks from the current resources.deploy that stack by using aws cloudformation in the same region. use amazon cloudwatch alarms to send notifications about underutilized resources to provide cost-savings suggestions.
B.create an auto scaling group to scale the instances, and use aws codedeploy to perform the configuration. change from a load balancer to an application load balancer. purchase a third-party product that provides suggestions for cost savings on aws resources.
C.deploy the application by using aws elastic beanstalk with default options. register for an aws support developer plan. review the instance usage for the application by using amazon cloudwatch, and identify less expensive instances that can handle the load. hold monthly meetings to review new instance types and determine whether reserved instances should be purchased.
D.deploy the application as a docker image by using amazon ecs. set up amazon ec2 auto scaling and amazon ecs scaling. register for aws business support and use trusted advisor checks to provide suggestions on cost savings.


Mtfr: a we don't need cloudformation hereb codedeploy is not really used to config infrastructures (config auto scale group) c this answer solve nothing.....


414, a company is moving a business-critical application onto aws. it is a traditional threetier web application using an oracle database. data must be encrypted in transit and at rest. the database hosts 12 tb of data.network connectivity to the source oracle database over the internal is allowed, and the company wants to reduce the operational costs by using aws managed services where possible. all primary keys only; however, it contains

many binary large object (blob) fields. it was not possible to use the database's native replication tools because of licensing restrictions. which database migration solution will result in the least amount of impact to the application's availability?
A.provision an amazon rds for oracle instance. host the rds database within a virtual private cloud (vpc) subnet with internet access, and set up the rds database as an encrypted read replica of the source database. use ssl to encrypt the connection between the two databases. monitor the replication performance by watching the rds replicalag metric. during the application maintenance window, shut down the on-premises database and switch over the application connection to the rds instance when there is no more replication lag. promote the read replica into a standalone database instance.
B.provision an amazon ec2 instance and install the same oracle database software. create a backup of the source database using the supported tools. during the application maintenance window, restore the backup into the oracledatabase running in the ec2 instance. set up an amazon rds for oracle instance, and create an import job between the database hosted in aws. shut down the source database and switch over the database connections to the rds instance when the job is complete.
C.use aws dms to load and replicate the dataset between the on-premises oracle database and the replication instance hosted on aws. provision an amazon rds for oracle instance with transparent data encryption (tde) enabled and configure it as target for the replication instance. create a customer-managed aws kms master key to set it as the encryption key for the replication instance.use aws dms tasks to load the data into the target rds instance. during the application maintenance window and after the load tasks reach the ongoing replication phase, switch the database connections to the new database.
D.create a compressed full database backup on the on-premises oracle database during an application maintenance window. while the backup is being performed, provision a 10 gbps aws direct connect connection to increase the transfer speed of the database backup files to amazon s3, and shorten the maintenance window period. use ssl/tls to copy the files over the direct connect connection. when the backup files are successfully copied, start the maintenance window, and rise any of the amazon rds supported tools to import the data into a newly provisioned amazon rds for oracle instance with encryption enabled. wait until the data is fully loaded and switch over the database connections to the new database.delete the direct connect connection to cut unnecessary charges.

Mi-fr: a best solution, but native replication cannot be usedb backup may not contain most up to date data c you cannot use a kms key for tde encryption, but we are encrypted the replication instance with kms and use tde for newly created rds instance.https://docs.aws.amazon.com/amazonrds/latest/userguide/appendix.oracle.optio    ns.advsecurity.html
https:/ / docs.aws.amazon.com/dms/latest/userguide/ chap_source.oracle.html https:/ /

docs.aws.amazon.com/dms/latest/userguide/chap_target.sqlserver.html https://docs.aws.amazon.com/ dms/latest/userguide/chap_replicationinstance.html d this will take hours when migrate


415, a large multinational company runs a timesheet application on aws that is used by staff across the world. the application runs on amazon ec2 instances in an auto scaling group behind an elastic load balancing (elb) load balancer, and stores in an amazon rds mysql multi-az database instance. the cfo is concerned about the impact on the business if the application is not available. the application must not be down for more than two hours, but he solution must be as cost-effective as possible. how should the solutions architect meet the cfo's requirements while minimizing data loss?
A.in another region, configure a read replica and create a copy of the infrastructure. when an issue occurs, promote the read replica and configure as an amazon rds multi-az database instance.update the dns to point to the other region's elb.
B.configure a 1-day window of 60-minute snapshots of the amazon rds multi-az database instance.create an aws cloudformation template of the application infrastructure that uses the latest snapshot.when an issue occurs, use the aws cloudformation template to create the environment in another region. update thedns record to point to the other region's elb.
C.configure a 1-day window of 60-minute snapshots of the amazon rds multi-az database instance which is copied to another region. create an aws cloudformation template of the application infrastructure that uses the latest copied snapshot. when an issue occurs, use the aws cloudformation template to create the environment in another region. update the dns record to point to the other region's elb.
D.configure a read replica in another region. create an aws cloudformation template of the application infrastructure. when an issue occurs, promote the read replica and configure as an amazon rds multi- az database instance and use the aws cloudformation template to create the environment in another region using the promoted amazon rds instance.update the dns record to point to the other region's elb.

Mi-fr: a multi site is expensive and we probably do not need it for 2 hour rto b under the hood, snapshot is regional resource, you will need to copy to other region to use it.c compare tod, there could be 1 hour data lost.d this is a typical pilot light structure and almost had no data lost


416, a group ofamazon ec2 instances have been configured as high performance computing (hpc) cluster. the instances are running in a placement group, and are able to

communicate with each other at network of up to 20 gbps. the cluster needs to communicate with a control ec2 instance outside of the placement group. the control instance has the same instance type and ami as the other instances, and is configured with a public ip address.how can the solutions architect improve the network speeds between the control instance and the instances in the placement group?
A.terminate the control instance and relaunch in the placement group.
B.ensure that the instances are communicating using the private ip addresses.
C.ensure that the control instance is using an elastic network adapter.
D.move the control instance inside the placement group.

fAIHfr: the hpc is already running with ena as it had up to 20 gbps connection already. all we need to do is move the control instance into the placement group. also, ena should be enabled automatically for supported kernel and instance type, so c is not
correct.https:/ / docs.aws.amazon.com/awsec2/latest/ userguide /placement- grou ps.html #change-instance- placement-
grouphttps:/ / docs.aws.amazon.com/awsec2/latest/userguide/placement-groups.html https://aws.amazon.com/ blogs/aws/elastic-network-adapter-high-performance-network­ interface-for- amazon-ec2/
https:/ / docs.aws.amazon.com/awsec2/latest/userguide/ enhanced-networking-ena.html


417, the security team needs to provide a team of interns with an aws environment so they can build the serverless video transcoding application. the project will use amazon s3, aws lambda, amazon api gateway, amazon cognito, amazon dynamodb, and amazon elastic transcoder. the interns should be able to create and configure the necessary resources, but they may not have access to create or modify aws iam roles. the solutions architect creates a policy and attaches it to the interns' group. how should the security team configure the environment to ensure that the interns are selfsufficient?
A.create a policy that allows creation of project-related resources only. create roles with required service permissions, which are assumable by the services.
B.create a policy that allows creation of all project-related resources, including roles that allow access only to specified resources.
C.create roles with the required service permissions, which are assumable by the services.have the interns create and use a bastion host to create the project resources in the project subnet only.

D.create a policy that allows creation of project-related resources only. require the interns to raise a request for roles to be created with the security team. the interns will provide the requirements for the permissions to be set in the role.

It-tfr: b intern should not have access to iamc this just won't work some of the mentioned resources are global resources and they do not belong to a subnet or a vpc, and you will need iam to do this, not just a bastion server.d raise request is not self sufficient


418, a company must deploy multiple independent instances of an application. the front­ end application is internet accessible. however, corporate policy stipulates that the backends are to be isolated from each other and the internet, yet accessible from a centralized administration server. the application setup should be automated to minimize the opportunity for mistakes as new instances are deployed. which option meets the requirements and minimizes costs?
A.use an aws cloudformation template to create identical iam roles for each region. use aws cloudformation stacksets to deploy each application instance by using parameters to customize for each instance, and use security groups to isolate each instance while permitting access to the central server.
B.create each instance of the application iam roles and resources in separate accounts by using aws cloudformation stacksets. include a vpn connection to the vpn gateway of the central administration server.
C.duplicate the application iam roles and resources in separate accounts by using a single cloudformation template. include vpc peering to connect the vpc of each application instance to a central vpc.
D.use the parameters of the aws cloudformation template to customize the deployment into separate accounts. include a nat gateway to allow communication back to the central administration server.

 It-tfr: a security group will not applied to newly created instance b https://docs.aws.amazon.com/ awscloudformation/latest/userguide/aws-resource-ec2- vpn- gateway.html c you cannot use a single cloudformation template to create resources across account d nat gateway will access the internet


419, a solutions architect has created an aws cloudformation template for a three-tier application that contains an auto scaling group of amazon ec2 instances running a custom

ami. the solutions architect wants to ensure that future updates to the custom ami can be deployed to a running stack by first updating the template to refer to the new ami, and then invoking updatestack to replace the ec2 instances with instances launched from the new ami.how can updates to the ami be deployed to meet these requirements?
A.create a change set for a new version of the template, view the changes to the running ec2 instances to ensure that the ami is correctly updated, and then execute the change set.
B.edit the aws::autoscaling: :launchconfiguration resource in the template, changing its to replace.deletionpolicy
C.edit the aws::autoscaling: :launchconfiguration resource in the template, inserting an attribute.updatepolicy
D.create a new stack from the updated template. once it is successfully deployed, modify the dns records to point to the new stack and delete the old stack.


tf tfr: a old instance is not going to be updatedhttps://docs.aws.amazon.com/awscloudformation/latest/userguide/ updating.stacks.walkthrough.html#update.walkthrough.amib deletionpolicy is for behaviour after deletionhttps://docs.aws.amazon.com/awscloudformation/latest/userguide/aws­ attribute-deletionpolicy.html c https://docs.aws.amazon.com/awscloudformation/latest/userguide/aws-attribute­ updatepolicy.html https://docs.aws.amazon.com/awscloudformation/latest/userguide/aws-properties-as­  launchconfig.html


420, a company is running a commercial apache hadoop cluster on amazon ec2. this cluster is being used daily to query large files on amazon s3. the data on amazon s3 has been curated and does not require any additional transformations steps. the company is using a commercial business intelligence (bi) tool on amazon ec2 to run queries against the hadoop cluster and visualize the data. the company wants to reduce or eliminate the overhead costs associated with managing the hadoop cluster and the bi tool. the company would like to remove to a more cost-effective solution with minimal effort. the visualization is simple and requires performing some basic aggregation steps only.which option will meet the company's requirements?
A.launch a transient amazon emr cluster daily and develop an apache hive script to tftfr: the files on amazon s3. shut down the amazon emr cluster when the job is complete. the use the amazon quicksight to connect to amazon emr and perform the visualization.

B.develop a stored procedure invoked from a mysql database running on amazon ec2 to tf i-Jr: ec2 to Mi-Jr: the files in amazon s3. then use a fast in-memory bl tool running on amazon ec2 to visualize the data.
C.develop a script that uses amazon athena to query and tfi-Jr: the files on amazon s3.then use amazon quicksight to connect to athena and perform the visualization.
D.use a commercial extract, transform, load (etl) tool that runs on amazon ec2 to prepare the data for processing. then switch to a faster and cheaper bl tool that runs on amazon ec2 to visualize the data from amazon s3.

tfi-Jr: a this could work but emr spin up daily still expensive. also, to connect quicksight to emr you will need presto running in the cluster...b this is just bad...and i do not think you can access s3 from a stored proc... d bad practice...etl could take very long time...


42L	a company has an organization in aws organizations that includes multiple aws accounts. each account has a single vpc.in an account named shared services, there is a transit gateway that is connected to a direct connect gateway that provides access to the company's on-premises network.the company configured aws resource access manager (aws ram) to share the transit gateway to all the accounts that are in the organization. the company has attached all the vpcs to the transit gateway to facilitate routing between each other.the company uses a dns server for on-premises servers.there are a pair of dns servers on premises and in the shared services account vpc.the company discovers that amazon ec2 instances that the company starts within the vpcs are not able to resolve addresses in the private on-premises domain. which solution will allow ec2 instances in all vpcs to resolve on-premises addresses'
A.define an amazon route 53 resolver outbound endpoint for the on-premises domain in the shared services account vpc. configure the outbound endpoint to use the ip addresses of the dns servers for the on-premises domain. configure a forwarder on the dns servers to point to the internal dns resolver of the vpc
B.create an amazon route 53 private hosted zone for the on-premises domain in the shared services account vpc. configure aws resource access manager (aws ram) to share the hosted zone to all accounts in the organization. associate the route 53 private hosted zone with each vpc
C.define an amazon route 53 resolver outbound endpoint for the on-premises domain in the shared services account vpc. configure the outbound endpoint to use the ip addresses of the dns servers for the on-premises domain. configure aws resource access manager (aws ram) to share the route 53 resolver rule to all accounts in the organization. associate the route 53 resolver rule with each vpc

D.define an amazon route 53 resolver inbound endpoint for the on-premises domain in the shared services account vpc. configure the inbound endpoint to use the ip addresses of the dns servers for the on-premises domain. configure aws resource access manager (aws ram) to share the route 53 resolver rule to all accounts in the organization. associate the route 53 resolver rule with each vpc



422, a company uses an on-premises data analytics platform. the system is highly available in a fully redundant configuration across 12 servers in the company's data center. the system runs scheduled jobs, both hourly and daily, in addition to one-time requests from users. scheduled jobs can take between 20 minutes and 2 hours to finish running and have tight slas. the scheduled jobs account for 65% of the system usage. user jobs typically finish running in less than 5 minutes and have no sla. the user jobs account for 35% of system usage. during system failures, scheduled jobs must continue to meet slas. however, user jobs can be delayed. a solutions architect needs to move the system to amazon ec2instances and adopt a consumption-based model to reduce costs with no long-term commitments. the solution must maintain high availability and must not affect the slas.which solution will meet these requirements most cost-effectively?
A.split the 12 instances across two availability zones in the chosen aws region. run two instances in each availability zone as on-demand instances with capacity reservations. run four instances in each availability zone as spot instances.
B.split the 12 instances across three availability zones in the chosen aws region. in one of the availability zones, runall four instances as on-demand instances with capacity reservations. run the remaining instances as spot instances.
C.split the 12 instances across three availability zones in the chosen aws region. run two instances in each availability zone as on-demand instances with a savings plan. run two instances in each availability zone as spot instances.
D.split the 12 instances across three availability zones in the chosen aws region. run three instances in each availability zone as on-demand instances with capacity reservations. run one instance in each availability zone as a spot instance.

Mtfr: 1/3odone-time request, 2/3spotscheduled job.


423, a company has a multi-tier web application that runs on a fleet of amazon ec2 instances behind an application load balancer (alb). the instances are in an auto scaling

group. the alb and the auto scaling group are replicated in a backup aws region. the minimum value and the maximum value for the auto scaling group are set to zero.an amazon rds multi-az db instance stores the application's data. the db instance has a read replica in the backup region the application presents an endpoint to end users by using an amazon route 53 record.the company needs to reduce its rto to less than 15 minutes by giving the application the ability to automatically fail over to the backup region. the company does not have a large enough budget for an active- active strategywhat should a solutions architect recommend to meet these requirements?
A.reconfigure the application's route 53 record with a latency-based routing policy that load balances traffic between the two albs. create an aws lambda function in the backup region to promote the read replica and modify the auto scaling group values. create an amazon cloudwatch alarm that is based on the httpcode_target_5xx count metric for the alb in the primary region. configure the cloudwatch alarm to invoke the lambda function
B.create an aws lambda function in the backup region to promote the read replica and modify the auto scaling group values. configure route 53 with a health check that monitors the web application and sends an amazon simple notification service (amazon sns) notification to the lambda function when the health check status is unhealthy. update the application's route 53 record with a failover policy that routes traffic to the alb in the backup region when a health check failure occurs
C.configure the auto scaling group in the backup region to have the same values as the auto scaling group in the primary region. reconfigure the application's route 53 record with a latency-based routing policy that load balances traffic between the two albs. remove the read replica. replace the read replica with a standalone rds db instance. confiaure cross­ reaion replication between the rds db instances by using snapshots and amazon s3
D.configure an endpoint in aws global accelerator with the two albs as equal weighted targets. create an aws lambda function in the backup region to promote the read replica and modify the auto scaling group values. create an amazon cloudwatch alarm that is based on the httpcode_target_5xx_count metric for the alb in the primary region. configure the cloudwatch alarm to invoke the lambda function.




424, a solutions architect is auditing the security setup of an aws lambda function for a company. the lambda function retrieves the latest changes from an amazon aurora database. the lambda function and the database run in the same vpc. lambda environment variables are providing the database credentials to the lambda function.the lambda function aggregates data and makes the data available in an amazon s3 bucket that is configured for server-side encryption with aws kms managed encryption keys (sse-kms). the data must
not travel across the internet. if any database credentials become compromised, the

company needs a solution that minimizes the impact of the compromise.what should the solutions architect recommend to meet these requirements?
A.enable iam database authentication on the aurora db cluster. change the iam role for the lambda function to allow the function to access the database by using iam database authentication. deploy a gateway vpc endpoint for amazon s3 in the vpc.enable iam database authentication on the aurora db cluster. change the iam role for the
B.lambda function to allow the function to access the database by using iam database authentication.enforce https on the connection to amazon s3 during data transfers.
C.save the database credentials in aws systems manager parameter store. set up password rotation on the credentials in parameter store. change the iam role for the lambda function to allow the function to access parameter store. modify the lambda function to retrieve the credentials from parameter store.deploy a gateway vpc endpoint for amazon s3 in the vpc.
D.save the database credentials in aws secrets manager. set up password rotation on the credentials in secrets manager. change the iam role for the lambda function to allow the function to access secrets manager. modify the lambda function to retrieve the credentials from secrets manager. enforce https on the connection to amazon s3 during data transfers.




425, a retail company has structured its aws accounts to be part of an organization in aws organizations. the company has set up consolidated billing and has mapped its departments to the following ous: finance sales, human resources (hr), marketing, and operations. each ou has multiple aws accounts, one for each environment within a department. these environments are development. test.pre-production. and production.the hr department is releasing a new system that will launch in 3 months. in preparation. the hr department has purchased several reserved instances (ris) in its production aws account. the hr department will install the new application on this account. the hr department wants to make sure that other departments cannot share the ri discountswhich solution will meet these requirements?
A.in the aws billing and cost management console for the hr department's production account, turn off ri sharing
B.remove the hr department's production aws account from the organization. add the account to the consolidating billing configuration only
C.in the aws billing and cost management console, use the organization's management account to turn off ri sharing for the hr department's production aws account
D.create an scp in the organization to restrict access to the ris. apply the scp to the ous of the other departments





426, a company has migrated its forms-processing application to aws. when users interact with the application, they upload scanned forms as files through a web application. a database stores user metadata and references to files that are stored in amazon s3. the web application runs on amazon ec2 instances and an amazon rds for postgresql database.when forms are uploaded, the application sends notifications to a team through amazon simple notification service (amazon sns).a team member then logs in and processes each form. the team member performs data validation on the form and extracts relevant data before entering the information into another system that uses an api.a solutions architect needs to automate the manual processing of the forms. the solution must provide accurate form extraction, minimize time to market, and minimize long-term operational overhead.which solution will meet these requirements?
A.develop custom libraries to perform optical character recognition (ocr) on the forms. deploy the libraries to an amazon elastic kubemetes service (amazon eks) cluster as an application tier. use this tier to process the forms when forms are uploaded. store the output in amazon s3. parse this output by extracting the data into an amazon dynamodbtable. submit the data to the target system's api. host the new application tier on ec2 instances.
B.extend the system with an application tier that uses aws step functions and aws lambda. configure this tier to use artificial intelligence and machine learning (al/ml) models that are trained and hosted on an ec2 instance to perform optical character recognition (ocr) on the forms when forms are uploaded.store the output in amazon s3. parse this output by extracting the data that is required within the application tier. submit the data to the target system's api.
C.host a new application tier on ec2 instances. use this tier to call endpoints that host artificial intelligence and machine learning (aljml) models that are trained and hosted in amazon sagemaker to perform optical character recognition (ocr) on the forms. store the output in amazon elasticache.parse this output by extracting the data that is required within the application tier. submit the data to the target system's api.
D.extend the system with an application tier that uses aws step functions and aws lambda. configure this tier to use amazon textract and amazon comprehend to perform optical character recognition (ocr) on the forms when forms are uploaded. store the output in amazon s3. parse this output by extracting the data that is required within the application tier. submit the data to the target system's api


427, a company that has multiple aws accounts is using aws organizations. the company's aws accounts host vpcs,amazon ec2instances, and containers.the company's compliance team has deployed a security tool in each vpc where the company has deployments. the security tools run on ec2instances and send information to the aws account that is dedicated for the compliance team. the company has tagged all the compliance-related resources with a key of "costcenter" and a value of "compliance.",the company wants to identify the cost of the security tools that are running on the ec2 instances so that the company can charge the compliance team's aws account. the costcalculation must be as accurate as possible.what should a solutions architect do to meet these requirements?
A.in the management account of the organization, activate the costcenter user-defined tag. configure monthly aws cost and usage reports to save to an amazon s3 bucket in the management account.use the tag breakdown in the report to obtain the total cost for the costcenter tagged resources.
B.in the member accounts of the organization, activate the costcenter user-defined tag. configure monthly aws costand usage reports to save to an amazon s3 bucket in the management account.schedule a monthly aws lambda function to retrieve the reports and calculate the total costfor the costcenter tagged resources.
C.in the member accounts of the organization, activate the costcenter user-defined tag. from the management account, schedule a monthly aws cost and usage report. use the tag breakdown in the report to calculate the total cost for the costcenter tagged resources.
D.create a custom report in the organization view in aws trusted advisor. configure the report to generate a monthly billing summary for the costcenter tagged resources in the compliance team's aws account.



428, a company is running a traditional web application on amazon ec2 instances. the company needs to refactor the application as microservices that run on containers. separate versions of the application exist in two distinct environments: production and testing. load for the application is variable, but the minimum load and the maximum load are known.a solutions architect needs to design the updated application with a serverless architecture that minimizes operational complexity. which solution will meet these requirements most cost- effectively?
A.upload the container images to aws lambda as functions. configure a concurrency limit for the associated lambda functions to handle the expected peak load. configure two separate lambda integrations within amazon api gateway: one for production and one for testing.
B.upload the container images to amazon elastic container registry (amazon ecr). configure two auto scaled amazon elastic container service (amazon ecs) clusters with the fargate

launch type to handle the expected load. deploy tasks from the ecr images. configure two separate application load balancers to direct traffic to the ecs clusters.
C.upload the container images to amazon elastic container registry (amazon ecr). configure two auto scaled amazon elastic kubernetes service (amazon eks) clusters with the fargate launch type to handle the expected load. deploy tasks from the ecr images. configure two separate application load balancers to direct traic to the eks clusters.
D.upload the container images to aws elastic beanstalk. in elastic beanstalk, create separate environments and deployments for production and testing. configure two separate application load balancers to direct traffic to the elastic beanstalk deployments




429, a company recently migrated a web application from an on-premises data center to the aws cloud. the web application infrastructure consists of an amazon cloudfront distribution that routes to an application load balancer (alb), with amazon elastic container service (amazon ecs) to process requests. a recent security audit revealed that the web application is accessible by using both cloudfront and alb endpoints. however, the company requires that the web application must be accessible only by using the cloudfront endpoint.which solution will meet this requirement with the least amount of effort?
A.create a new security group and attach it to the cloudfront distribution. update the alb security group ingress to allow access only from the cloudfront security group
B.update alb security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing cloudfront managed prefix list
C.create a com.amazonaws.region.elasticloadbalancing vpc interface endpoint for elastic load balancing. update the alb scheme from internet-facing to internal
D.extract cloudfront ips from the aws provided ip-ranges json document. update alb security group ingress to allow access only from cloudfront ips




430, a company is running an application in the aws cloud. the application collects and stores alarge amount of unstructured data in an amazon s3 bucket. the s3 bucket contains several terabytes of data and uses the s3 standard storage class. the data increases in size by several gigabytes every day. the company needs to query and analyze the data. the company does not access data that is more than 1 year old. however, the company must retain all the data indefinitely for compliance reasons. which solution will meet these requirements most cost-effectively?

A.use s3 select to query the data. create an s3lifecycle policy to transition data that is more than 1 year old to s3 glacier deep archive.
B.use amazon redshift spectrum to query the data. create an s3 lifecycle policy to transition data that is more than 1 year old to s3 glacier deep archive.
C.use an aws glue data catalog and amazon athena to query the data. create an s3 lifecycle policy to transition data that is more than 1 year old to s3 glacier deep archive.
D.use amazon redshift spectrum to query the data. create an s3 lifecycle policy to transition data that is more than 1 year old to s3 intelligent-tiering




43L  a company has its cloud infrastructure on aws. a solutions architect needs to define the infrastructure as code. the infrastructure is currently deployed in one aws region.the company's business expansion plan includes deployments in multiple regions across multiple aws accounts. what should the solutions architect do to meet these requirements?
A.use aws cloudformation templates. add iam policies to control the various accounts. deploy the templates across the multiple regions
B.use aws organizations. deploy aws cloudformation templates from the management account. use aws control tower to manage deployments across accounts
C.use aws organizations and aws cloudformation stacksets. deploy a cloudformation template from an account that has the necessary iam permissions
D.use nested stacks with aws cloudformation templates. change the region by using nested stacks




432, a company has developed apis that use amazon api gateway with regional endpoints. the apis call aws lambda functions that use api gateway authentication mechanisms. after a design review, a solutions architect identifies a set of apis that do not require public access. the solutions architect must design a solution to make the set of apis accessible only from a vpc. all apis need to be called with an authenticated user.which solution will meet these requirements with the least amount of effort?
A.create an internal application load balancer (alb). create a target group. select the lambda function to call. use the alb dns name to call the api from the vpc

B.remove the dns entry that is associated with the api in api gateway. create a hosted zone in amazon route 53. create a cname record in the hosted zone. update the api in api gateway with the cname record. use the cname record to call the api from the vpc
C.update the api endpoint from regional to private in api gateway. create an interface vpc endpoint in the vpc. create a resource policy, and attach it to the api. use the vpc endpoint to call the api from the vpc
D.deploy the lambda functions inside the vpc. provision an ec2 instance,and install an apache server.from the apache server, call the lambda functions. use the internal cname record of the ec2 instance to call the api from the vpc




433, a company's solutions architect is reviewing a web application that runs on aws. the application references static assets in an amazon s3 bucket in the us-east-1 region. the company needs resiliency across multiple aws regions. the company already has created an s3 bucket in a second region. which solution will meet these requirements with the least operational overhead?
A.configure the application to write each object to both s3 buckets. set up an amazon route 53public hosted zone with a record set by using a weighted routing policy for each s3bucket. configure the application to reference the objects by using the route 53 dns name.
B.create an aws lambda function to copy objects from the s3 bucket in us-east-lto the s3 bucket in the second region. invoke the lambda function each time an object is written to the s3 bucket in us-east-1.set up an amazon cloudfront distribution with an origin group that contains the two s3 buckets as origins.
C.configure replication on the s3 bucket in us-east-lto replicate objects to the s3 bucket in the second region. set up an amazon cloudfront distribution with an origin group that contains the two s3 buckets as origins.
D.configure replication on the s3 bucket in us-east-lto replicate objects to the s3 bucket in the second region. if failover is required, update the application code to load s3 objects from the s3 bucket in the second region.




434, a company has created an amazon cloudfront distribution with two amazon s3 buckets as origins. the company discovers that objects in both s3 buckets are publicly accessible. the desired state for the first s3 bucket is to allow access through cloudfront and other aws resources with appropriate permissions. objects in the second s3 bucket should

be accessible only through cloudfront for all users except for the s3 bucket owner.how should a solutions architect configure access to the buckets to meet these requirements?
A.create an iam policy and an iam role that allow access to the first s3 bucket. assign the role to the cloudfront distribution and the other resources that need access to the first s3bucket. create an origin access identity (oai) for the cloudfront distribution. create an s3 bucket policy for the the second s3 bucket that allows access only with the oai as the principal.
B.create a separate s3 bucket policy for each s3 bucket. configure the policy for the first s3 bucket to allow read access to appropriate aws resources and the cloudfront distribution as principals.configure the policy for the second s3 bucket to allow only read access for the cloudfront distribution as the principal.
C.create a separate origin access identity (oai) for the cloudfront distribution for each s3 bucket.configure the first oai to include access for the other aws resources configure the second oai to include only cloudfront. update the s3 bucket policies to restrict access to the correct oais.
D.create a separate origin access identity (oai) for the cloudfront distribution for each s3 bucket. create an s3 bucket policy for the first s3 bucket that allows access to the appropriate aws resources and the oai as principals create an s3bucket policy for the the second s3 bucket that allows access to the appropriate oai as the principal.




435, a company is planning to store a large number of archived documents and make the documents available to employees through the corporate intranet employees will access the system by connecting through a client vpn service that is attached to a vpc. the data must not be accessible to the public. the documents that the company is storing are copies of data that is held on physical media elsewhere. the number of requests will be low availability and speed of retrieval are not concerns of the company. which solution will meet these requirements at the lowest cost?
A.create an amazon s3 bucket configure the s3 bucket to use the s3 one zone-infrequent access (s3 one zone-ia) storage class as default. configure the s3 bucket for website hosting. create an s3 interface endpoint. configure the s3 bucket to allow access only through that endpoint
B.launch an amazon ec2 instance that runs a web server. attach an amazon elastic file system (amazon efs) file system to store the archived data in the efs one zone-infrequent access (efs one zone-ia) storage class. configure the instance security groups to allow access only from private networks

C.launch an amazon ec2 instance that runs a web server. attach an amazon elastic block store (amazon ebs) volume to store the archived data use the cold hdd (scl) volume type. configure the instance security groups to allow access only from private networks
D.create an amazon s3 bucket. configure the s3 bucket to use the s3 glacier deep archive storage class as default configure the s3 bucket for website hosting. create an s3interface endpoint. configure the s3 bucket to allow access only through that endpoint




436, a company has a serverless application that is deployed on aws. the application uses an amazon api gateway rest api and aws lambda to receive and process requests from other applications within the company's on-premises network. the application uses a preshared api key as the authentication method. a recent security review showed that the application was accessible from anywhere on the internet. the company's security policy states that requests can be accepted only from the company's on- premises network.what should a solutions architect recommend to meet this requirement?
A.configure a security group with rules to allow traffic only from within the company's public ip address range.attach the security group to the api gateway api. and redeploy the api
B.create a lambda function to inspect the requests and deny the execute-api:invoke action if the request is not from within the company's public ip address range configure the lambda function as a custom authorizer for the api gateway api redeploy the api
C.create a resource policy with a statement to deny the execute-api:invoke action if the aws:sourcelp attribute is not from within the company's public ip address range attach that resource policy to the api gateway api redeploy the api.
D.configure a request validator for api gateway to inspect the requests and deny the execute-api invoke action if the aws:sourcelp attribute is not from within the company's public ip address range redeploy the api gateway api


fAIHfr: modify on a>b



437, a company manages hundreds of aws accounts centrally in an organization in aws organizations. the company recently started to allow product teams to create and manage their own s3 access points in their accounts. the s3 access points can be accessed only within vpcs, not on the internet. what is the mostoperationally efficient way to enforce this requirement?

A.set the s3 access point resource policy to deny the s3:createaccesspoint action unless the s3:accesspointnetworkorigin condition key evaluates to vpc.
B.create an scp at the root level in the organization to deny the s3:createaccesspoint action unless the s3:accesspointnetworkorigin condition key evaluates to vpc.
C.use aws cloudformation stacksets to create a new iam policy in each aws account that allows the s3:createaccesspoint action only if the s3:accesspointnetworkorigin condition key evaluates to vpc.
D.set the s3 bucket policy to deny the s3:createaccesspoint action unless the s3:accesspointnetworkorigin condition key evaluates to vpc.

ffllHfr: https:/ / aws.amazon.com/blogs/storage /managing-amazon-s3-access-with-vpc­ endpoints-and-s3-access-points/"you can set up aws scps to require any new access point in the organization to be restricted to vpc-only type. this makes sure that any access point created in your organization provides access only from within the vpcs and there by firewalling your data to within your private networks."https://aws.amazon.com/blogs/storage/managing-amazon-s3-access-with-vpc­ endpoints-and-s3-access-points/


438, a research company is running daily simulations in the aws cloud to meet high demand. the simulations run on several hundred amazon ec2 instances that are based on amazon linux 2 occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an ec2 instance through ssh. company policy states that no ec2 instance can use the same ssh key and that all connections must be logged in aws cloudtrail.how can a solutions architect meet these requirements?
A.launch new ec2 instances, and generate an individual ssh key for each instance store the ssh key in aws secretsmanager.create a new iam policy, and attach it to the engineers' iam role with an allow statement for the getsecretvalue action.instruct the engineers to fetch the ssh key from secrets manager when they connect through any ssh client.
B.create an aws systems manager document to run commands on ec2 instances to set a new unique ssh key.create a new iam policy, and attach it to the engineers' iam rote with an allow statement to run systems manager documents.instruct the engineers to run the document to set an ssh key and to connect through any ssh client
C.launch new ec2 instances without setting up any ssh key for the instances set up ec2 instance connect on each instance.create a new iam policy, and attach it to the engineers' iam role with an allow statement for the sendsshpublickey action.instruct the engineers to connect to the instance by using a browser-based ssh client from the ec2 console.

D.set up aws secrets manager to store the ec2 ssh key.create a new aws lambda function to create a new ssh key and to call aws systems manager session manager to set the ssh key on the ec2 instance configure secrets manager to use the lambda function for automatic rotation once daily instruct the engineers to fetch the ssh key from secrets manager when they connect through any ssh client.

ffllHJr: modify on a>d


439, a company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. the backend services run on a pair of amazon ec2 instances behind an application load balancer with amazon dynamodb as the datastore application read and write traffic is slow during peak seasons which option provides a scalable application architecture to handle peak seasons with the least development effort?
A.migrate the backend services to aws lambda.increase the read and write capacity of dynamodb
B.migrate the backend services to aws lambda.configure dynamodb to use global tables.
C.use auto scaling groups for the backend services.use dynamodb auto scaling.
D.use auto scaling groups for the backend services.use amazon simple queue service (amazon sqs) and an aws lambda function to write to dynamodb.

tfr: key: least development work


440, a large company is running a popular web application. the application runs on several amazon ec2 linux instances in an auto scaling group in a private subnet.an application load balancer is targeting the instances in the auto scaling group in the private subnet. aws systems manager session manager is configured, and aws systems manager agent is running on all the ec2 instances.the company recently released a new version of the application some ec2 instances are now being marked as unhealthy and are being terminated.as a result, the application is running at reduced capacity. a solutions architect tries to determine the root cause by analyzing amazon cloudwatch logs that are collected from the application, but the logs are inconclusive.how should the solutions architect gam access to an ec2 instance to troubleshoot the issue?
A.suspend the auto scaling group's healthcheck scaling process.use session manager to log in to an instance that is marked as unhealthy

B.enable ec2 instance termination protection.use session manager to log in to an instance that is marked as unhealthy.
C.set the termination policy to oldestlnstance on the auto scaling group use session manager to log in to an instance that is marked as unhealthy.
D.suspend the auto scaling group's terminate process.use session manager to log in to an instance that is marked as unhealthy.

Mi9T: o because it will prevent termination of instances that are already marked unhealthy.  https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume­ processes.html#choosing-suspend-resume


44L	a financial services company loaded millions of historical stock trades into an amazon dynamodb table the table uses on-demand capacity mode once each day at midnight. a few million new records are loaded intothe table application road activity against the table happens in bursts throughout the day. and a limited set of keys are repeatedly looked up the company needs to notice costs associated with dynamodb. which strategy should a solutions architect recommend to meet this requirement?
A.deploy an amazon elasticache cluster in front of the dynamodb table.
B.deploy dynamodb accelerator (dax) configure dynamodb auto scaling purchase savings flans in cost explorer.
C.use provisioned capacity mode purchase savings plans in cost explorer.
D.deploy dynamodb accelerator (dax). use provisioned capacity mode. configure dynamodb auto scaling.

It-tfr: modify on a>d


442, a company wants to retire its oracle solans nfs storage arrays. the company requires rapid data migration over its internet network connection to a combination of destinations for amazon s3, amazon elastic file system (amazon efs), and amazon fsx for windows file server. the company also requires a full initial copy, as well as incremental transfers of changes until the retirement of the storage arrays. all data must be encrypted and checked for integrity. what should a solutions architect recommend to meet these requirements?

A.configure cloudendure.create a project and deploy the cloudendure agent and token to the storage array run the migration plan to start the transfer.
B.configure aws datasync.configure the datasync agent and deploy it to the local network create a transfer task and start the transfer.
C.configure the aws s3 sync command.configure the aws client on the client side with credentials run the sync command to start the transfer.
D.configure aws transfer for ftp.configure the ftp client with credentials script the client lo connect and sync to start the transfer


tfr: modify on a>b


443, a company has application services that have been containerized and deployed on multiple amazon ec2 instances with public ips.an apache kafka cluster has been deployed to the ec2 instances. a postgresql database has been migrated to amazon rds for postgresql the company expects a significant increase of orders on its platform when a new version of is flagship product is released. what changes to the current architecture will reduce operational overhead and support the product release?
A.create an ec2 auto scaling group behind an application load balancer.create additional read replicas for the db instance.create amazon kinesis data streams and configure the application services to use the data streams. store and serve static content directly from amazon s3.
B.create an ec2 auto scaling group behind an application load balancer.deploy the db instance in multi-az mode and enable storage auto sea ing.create amazon kinesis data streams and configure the application services to use the data streams. store and serve static content directly from amazon s3.
C.deploy the application on a kubernetes cluster created on the ec2 instances behind an application load balancer.deploy the db instance in multi-az mode and enable storage auto scaling.create an amazon managed streaming for apache kafka cluster and configure the application services to use the cluster.store static content in amazon s3 behind an amazon cloudfront distribution.
D.deploy the application on amazon elastic kubernetes service (amazon eks) with aws fargate and enable auto scaling behind an application load balancer.create additional read replicas for the db instance.create an amazon managed streaming for apache kafka cluster and configure the application services to use the cluster.store static content in amazon s3 behind an amazon cloudfront distribution.





444, a company is running a workload that consists of thousands of amazon ec2 instances. the workload is running in a vpc that contains several public subnets and private subnets. the public subnets have a route fo 000 0/0 to an existing internet gateway. the private subnets have a route for 0.0.0.0/0 to an existing nat gateway. a solutions architect needs to migrate the entire fleet of ec2 instances to use ipv6. the ec2 instances that are in private subnets must not be accessible from the public internet. what should the solutions architect do to meet these requirements?
A.update the existing vpc, and associate a custom ipv6 cidr block with the vpc and all subnets update all the vpc route tables, and add a route tor 70 to the internet gateway.
B.update the existing vpc, and associate an amazon-provided ipv6 cidr block with the vpc and all subnets update the vpc route tables for all private subnets, and add a route for :/0 to the nat gateway
C.update the existing vpc, and associate an amazon-provided ipv6 cidr block with the vpc and all subnets create an egress-only internet gatewayupdate the vpc route tables for all private subnets, and add a route for :/0 to the egress-only internet gateway.
D.update the existing vpc, and associate a custom ipv6 cidr block with the vpc and all subnets create a new nat gateway, and enable ipv6 supportupdate the vpc route tables for all private subnets, and add a route for: 70 to the ipv6-enabled nat gateway


fAIHfr: modify on d>c



445, a company operates quick-service restaurants.the restaurants follow a predictable model with high sales traffic for 4 hours daily. sales traffic is lower outsize of those peak hours.-the point of sale and management platform is deployed in the aws cloud and has a backend that is based on amazon dynamodb.-the database table uses provisioned throughput mode with 100,000 rcus and 80,000 wcus to match known peak resource consumption.-the company wants to reduce its dynamodb cost and minimize the operational overhead for the it staff. which solution meets these requirements most cost­ effectively?
A.reduce the provisioned rcus and wcus
B.change the dynamoob table to use on-demand capacity
C.enable dyn mo db auto scalling for the table

D.purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day,


tfr: modify on a>c


446, a company hosts a web application on aws that uses amazon rds for mysql multi-az db instances. usage of the web application has increased recently. users have indicated that dynamic reports in the application load slowly. which configuration change will improve
application performance while ensuring the database is highly available for data operations? A.add a read replica and configure the application to direct read requests to it
B.configure the application to direct read requests to the primary and standby db instances
C.create two read replicas in the same availability zone as the primary db instance use amazon route 53 to evenly distribute read requests to the replicas
D.migrate to amazon aurora mysql with two aurora replicas in different availability zones configure the application to direct read requests to the reader endpoint



447, a company has an application that sends newsletters through email to users. the application runs on two amazon ec2 instances in a vpc. the first ec2 instance contains the email application that sends email directly to users. the second ec2 instance contains a mysql database that is heavily dependent upon relational data each ec instance is controlled by its own auto scaling group with a minimum and maximum of one instance management wants improved application reliability and support for personalized email. which set of steps should a solutions architect take to meet these requirements?
A.migrate the database to amazon dynamodb global tables reconfigure the email application to use amazon simple email service (amazon ses) to send email
B.migrate the database to an amazon aurora mysql db cluster with aurora replicas.reconfigure the email application to use amazon simple notification service (amazon sns) to send email
(.increase the minimum number of ec2 instances in the auto scaling group to three reconfigure the email application to use amazon simple notification service (amazon sns) to send email

D.migrate the database to an amazon rds mysql multi-az db instance reconfigure the email application to use amazon pinpoint to send email


tfr: modify on b>d


448, a company wants to migrate an application to amazon ec2 from vmware infrastructure that runs in an on- premises data center.a solutions architect must preserve the software and configuration settings during the migration. what should the solutions architect do to meet these requirements?
A.configure the aws datasync agent to start replicating the data store to amazon fsx for windows file server.use the smb share to host the vmware data store.use vm import/export to move the vms to amazon ec2.
B.use the vmware vsphere client to export the application as an image in open visualization format (ovf) format.create an amazon s3 bucket to store the image in the destination aws region. create and apply an iam role for vm import.use the aws di to run the ec2 import command.
C.configure aws storage gateway for files service to export a common internet file system (cifs) share.create a backup copy to the shared folder.sign in to the aws management console and create an ami from the backup copy. launch an ec2 instance that is based on the ami.
D.create a managed-instance activation for a hybrid environment in aws systems manager.download and install systems manager agent on the on-premises vm. register the vm with systems manager to be a managed instance.use aws backup to create a snapshot of the vm and create an ami. launch an ec2 instance that is based on the ami.



449, a company has an on-premises microsoft sql server database that writes a nightly 200 gb export to a local drive.the company wants to move the backups to more robust cloud storage on amazon s3. the company has set up a 10 gbps aws direct connect connection between the on-premises data center and aws. which solution meets these requirements most cost effectively?
A.create a new s3 bucket deploy an aws storage gateway file gateway within the vpc that is connected to the direct connect connection.create a new smb file share.write nightly database exports to the new smb file share.

B.create an amzon fsx for windows file server single-az file system within the vpc that is connected to the direct connect connection.create a new smb file share.write nightly database exports to an smb file share on the amazon fsx file system enable backups.
C.create an amazon fsx for windows file server multi-az system within the vpc that is connected to the direct connect connection.create a new smb file share.write nightly database exports to an smb file share on the amazon fsx file system. enable nightly backups.
D.create a new s3 buckets deploy an aws storage gateway volume gateway within the vpc that is connected to the direct connect connection.create a new smb file share.write nightly database exports to the new smb file share on the volume gateway, and automate copies of this data to an s3 bucket.



450, a company has a project that is launching amazon ec2 instances that arc larger than required. the project's account cannot be part of the company's organization in aws organizations due to policy restrictions to keep this activity outside of corporate it. the company wants to allow only the launch of t3.small ec2 instances by developers in the project's account. these ec2 instances must be restricted the us-east-2 region.what should a solutions architect do to meet these requirements?
A.create a new developer account.move all ec2 instances, users, and assets into us-east-
2.add the account to the company's organization in aws organization. enforce a tagging policy that denotes region affinity
B.create an scp that denies the launch of all ec2 instances except t3.small ec2 instances in us- east-2.attach the scp to the project's account
C.create and purchase a t3.small ec2 reserved instance for each developer in us-east-
2.assign each developer a specific ec2 instance with their name as the tag.
D.create an iam policy than allows the launch of only t3.small ec2 instances in us-east-
2.attach the policy to the roles and groups that the developers use in the proofs account



45L	a company runs an iot platform on aws. iot sensors in various locations send data to the company's node.js api servers on amazon ec2 instances running behind an application load balancer. the data is stored in an amazon rds mysql db instance that uses a 4 tb general purpose ssd volume. the number of sensors the company has deployed in the field has increased over time, and is expected to grow significantly. the api servers are consistently overloaded and rds metrics show high write latency. which of the following steps together

will resolve the issues permanently and enable growth as new sensors are provisioned, while keeping this platform cost-efficient? (choose two.)
A. resize the mysql general purpose ssd storage to 6 tb to improve the volume's iops
B. re-architect the database tier to use amazon aurora instead of an rds mysql db instance and add read replicas
(.leverage amazon kinesis data streams and aws lambda to ingest and process the raw data
D. use aws-x-ray to fft tfr: and debug application issues and add more api servers to match the load
E. re-architect the database tier to use amazon dynamodb instead of an rds mysql db instance

Mtfr: a resize to 6th will change the iops performance from 12288 to 16000, but 16000 will be the maximum io a general pupose ssd (gp2) can get to. therefore, this will not solve the issue permanently. b theoretically aurora should not have iops issue, however it still have maximum size limit of 64tb ca stream is good fit for data processing like thisd dynamo has no maximum data size limit, which is a good fit for this


452, a public retail web application uses an application load balancer (alb) in front of amazon ec2 instances running across multiple availability zones (azs) in a region backed by an amazon rds mysql multi-az deployment. target group health checks are configured to use http and pointed at the product catalog page. auto scaling is configured to maintain the web fleet size based on the alb health check. recently, the application experienced an outage. auto scaling continuously replaced the instances during the outage. a subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times. which of the following changes together would re mediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (select two.)
A. configure read replicas for amazon rds mysql and use the single reader endpoint in the web application to reduce the load on the backend database tier.
B. configure the target group health check to point at a simple html page instead of a product catalog page and the amazon route 53 health check against the product page to evaluate full application functionality. configure amazon cloudwatch alarms to notify administrators when the site fails.
A. 
C. configure the target group health check to use a tcp check of the amazon ec2 web server and the amazon route 53 health check against the product page to evaluate full application functionality.configure amazon cloudwatch alarms to notify administrators when the site fails.
D. configure an amazon cloudwatch alarm for amazon rds with an action to recover a high­ load, impaired rds instance in the database tier.
E. configure an amazon elasticache cluster and place it between the web application and rds mysql instances to reduce the load on the backend database tier.

 It-tfr: b full stack health check on route 53 level can reduce the number of requests a lot as we don't need to check each instancec target group health check for alb need to be http or https. for tcp you will need a nib https:// docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health­ checks.html https:// docs.aws.amazon.com/elasticloadbalancing/latest/network/target­ group-health-checks.html


453, a company has been using a third-party provider for its content delivery network and recently decided to switch to amazon cloudfront the development team wants to maximize performance for the global user base. the company uses a content management system (ems) that serves both static and dynamic content. the ems is both md an application load
balancer (alb) which is set as the default origin for the distribution. static assets are served from an amazon s3 bucket. the origin access identity (oai) was created property d the s3 bucket policy has been updated to allow the getobject action from the oai, but static assets are receiving a 404 error which combination of steps should the solutions architect take to fix the error? (select two.)
A. add another origin to the cloudfront distribution for the static assets
B. add a path based rule to the alb to forward requests for the static assets
C. add an rtmp distribution to allow caching of both static and dynamic content
D. add a behavior to the cloudfront distribution for the path pattern and the origin of the static assets
E. add a host header condition to the alb listener and forward the header from cloudfront to add traffic to the allow list


 tfr: ad allows us to store static content on s3 and have cloudfront assess it directly. add an origin and a behavior.https://aws.amazon.com/premiumsupport/knowledge-
center/ cloudfront-distribution-serve-content/


454, an it company owns a web product in aws that provides discount restaurant information to customers. it has used one s3 bucket (my_bucket) to store restaurant data such as pictures, menus, etc. the product is deployed in vpc subnets. the company's cloud architect decides to configure a vpc endpoint for this s3 bucket so that the performance will be enhanced. to be compliance to security rules, it is required that the new vpc endpoint is only used to communicate with this specific s3 bucket and on the other hand, the s3 bucket only allows the read/write operations coming from this vpc endpoint. which two options should the cloud architect choose to meet the security needs?
A. use a vpc endpoint policy for amazon s3 to restrict access to the s3 bucket "my_bucket" so that the vpc endpoint is only allowed to perform s3 actions on "my_bucket".
B. modify the security group of the ec2 instance to limit the outbound actions to the vpc endpoint if the outgoing traffic destination is the s3 bucket "my_bucket"'.
C. in the s3 bucket "my_bucket", add a s3 bucket policy in which all actions are denied if the source ip address is not equal to the ec2 public ip (use "notipaddress" condition).
D. for the s3 bucket "my_bucket", use a s3 bucket policy that denies all actions if the source vpc endpoint is no equal to the endpoint id that is created.s
E. create a s3 bucket policy in the s3 bucket "my_bucket" which denies all actions unless the source ip address is equal to the ec2 public ip (use "ipaddress" condition).




455, a financial company is using a high-performance compute cluster running on amazon ec2 instances to perform market simulations a dns record must be created in an amazon route 53 private hosted zone when instances start the dns record must be removed after instances are terminated. currently the company uses a combination of amazon ctoudwatch events and aws lambda to create the dns record. the solution worked well in testing with small clusters, but in production with clusters containing thousands of instances the company sees the following error in the lambda logs:-http 400 error (bad request).-the response header also includes a status code element with a value of "throttling" and a status message element with a value of "rate exceeded".which combination of steps should the solutions architect take to resolve these issues? (select three)

A. configure an amazon sos fifo queue and configure a cloudwatch events rule to use this queue as a target. remove the lambda target from the cloudwatch events rule
B. configure an amazon kinesis data stream and configure a cloudwatch events rule to use this queue as a target remove the lambda target from the cloudwatch events rule
C. update the cloudwatch events rule to trigger on amazon ec2 "instance launch successful" and "instance terminate successful" events for the auto scaling group used by the cluster
D. configure a lambda function to retrieve messages from an amazon sqs queue modify the lambda function to retrieve a maximum of 10 messages then batch the messages by amazon route 53 api call type and submit delete the messages from the sqs queue after successful api calls.
E. configure an amazon sqs standard queue and configure the existing cloudwatch events rule to use this queue as a target remove the lambda target from the cloudwatch events rule.
F. configure a lambda function to read data from the amazon kinesis data stream and configure the batch window to 5 minutes modify the function to make a single api call to amazon route 53 with all records read from the kinesis data stream


 tfr: if use amazon sqs standard queue, might have duplicated request, this will failed the lambda task when proceed the same second request, and it will be accumulate in the queue and fail the lambda overall as 10 message maximum had set.


456, a large trading company is using an on-premise system to	tfr: the trade data. after the trading day closes, the data including the day's transaction costs, execution reporting, and market performance is sent to a redhat server which runs big data analytics tools for predictions for next day trading. a bash script is used to configure resource and schedule when to run the data analytics workloads. how should the on- premise system be migrated to aws with appropriate tools? (select three)
A. create a s3 bucket to store the trade data that is used for post processing.
B. send the trade data from various sources to a dedicated sqs queue.
C. use aws batch to execute the bash script using a proper job definition.
D. create ec2 instances with auto-scaling to handle with the big data analytics workloads.
E. use cloudwatch events to schedule the data analytics jobs.


457, a solutions architect needs to migrate an on-premises legacy application to aws. the application runs on two servers behind a load balancer. the application requires a license file that is associated with the mac address of the server's network adapter. it takes the software vendor 12hours to send new license files. the application also uses configuration files with a static ip address to access a database server. host names are not supported.given these requirements, which combination of steps should be taken to enable highly available architecture for the application servers in aws? (select two)
A. create a pool of enis. request license files from the vendor for the pool, and store the license files in amazon s3 create a bootstrap automation script to download a license file and attach the corresponding eni to an amazon ec2 instance
B. create a pool of enis request license files from the vendor for the pool, store the license files on an amazon ec2instance create an ami from the instance and use this ami for all future ec2 instances
C. create a bootstrap automation script to request a new license file from the vendor when the response is received,apply the license fie to an amazon ec2 instance
D. edit the bootstrap automation script to read the database server ip address from the aws systems manager parameter store, and inject the value into the local configuration files
E. edit an amazon ec2 instance to include the database server ip address in the configuration files and re-create the ami to use for all futureec2 instances


 tfr: eni to resolve the mac address restrictions.having the license files on an amazon s3 bucket reduces the management overhead for the ec2 instances, as you can easily add/remove more license keys if needed.having the database ip addresses on parameter store ensures that all the ec2 instances will have a central location to retrieve the ip addresses. this also reduces the need to constantly update any script from inside the ec2 instance even if you add/remove more databases in the future.


458, api gateway and lambda non-proxy integrations have been chosen to implement an application by a software engineer. the application is a data analysis tool that returns some statistic results when the http endpoint is called. the lambda needs to communicate with some back-end data services such as keen.io however there are chances that error happens such as wrong data requested, bad communications, etc. the lambda is written using java and two exceptions may be returned which are badrequestexception and internalerrorexception. what should the software engineer do to map these two exceptions

in api gateway with proper http return codes? for example, badrequestexception and internalerrorexception are mapped to http return codes 400 and 500 respectively. select 2.
A. add the corresponding error codes (400 and 500) on the integration response in api gateway
B. add the corresponding error codes (400 and 500) on the method response in api gateway.
C. put the mapping logic into lambda itself so that when exception happens, error codes are returned at the same time in a json body.
D. add integration responses where regular expression patterns are set such as badrequest or internalerror. associate them with http status codes
E. add method responses where regular expression patterns are set such as badrequest or internalerror. associate them with http status codes 400 and 500.


f@Hfr: https:/ / docs.aws.amazon.com/apigateway/latest/developerguide/ api-gateway­ method-settings-method-
response.htmlhttps:// docs.aws.amazon.com/apigateway /latest/ developerguide/ api­ gateway-integration-settings- integration- response.html


459, a large company has increased its utilization of aws over time in an unmanaged way. as such, they have a large number of independent aws accounts across different business units, projects, and environments. the company has created a cloud center of excellence team, which is responsible for managing all aspects of the aws cloud, including their aws accounts.which of the following should the cloud center of excellence team do to best address their requirements in a centralized way? (select two.)
A. control all aws account root user credentials. assign aws iam users in the account of each user who needs to access aws resources. follow the policy of least privilege in assigning permissions to each user.
B. tag all aws resources with details about the business unit, project, and environment. send all aws cost and usage reports to a central amazon s3 bucket, and use tools such as amazon athena and amazon quicksight to collect billing details by business unit.
C. use the aws marketplace to choose and deploy a cost management tool. tag all aws resources with details about the business unit, project, and environment. send all aws cost and usage reports for the aws accounts to this tool for analysis.
D. set up aws organizations. enable consolidated billing, and link all existing aws accounts to a master billing account. tag all aws resources with details about the business unit, project
A. 
and environment.analyze cost and usage reports using tools such as amazon athena and amazon quicksight to collect billing details by business unit.
E. using a master aws account, create iam users within the master account. define iam roles in the other aws accounts, which cover each of the required functions in the account. follow the policy of least privilege in assigning permissions to each role, then enable the iam users to assume the roles that they need to use.


 tfr: a - incorrect - users - decentralises management of user accounts whereas e offers centralisation b - incorrect - not really centralising anything, although technically feasible c - incorrect - could do, but much easier using aws services, thus minimising effort needed collect and deliver aws cost and usage reports from other accountsd - correct - makes sense, no further discussion necessary e - correct - makes sense. ideally, you'd define another account as the identity account and host users/ groups there, but this works. its a better answer than a as only roles are defined in a decentralised manner, whereas witha, both users and roles are decentralised.ada: root access is always the holy grail and needs to be protected at all cost.b: while this is correct, it is not complete. aws organization needs to be used. c: this is not needed.d: this has everything we need.e: while this is correct, it's not as scalable as aws organizations.doesn't address billing. accounts & users already exist and no mention of master account. even with the assumption that ma exists, it is too much work


460, a company is running an email application across multiple aws regions. the company uses ohio (us- east-2) as the primary region and northern virginia (us-east-1) as the disaster recovery (dr) region. the data is continuously replicated from the primary region to the dr region by a single instance on the public subnet in both regions. the replication messages between the regions have a significant backlog during certain times of the day. the backlog clears on its own after a short time, but it affects the application's rpo. which of the following solutions should help remediate this performance problem? (select two)
A. increase the size of the instances.
B. have the instance in the primary region write the data to an amazon sqs queue in the primary region instead, and have the instance in the dr region poll from this queue.
C. use multiple instances on the primary and dr regions to send and receive the replication data.
D. change the dr region to oregon (us-west-2) instead of the current dr region.
E. attach an additional elastic network interface to each of the instances in both regions and set up load balancing between the network interfaces.
A. 


Mtfr: b this could be a good solution as it enables scaling on both side



461, a team has just received a task to build an application that needs to recognize faces in streaming videos. they will get the source videos from a third party which use a container format (mkv). the app should be able to quickly address faces through the video in real time and save the output in a suitable manner for downstream to process. as recommended by the aws solutions architect colleague, they would like to develop the service using aws rekognition. which below options are needed to accomplish the task? select 3.
A. s3 buckets to store the source mkv videos for aws rekognition to process. s3 should be used in this case as it has provided an unlimited, highly available and durable storing space.make sure that the third party has the write access to s3 buckets.
B.a kinesis video stream for sending streaming video to amazon rekognition video. this can be done by using kinesis utmedia?api in java sdk. the putmedia operation writes video data fragments into a kinesis video stream that amazon rekognition video consumes.
C. an amazon rekognition video stream processor to manage the analysis of the streaming video. it can be used to start, stop, and manage stream processors according to needs.
D. use ec2 or lambda to call rekognition api etectfaces?with the source videos saved in s3 bucket. for each face detected, the operation returns face details. these details include a bounding box of the face, a confidence value, and a fixed set of attributes such as facial landmarks, etc.
E. after the app has utilized rekognition api to fetch the recognized faces from live videos, use s3 or rds database to store the output from rekognition. another lambda can be used to post-process the result and present to ui.
F. a kinesis data stream consumer to read the analysis results that amazon rekognition video sends to the kinesis data stream. it can be an amazon ec2 instance by adding to one of amazon machine images (amis). the consumer can be autoscaled by running it on multiple amazon ec2 instances under an auto scaling group.


Mtfr:   https:/ / docs.aws.amazon.com/rekognition/latest/ dg/ streaming-video.html



462, a company would like to implement a serverless application by using amazon api gateway, aws lambda and amazon dynamodb. they deployed a proof of concept and stated that the average response time is greater than what their upstream services can accept

amazon cloudwatch metrics did not indicate any issues with dynamodb but showed that some lambda functions were hitting their timeout. which of the following actions should the solutions architect consider to improve performance? (choose two.)
A. configure the aws lambda function to reuse containers to avoid unnecessary startup time.
B. increase the amount of memory and adjust the timeout on the lambda function. complete performance testing to identify the ideal memory and timeout configuration for the lambda function
C. increase the amount of cpu, and adjust the timeout on the lambda function. complete performance testing to identify the ideal cpu and timeout configuration for the lambda function
D. create an amazon elasticache cluster running memcached, and configure the lambda function for vpc integration with access to the amazon elasticache cluster.
E. enable api cache on the appropriate stage in amazon api gateway, and override the ttl for individual methods that require a lower ttl than the entire stage

It-tfr: a no such thing, this is automatic. c this won't help much.you cannot increase cpu for lambda. cpu is related to memory.
https:/ / docs.aws.amazon.com/lambda/latest/ dg/resource-model.html


463, a solutions architect must build a highly available infrastructure for a popular global video game that runs on a mobile phone platform. the application runs on amazon ec2 instances behind an application load balancer. the instances run in an auto scaling group across multiple availability zones. the database tier is an amazon rds mysql. multi-az instance. the entire application stack is deployed in both us-east-1 and eu- central-1. amazon route 53 is used to route traffic to the two installations using a latency-based routing policy. a weighted routing policy is configured in route 53 as a fail over to another region in case the installation in a region becomes unresponsive.during the testing of disaster recovery scenarios, after blocking access to the amazon rds mysql instance in eu­ central-1 from all the application instances running in that region. route 53 does not automatically failover all traffic to us-east-1.based on this situation, which changes would allow the infrastructure to failover to us-east-1? (choose two.)
A. specify a weight of 100 for the record pointing to the primary application load balancer in us-east-1 and a weight of 60 for the pointing to the primary application load balancer in eu­ central-1.
A. 
B. specify a weight of 100 for the record pointing to the primary application load balancer in us-east-1 and a weight of 0 for the record pointing to the primary application load balancer in eu-central-1.
C. set the value of evaluate target health to yes on the latency alias resources for both eu­ central-1 and us-east-1.
D. write a url in the application that performs a health check on the database layer. add it as a health check within the weighted routing policy in both regions.
E. disable any existing health checks for the resources in the policies and set a weight of 0 for the records pointing to primary in both eu-central-1 and us-east-1, and set a weight of 100 for the primary application load balancer only inthe region that has healthy resources.

Mi-fr: "after blocking access to the amazon rds mysql instance in eu-central-1 from all the application instances running in that region. route 53 does not automatically failover all traffic to us-east-1." when you don't enable health check on route53, it considers the instances as always healthy. for both latency alias records, you set the value of evaluate target health to yes. this causes route 53 to determine whether there are any healthy resources in a region before trying to route traffic there. if not, route 53 chooses a healthy resource in the other region.


464, what combination of steps could a solutions architect take to protect a web workload running on amazon ec2 from ddos and application layer attacks? (select two.)
A. put the ec2 instances behind a network load balancer and configure aws waf on it.
B. migrate the dns to amazon route 53 and use aws shield.
C. put the ec2 instances in an auto scaling group and configure aws waf on it.
D. create and use an amazon cloudfront distribution and configure aws waf on it.
E. create and use an internet gateway in the vpc and use aws shield.

Mi-fr: b https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/ c waf is not for auto scale groupe
https:/  / docs.aws.amazon.com/waf/latest/developerguide/ cloudfront-features.html cloudfront, route 53, aws shield and waf are all mentioned in the blow document https:/ / aws.amazon.com/answers/ networking/ aws-ddos-attack- mitigation/

465, an aws solutions architect has noticed that their company is using almost exclusively ebs general purpose ssd (gp2) volume types for their ebs volumes. they are considering modifying the type of some of these volumes, but it is important that performance is not affected. which of the following actions could the solutions architect consider? (select two)
A.a 50gb gp2 root volume can be modified to an ebs provisioned iops ssd (iol) without stopping the instance.
B.a gp2 volume that is attached to an instance as a root volume needs can be modified to a throughput optimized hdd (stl) volume.
C.a 1gb gp2 volume that is attached to an instance as a non-root volume can be modified to a cold hdd (scl) volume.
D.a ltb gp2 volume that is attached to an instance as a non-root volume can be modified to a throughput optimized hdd (stl) volume without stopping the instance or detaching the volume.


 tfr: b this cannot happenhttps://docs.aws.amazon.com/awsec2/latest/userguide/modify-volume­ requirements.html c won't work as scl has min size S00gbhttps://aws.amazon.com/ebs/features/
https:/ / docs.aws.amazon.com/awsec2 /latest/ userguide / modify-volume­ requirements.html


466, a group of research institutions are partnering to study 2 pb of genomic data that changes regularly. the primary institution that owns the data is storing it in an amazon s3 bucket in its aws account all of the secondary institutions in the partnership have their own aws accounts and require read access to the data. the institute that owns the data does not want to pay for the data transfer costs associated with allowing the secondary institutes access to the data.which of the following solutions will meet the requirements?
A. in the primary account, create a cross-account aws iam role for each secondary account that allows read access to the data. have the secondary institutions assume the role when accessing the data.
B. in the primary account, create an s3 bucket policy to give read access to each secondary account.enable requester pays on the s3 bucket. have the secondary institutions use their own aws credentials with read permissions to the s3 bucket, when accessing the data.
C. create an s3 bucket in each of the secondary accounts with a s3 bucket policy that gives write access to the primary account. periodically synchronize the s3 buckets from the
